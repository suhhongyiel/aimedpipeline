"""
MICA Pipeline Airflow DAG
ì—¬ëŸ¬ ì‚¬ìš©ìê°€ ë™ì‹œì— MICA Pipelineì„ ì‹¤í–‰í•  ë•Œ ì¤‘ì•™ ì§‘ì¤‘ì‹ ê´€ë¦¬ë¥¼ ìœ„í•œ DAG
ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§ì„ í†µí•´ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤(CPU, ë©”ëª¨ë¦¬)ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì‘ì—… í• ë‹¹ëŸ‰ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.
"""
from __future__ import annotations
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
import os
from pathlib import Path
import re

# ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ëª¨ë“ˆ import
try:
    from resource_manager import check_system_resources, get_resource_pool_slots
except ImportError:
    # ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ëª¨ë“ˆì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ í•¨ìˆ˜ ì •ì˜
    def check_system_resources():
        return {"success": True, "recommended_max_tasks": 5, "can_run_more": True}
    def get_resource_pool_slots():
        return 5

DAG_ID = "mica_pipeline"

def log_start(**context):
    """ì‘ì—… ì‹œì‘ ë¡œê·¸"""
    conf = context['dag_run'].conf
    subject_id = conf.get('subject_id', 'unknown')
    session_id = conf.get('session_id', '')
    processes = conf.get('processes', [])
    user = conf.get('user', 'anonymous')
    
    print(f"=" * 80)
    print(f"MICA Pipeline ì‹œì‘")
    print(f"User: {user}")
    print(f"Subject: {subject_id}")
    print(f"Session: {session_id if session_id else 'auto-detect'}")
    print(f"Processes: {', '.join(processes)}")
    print(f"=" * 80)
    
    # XComì— ì •ë³´ ì €ì¥ (ë‹¤ìŒ taskì—ì„œ ì‚¬ìš©)
    return {
        'subject_id': subject_id,
        'session_id': session_id,
        'processes': processes,
        'user': user
    }

def build_docker_command(**context):
    """Docker ì‹¤í–‰ ëª…ë ¹ì–´ ìƒì„±"""
    import os
    from pathlib import Path
    
    ti = context['ti']
    conf = context['dag_run'].conf

    # ì„¸ë¶€ í”Œë˜ê·¸ ì…ë ¥ (struct/surfëŠ” ì˜µì…˜ ì‚¬ìš© ì•ˆ í•¨)
    proc_structural_flags = conf.get('proc_structural_flags', [])
    proc_surf_flags = conf.get('proc_surf_flags', [])
    post_structural_flags = conf.get('post_structural_flags', [])
    proc_func_flags = conf.get('proc_func_flags', [])
    dwi_flags = conf.get('dwi_flags', [])
    sc_flags = conf.get('sc_flags', [])

    # í˜¸ìŠ¤íŠ¸ ê²½ë¡œ (Docker-in-Dockerë¥¼ ìœ„í•œ ì ˆëŒ€ ê²½ë¡œ)
    host_data_dir = os.getenv('HOST_DATA_DIR', '/home/admin1/Documents/aimedpipeline/data')

    # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    subject_id = conf.get('subject_id', 'sub-001')
    session_id = conf.get('session_id', '')
    processes = conf.get('processes', ['proc_structural'])
    bids_dir = conf.get('bids_dir', '/data/bids')
    output_dir = conf.get('output_dir', '/data/derivatives')
    fs_licence = conf.get('fs_licence', '/home/admin1/Documents/aimedpipeline/data/license.txt')
    threads = conf.get('threads', 4)
    freesurfer = conf.get('freesurfer', True)

    must_mount_fs_licence = bool(fs_licence)

    # ë””ë²„ê¹…: confì—ì„œ ë°›ì€ ê°’ í™•ì¸
    print(f"ğŸ” DEBUG - Received from Airflow conf:")
    print(f"  subject_id: {subject_id}")
    print(f"  session_id (raw): '{session_id}' (type: {type(session_id)})")
    print(f"  processes: {processes}")

    # âœ… proc_structural ë‹¨ë… ì—¬ë¶€
    simple_structural = (processes == ['proc_structural'])
    
    # subject IDì—ì„œ "sub-" ì œê±°
    sub_id = subject_id.replace("sub-", "")
    
    # session_idì—ì„œ "ses-" ì ‘ë‘ì‚¬ ì œê±° (ì‚¬ìš©ìê°€ "ses-01" í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•  ìˆ˜ ìˆìŒ)
    original_session_id = session_id
    if session_id:
        session_id = session_id.replace("ses-", "").strip()
        print(f"ğŸ” DEBUG - session_id after processing: '{session_id}' (original: '{original_session_id}')")
    else:
        print(f"ğŸ” DEBUG - session_id is empty or falsy: '{session_id}'")

    # --- flags ì •ë¦¬ ìœ í‹¸ ---
    def normalize_flags(tokens: list[str]) -> list[str]:
        with_val = {"-T1wStr", "-fs_licence", "-surf_dir", "-T1", "-atlas",
                    "-mainScanStr", "-func_pe", "-func_rpe", "-mainScanRun",
                    "-phaseReversalRun", "-topupConfig", "-icafixTraining",
                    "-sesAnat"}
        kv, toggles, passthrough = {}, set(), []
        it = iter(tokens)
        for t in it:
            if t in with_val:
                v = next(it, None)
                if v is None or (isinstance(v, str) and v.startswith("-")):
                    continue
                kv[t] = v
            else:
                if t in ("-freesurfer",):
                    continue
                if t == "-fs_licence":
                    _ = next(it, None)
                    continue
                toggles.add(t) if t.startswith("-") else passthrough.append(t)
        out = []
        for k, v in kv.items():
            if k == "-fs_licence":
                continue
            out += [k, v]
        out += sorted(t for t in toggles if t not in ("-freesurfer",))
        out += passthrough
        return out

    sub_dirname = subject_id if subject_id.startswith("sub-") else f"sub-{subject_id}"
    
    # Session ìë™ ê°ì§€ (session_idê°€ ì—†ì„ ë•Œ = ì „ì²´ ì„¸ì…˜ ì²˜ë¦¬)
    # session_idê°€ ë¹ˆ ë¬¸ìì—´ì´ë©´ -ses ì˜µì…˜ì„ ì¶”ê°€í•˜ì§€ ì•Šì•„ ì „ì²´ ì„¸ì…˜ì´ ì²˜ë¦¬ë¨
    # ë”°ë¼ì„œ ì—¬ê¸°ì„œëŠ” session_idë¥¼ ì„¤ì •í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë¹ˆ ë¬¸ìì—´ë¡œ ìœ ì§€
    if not session_id:
        print(f"â„¹ï¸ No session_id specified - will process all sessions for {subject_id}")
        # session_idë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ìœ ì§€í•˜ì—¬ ì „ì²´ ì„¸ì…˜ ì²˜ë¦¬
        # micapipeëŠ” -ses ì˜µì…˜ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ëª¨ë“  ì„¸ì…˜ì„ ì²˜ë¦¬í•¨
    
    # ì»¨í…Œì´ë„ˆ ì´ë¦„
    container_name = f"{subject_id}"
    if session_id:
        container_name += f"_ses-{session_id}"
    if processes:
        container_name += f"_{processes[0]}"
    
    # ë¡œê·¸ ê²½ë¡œ
    log_base = f"{output_dir}/logs/{processes[0] if processes else 'default'}"
    log_file = f"{log_base}/fin/{container_name}.log"
    error_log_file = f"{log_base}/error/{container_name}_error.log"

    # ì»¨í…Œì´ë„ˆì—ì„œ ë³´ì´ëŠ” ë¡œê·¸ ê²½ë¡œ(/dataë¡œ ì¹˜í™˜)
    container_log_base = log_base.replace(host_data_dir, '/data')
    container_log_file = log_file.replace(host_data_dir, '/data')
    container_error_log_file = error_log_file.replace(host_data_dir, '/data')

    # ê¸°ë³¸ í”„ë¡œì„¸ìŠ¤ ìŠ¤ìœ„ì¹˜ë“¤(-proc_structural, -proc_surf, -post_structural, -proc_func, -dwi, -SC ...)
    process_switches = [f"-{p}" for p in processes]

    # ì„¸ë¶€ í”Œë˜ê·¸(í—ˆìš©ëœ ê²ƒë§Œ): post_structural/func/dwi/sc
    # (struct/surf ì˜µì…˜ì€ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œì™¸)
    extra_flags = []
    #extra_flags += proc_structural_flags
    #extra_flags += proc_surf_flags  
    extra_flags += post_structural_flags
    extra_flags += proc_func_flags
    extra_flags += dwi_flags
    extra_flags += sc_flags
    normalized = normalize_flags(extra_flags)
   
    # âœ… ìˆœì„œ ê³ ì • ë° ì¤‘ë³µ ì œê±°
    ordered_flags = []
    if "-proc_structural" in process_switches:
        ordered_flags.append("-proc_structural")
    if "-proc_func" in process_switches:
        ordered_flags.append("-proc_func")
        # func ê´€ë ¨ ì˜µì…˜ì€ -proc_func ë°”ë¡œ ë’¤ì— ë¶™ì„
        ordered_flags += [f for f in normalized if f in ("-NSR", "-dropTR", "-noFIX")]
    if "-proc_dwi" in process_switches:
        ordered_flags.append("-proc_dwi")
    if "-SC" in process_switches:
        ordered_flags.append("-SC")  
    ordered_flags += [f for f in normalized if f not in ("-NSR", "-dropTR", "-noFIX")]
    process_flags = " ".join(ordered_flags)
  

    # -------------------------
    # Docker ëª…ë ¹ì–´ êµ¬ì„± ë¶„ê¸°
    # -------------------------
    cmd_parts = [
        "docker run --rm",
        f"--name {container_name}",
        f"-v {bids_dir}:{bids_dir}",
        f"-v {output_dir}:{output_dir}",
    ]

    if simple_structural:
        if must_mount_fs_licence:
            cmd_parts.append(f"-v {fs_licence}:{fs_licence}")

        cmd_parts += [
            "micalab/micapipe:v0.2.3",
            f"-bids {bids_dir}",
            f"-out {output_dir}",
            f"-sub {sub_id}",
        ]
        if session_id:
            cmd_parts.append(f"-ses {session_id}")
            print(f"âœ… DEBUG (simple_structural) - Added -ses {session_id} to command")
        else:
            print(f"âš ï¸ DEBUG (simple_structural) - session_id is empty, NOT adding -ses option")
        cmd_parts.append("-proc_structural")

        if must_mount_fs_licence:
            cmd_parts.append(f"-v {fs_licence}:{fs_licence}")
    else:
        if must_mount_fs_licence:
            cmd_parts.append(f"-v {fs_licence}:{fs_licence}")

        cmd_parts += [
            "micalab/micapipe:v0.2.3",
            f"-bids {bids_dir}",
            f"-out {output_dir}",
            f"-sub {sub_id}",
        ]
        if session_id:
            cmd_parts.append(f"-ses {session_id}")
            print(f"âœ… DEBUG (general) - Added -ses {session_id} to command")
        else:
            print(f"âš ï¸ DEBUG (general) - session_id is empty, NOT adding -ses option")

        cmd_parts += [
            f"-threads {threads}",
            process_flags,
        ]

        if 'proc_surf' in processes:
            cmd_parts.append(f"-freesurfer {'TRUE' if freesurfer else 'FALSE'}")

        # ë¼ì´ì„ ìŠ¤ ì¸ìëŠ” í•­ìƒ ì¶”ê°€
        if must_mount_fs_licence:
            cmd_parts.append(f"-fs_licence {fs_licence}")

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± (Airflow ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
    mkdir_cmd = f"mkdir -p {container_log_base}/fin {container_log_base}/error"

    # Docker ì‹¤í–‰ (ë¡œê·¸ ë¦¬ë‹¤ì´ë ‰ì…˜ í¬í•¨ - Airflow ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
    docker_cmd = f"{' '.join(cmd_parts)} > {container_log_file} 2> {container_error_log_file}"

    # docker waitë¡œ ì¢…ë£Œ ëŒ€ê¸° ë° ì˜¤ë¥˜ íƒì§€(í•„ìš”ì‹œ ê°•í™”)
    full_cmd = f"""
    {mkdir_cmd} && \\
    ({docker_cmd} &) && \\
    sleep 2 && \\
    docker wait {container_name} || \\
    (echo "Container {container_name} failed" && exit 1)
    """.strip()

    print("Generated command:")
    print(full_cmd)

    # XCom push
    ti.xcom_push(key='docker_command', value=full_cmd)
    ti.xcom_push(key='container_name', value=container_name)
    ti.xcom_push(key='log_file', value=container_log_file)
    ti.xcom_push(key='error_log_file', value=container_error_log_file)

    return full_cmd


def log_completion(**context):
    """MICA Pipeline ì™„ë£Œ í›„ ë¡œê·¸ ê²€ì¦ (error íŒ¨í„´ ë° ë¡œê·¸ ê¸¸ì´ í¬í•¨)"""
    from pathlib import Path
    import re

    ti = context['ti']
    container_name = ti.xcom_pull(key='container_name', task_ids='build_command')
    main_log_file = ti.xcom_pull(key='log_file', task_ids='build_command')
    error_log_file = ti.xcom_pull(key='error_log_file', task_ids='build_command')

    print("=" * 80)
    print(f"ğŸ§  MICA Pipeline ì™„ë£Œ ê²€ì¦ ì‹œì‘")
    print(f"Container: {container_name}")
    print("=" * 80)

    # ì£¼ìš” ê²€ì‚¬ ê¸°ì¤€
    error_keywords = [
        "error", "traceback", "exception", "license",
        "no such file", "killed", "segmentation fault",
        "failed", "permission denied"
    ]
    
    # í˜¸ìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    host_data_dir = os.getenv('HOST_DATA_DIR', '/home/admin1/Documents/aimedpipeline/data')
    
    # ë¡œê·¸ ê²½ë¡œ ëª©ë¡ (fin / error ë””ë ‰í† ë¦¬ ëª¨ë‘ í™•ì¸)
    log_dirs = [
        Path(f"{host_data_dir}/derivatives/logs/proc_func/error"),
        Path(f"{host_data_dir}/derivatives/logs/proc_func/fin"),
        Path(f"{host_data_dir}/derivatives/logs/proc_structural/error"),
        Path(f"{host_data_dir}/derivatives/logs/proc_structural/fin"),
    ]

    # ê°œë³„ ë¡œê·¸ íŒŒì¼ë„ ì§ì ‘ ì¶”ê°€ (XComìœ¼ë¡œ ì „ë‹¬ëœ íŒŒì¼)
    xcom_logs = [Path(main_log_file), Path(error_log_file)]

    found_issues = []
    total_lines = 0

    # ë¡œê·¸ íŒŒì¼ë“¤ ìˆœíšŒ
    for log_source in log_dirs + xcom_logs:
        if not log_source.exists():
            continue

        # ê°œë³„ íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ì²˜ë¦¬
        if log_source.is_dir():
            log_files = list(log_source.glob("*.log"))
        else:
            log_files = [log_source]

        for log_file in log_files:
            try:
                text = log_file.read_text(errors="ignore")
            except Exception as e:
                print(f"âš ï¸ Failed to read {log_file}: {e}")
                continue

            lines = text.splitlines()
            total_lines += len(lines)

            # 1ï¸âƒ£ ì—ëŸ¬ ë¬¸ìì—´ ê²€ì‚¬
            for kw in error_keywords:
                if re.search(kw, text, re.IGNORECASE):
                    found_issues.append((log_file, kw))

            # 2ï¸âƒ£ ë¡œê·¸ ì¤„ ìˆ˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ê²½ê³ 
            if len(lines) < 50:
                found_issues.append((log_file, f"Too short ({len(lines)} lines)"))

    # 3ï¸âƒ£ ë¬¸ì œ ìˆìœ¼ë©´ ì‹¤íŒ¨ ì²˜ë¦¬
    if found_issues:
        print("\nâŒ Issues found in MICA logs:")
        for f, msg in found_issues:
            print(f"  - {f}: {msg}")
        print("=" * 80)
        raise Exception("Detected errors or insufficient log content in MICA pipeline outputs.")

    # 4ï¸âƒ£ ë¡œê·¸ê°€ ë„ˆë¬´ ì—†ìœ¼ë©´ ì‹¤íŒ¨
    if total_lines == 0:
        raise Exception("No log content found â€” pipeline may have crashed early.")

    print("âœ… Log completion check passed successfully.")
    print("=" * 80)


default_args = {
    "owner": "mica_pipeline",
    "depends_on_past": False,
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 1,  # ì‹¤íŒ¨ ì‹œ 1ë²ˆ ì¬ì‹œë„
    "retry_delay": timedelta(minutes=5),
    "execution_timeout": timedelta(hours=6),  # ìµœëŒ€ 6ì‹œê°„
}

# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ì— ê¸°ë°˜í•œ ë™ì  concurrency ê³„ì‚°
# ë¦¬ì†ŒìŠ¤ê°€ ì¶©ë¶„í•˜ë©´ ë” ë§ì€ ì‘ì—… ì‹¤í–‰, ë¶€ì¡±í•˜ë©´ ì œí•œ
try:
    resource_info = check_system_resources()
    if resource_info.get("success"):
        # ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ë™ì  concurrency ì„¤ì •
        # CPUì™€ ë©”ëª¨ë¦¬ ì¤‘ ì‘ì€ ê°’ ì‚¬ìš©, ìµœì†Œ 2ê°œëŠ” ë³´ì¥
        dynamic_concurrency = max(2, resource_info.get("recommended_max_tasks", 5))
        # ìµœëŒ€ 10ê°œë¡œ ì œí•œ (ê³¼ë„í•œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ë°©ì§€)
        dynamic_concurrency = min(dynamic_concurrency, 10)
        print(f"ğŸ” System resources check: Recommended concurrency = {dynamic_concurrency}")
    else:
        dynamic_concurrency = 5  # ê¸°ë³¸ê°’
except Exception as e:
    print(f"âš ï¸ Failed to check system resources: {e}, using default concurrency")
    dynamic_concurrency = 5  # ê¸°ë³¸ê°’

with DAG(
    dag_id=DAG_ID,
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,  # Manual trigger only
    catchup=False,
    default_args=default_args,
    tags=["mica", "neuroimaging", "production"],
    max_active_runs=5,  # ìµœëŒ€ 5ê°œì˜ DAG ë™ì‹œ ì‹¤í–‰
    concurrency=dynamic_concurrency,  # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ì— ê¸°ë°˜í•œ ë™ì  ë™ì‹œ ì‹¤í–‰ ìˆ˜
    description="MICA Pipeline - Multi-user neuroimaging processing pipeline with resource-based scheduling",
) as dag:

    # Task 0: ë¦¬ì†ŒìŠ¤ í™•ì¸ (ì„ íƒì , ë¦¬ì†ŒìŠ¤ ì²´í¬ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
    def check_resources(**context):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¸í•˜ê³  ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤."""
        try:
            resources = check_system_resources()
            if resources.get("success"):
                print(f"ğŸ“Š System Resources:")
                print(f"  CPU: {resources['cpu']['percent']}% used ({resources['cpu']['available']:.1f} cores available)")
                print(f"  Memory: {resources['memory']['percent']}% used ({resources['memory']['available_gb']:.2f} GB available)")
                print(f"  Running containers: {resources['running_containers']}")
                print(f"  Recommended max tasks: {resources['recommended_max_tasks']}")
                print(f"  Can run more: {resources['can_run_more']}")
            else:
                print(f"âš ï¸ Resource check failed: {resources.get('error', 'Unknown error')}")
        except Exception as e:
            print(f"âš ï¸ Resource check error: {e}")
    
    resource_check_task = PythonOperator(
        task_id="check_resources",
        python_callable=check_resources,
        pool="default_pool",  # ë¦¬ì†ŒìŠ¤ í’€ ì‚¬ìš© (ì„ íƒì )
    )
    
    # Task 1: ì‹œì‘ ë¡œê·¸
    start_task = PythonOperator(
        task_id="log_start",
        python_callable=log_start,
        pool="default_pool",  # ë¦¬ì†ŒìŠ¤ í’€ ì‚¬ìš©
    )
    
    # Task 2: Docker ëª…ë ¹ì–´ ìƒì„±
    build_command_task = PythonOperator(
        task_id="build_command",
        python_callable=build_docker_command,
        pool="default_pool",  # ë¦¬ì†ŒìŠ¤ í’€ ì‚¬ìš©
    )
    
    # Task 3: MICA Pipeline ì‹¤í–‰
    # ì£¼ì˜: Airflow ì»¨í…Œì´ë„ˆì—ì„œ í˜¸ìŠ¤íŠ¸ì˜ Dockerë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ Docker socket ë§ˆìš´íŠ¸ í•„ìš”
    run_micapipe_task = BashOperator(
        task_id="run_micapipe",
        bash_command="{{ ti.xcom_pull(key='docker_command', task_ids='build_command') }}",
        execution_timeout=timedelta(hours=6),
        pool="default_pool",  # ë¦¬ì†ŒìŠ¤ í’€ ì‚¬ìš© (ê°€ì¥ ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì ì¸ ì‘ì—…)
    )
    
    # Task 4: ì™„ë£Œ ë¡œê·¸
    complete_task = PythonOperator(
        task_id="log_completion",
        python_callable=log_completion,
        pool="default_pool",  # ë¦¬ì†ŒìŠ¤ í’€ ì‚¬ìš©
    )
    
    # Task ì˜ì¡´ì„± ì„¤ì •
    # ë¦¬ì†ŒìŠ¤ í™•ì¸ -> ì‹œì‘ ë¡œê·¸ -> ëª…ë ¹ì–´ ìƒì„± -> ì‹¤í–‰ -> ì™„ë£Œ ë¡œê·¸
    resource_check_task >> start_task >> build_command_task >> run_micapipe_task >> complete_task

