"""
MICA Pipeline Airflow DAG
ì—¬ëŸ¬ ì‚¬ìš©ìê°€ ë™ì‹œì— MICA Pipelineì„ ì‹¤í–‰í•  ë•Œ ì¤‘ì•™ ì§‘ì¤‘ì‹ ê´€ë¦¬ë¥¼ ìœ„í•œ DAG
"""
from __future__ import annotations
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
import os
from pathlib import Path
import re

DAG_ID = "mica_pipeline"

def log_start(**context):
    """ì‘ì—… ì‹œì‘ ë¡œê·¸"""
    conf = context['dag_run'].conf
    subject_id = conf.get('subject_id', 'unknown')
    session_id = conf.get('session_id', '')
    processes = conf.get('processes', [])
    user = conf.get('user', 'anonymous')
    
    print(f"=" * 80)
    print(f"MICA Pipeline ì‹œì‘")
    print(f"User: {user}")
    print(f"Subject: {subject_id}")
    print(f"Session: {session_id if session_id else 'auto-detect'}")
    print(f"Processes: {', '.join(processes)}")
    print(f"=" * 80)
    
    # XComì— ì •ë³´ ì €ì¥ (ë‹¤ìŒ taskì—ì„œ ì‚¬ìš©)
    return {
        'subject_id': subject_id,
        'session_id': session_id,
        'processes': processes,
        'user': user
    }

def build_docker_command(**context):
    """Docker ì‹¤í–‰ ëª…ë ¹ì–´ ìƒì„±"""
    import os
    from pathlib import Path
    
    ti = context['ti']
    conf = context['dag_run'].conf
    proc_structural_flags = conf.get('proc_structural_flags', [])
    proc_surf_flags = conf.get('proc_surf_flags', [])
    post_structural_flags = conf.get('post_structural_flags', [])
    proc_func_flags = conf.get('proc_func_flags', [])
    dwi_flags = conf.get('dwi_flags', [])
    sc_flags = conf.get('sc_flags', [])

    # í˜¸ìŠ¤íŠ¸ ê²½ë¡œ (Docker-in-Dockerë¥¼ ìœ„í•œ ì ˆëŒ€ ê²½ë¡œ)
    host_data_dir = os.getenv('HOST_DATA_DIR', '/private/boonam/98-dev/aimedpipeline/data')
    # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    subject_id = conf.get('subject_id', 'sub-001')
    session_id = conf.get('session_id', '')
    processes = conf.get('processes', ['proc_structural'])
    bids_dir = conf.get('bids_dir', '/data/bids')
    output_dir = conf.get('output_dir', '/data/derivatives')
    fs_licence = conf.get('fs_licence', '/data/license.txt')
    threads = conf.get('threads', 4)
    freesurfer = conf.get('freesurfer', True)
    
    # subject IDì—ì„œ "sub-" ì œê±°
    sub_id = subject_id.replace("sub-", "")
    
    # Session ìë™ ê°ì§€ (session_idê°€ ì—†ì„ ë•Œ)
    if not session_id:
        subject_path = Path(bids_dir) / subject_id
        if subject_path.exists():
            # ses-* ë””ë ‰í† ë¦¬ ì°¾ê¸°
            session_dirs = [d.name.replace("ses-", "") for d in subject_path.iterdir() 
                          if d.is_dir() and d.name.startswith("ses-")]
            if session_dirs:
                session_id = session_dirs[0]  # ì²« ë²ˆì§¸ session ì‚¬ìš©
                print(f"Auto-detected session: {session_id}")
            else:
                print("No session found - using direct path")
        else:
            print(f"Warning: Subject path not found: {subject_path}")
    
    # ì»¨í…Œì´ë„ˆ ì´ë¦„ ìƒì„±
    container_name = f"{subject_id}"
    if session_id:
        container_name += f"_ses-{session_id}"
    if processes:
        container_name += f"_{processes[0]}"
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬
    log_base = f"{output_dir}/logs/{processes[0] if processes else 'default'}"
    log_file = f"{log_base}/fin/{container_name}.log"
    error_log_file = f"{log_base}/error/{container_name}_error.log"

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ - ë¡œê·¸ íŒŒì¼ ì½ê¸°ìš©)
    container_log_base = log_base.replace(host_data_dir, '/data')
    container_log_file = log_file.replace(host_data_dir, '/data')
    container_error_log_file = error_log_file.replace(host_data_dir, '/data')

    # í”„ë¡œì„¸ìŠ¤ í”Œë˜ê·¸
    # ê¸°ë³¸ í”„ë¡œì„¸ìŠ¤ ìŠ¤ìœ„ì¹˜ë“¤(-proc_func, -proc_dwi, -SC ë“±)
    process_switches = [f"-{p}" for p in processes]

    # ì„¸ë¶€ í”Œë˜ê·¸(ì´ë¯¸ ["-ì˜µì…˜", "ê°’", ...] í˜•íƒœë¼ê³  ê°€ì •)
    extra_flags = []
    extra_flags += proc_structural_flags
    extra_flags += proc_surf_flags
    extra_flags += post_structural_flags
    extra_flags += proc_func_flags
    extra_flags += dwi_flags
    extra_flags += sc_flags

    process_flags = " ".join(process_switches + extra_flags)


    
    # Docker ëª…ë ¹ì–´ êµ¬ì„±
    cmd_parts = [
        "docker run --rm",
        f"--name {container_name}",
        f"-v {bids_dir}:{bids_dir}",
        f"-v {output_dir}:{output_dir}",
    ]
    
    # FreeSurfer ë¼ì´ì„¼ìŠ¤
    if os.path.exists(fs_licence):
        cmd_parts.append(f"-v {fs_licence}:{fs_licence}")
    
    cmd_parts.extend([
        "micalab/micapipe:v0.2.3",
        f"-bids {bids_dir}",
        f"-out {output_dir}",
        f"-sub {sub_id}",
    ])
    
    if session_id:
        cmd_parts.append(f"-ses {session_id}")
    
    if os.path.exists(fs_licence):
        cmd_parts.append(f"-fs_licence {fs_licence}")
    
    cmd_parts.extend([
        f"-threads {threads}",
        process_flags,
        f"-freesurfer {'TRUE' if freesurfer else 'FALSE'}",
    ])
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± ëª…ë ¹ (Airflow ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
    mkdir_cmd = f"mkdir -p {container_log_base}/fin {container_log_base}/error"
    # Docker ëª…ë ¹ì–´ (ë¡œê·¸ ë¦¬ë‹¤ì´ë ‰ì…˜ í¬í•¨ - Airflow ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
    docker_cmd = f"{' '.join(cmd_parts)} > {container_log_file} 2> {container_error_log_file}"

    # docker ì™„ë£Œ í›„ logì— error ê²€ì‚¬ (Airflow fail ìœ ë„)
    check_log_cmd = f"""
    if grep -iE 'error|traceback|license|failed|killed|permission denied' {container_log_file} {container_error_log_file} >/dev/null 2>&1; then
        echo 'âŒ Error detected in logs.';
        tail -n 10 {container_log_file};
        exit 1;
    fi
    """
    # ìµœì¢… ëª…ë ¹ì–´: docker run í›„ ì»¨í…Œì´ë„ˆê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    # 1. ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    # 2. Docker ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
    # 3. ì»¨í…Œì´ë„ˆ ì‹œì‘ ëŒ€ê¸°
    # 4. docker waitë¡œ ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ëŒ€ê¸°
    # 5. exit code í™•ì¸í•˜ì—¬ ì—ëŸ¬ë©´ ì‹¤íŒ¨ ì²˜ë¦¬
    full_cmd = f"""
    {mkdir_cmd} && \\
    ({docker_cmd} &) && \\
    sleep 2 && \\
    
    docker wait {container_name} || \\
    (echo "Container {container_name} failed" && exit 1)
    """.strip()
    
    
    print(f"Generated command:")
    print(full_cmd)
    
    # XComì— ì €ì¥
    ti.xcom_push(key='docker_command', value=full_cmd)
    ti.xcom_push(key='container_name', value=container_name)
    ti.xcom_push(key='log_file', value=container_log_file)
    ti.xcom_push(key='error_log_file', value=container_error_log_file)

    return full_cmd

def log_completion(**context):
    """MICA Pipeline ì™„ë£Œ í›„ ë¡œê·¸ ê²€ì¦ (error íŒ¨í„´ ë° ë¡œê·¸ ê¸¸ì´ í¬í•¨)"""
    from pathlib import Path
    import re

    ti = context['ti']
    container_name = ti.xcom_pull(key='container_name', task_ids='build_command')
    main_log_file = ti.xcom_pull(key='log_file', task_ids='build_command')
    error_log_file = ti.xcom_pull(key='error_log_file', task_ids='build_command')

    print("=" * 80)
    print(f"ğŸ§  MICA Pipeline ì™„ë£Œ ê²€ì¦ ì‹œì‘")
    print(f"Container: {container_name}")
    print("=" * 80)

    # ì£¼ìš” ê²€ì‚¬ ê¸°ì¤€
    error_keywords = [
        "error", "traceback", "exception", "license",
        "no such file", "killed", "segmentation fault",
        "failed", "permission denied"
    ]
    #100ë²ˆ ê¸°ì¤€ìœ¼ë¡œ /home/admin1/Documents/aimedpipeline ì´ê±°ë¡œ ë°”ê¾¸ê¸´ í•´ì•¼í•¨
    # ë¡œê·¸ ê²½ë¡œ ëª©ë¡ (fin / error ë””ë ‰í† ë¦¬ ëª¨ë‘ í™•ì¸)
    log_dirs = [
        Path("/private/boonam/98-dev/aimedpipeline/data/derivatives/logs/proc_func/error"),
        Path("/private/boonam/98-dev/aimedpipeline/data/derivatives/logs/proc_func/fin"),
        Path("/private/boonam/98-dev/aimedpipeline/data/derivatives/logs/proc_structural/error"),
        Path("/private/boonam/98-dev/aimedpipeline/data/derivatives/logs/proc_structural/fin"),
    ]

    # ê°œë³„ ë¡œê·¸ íŒŒì¼ë„ ì§ì ‘ ì¶”ê°€ (XComìœ¼ë¡œ ì „ë‹¬ëœ íŒŒì¼)
    xcom_logs = [Path(main_log_file), Path(error_log_file)]

    found_issues = []
    total_lines = 0

    # ë¡œê·¸ íŒŒì¼ë“¤ ìˆœíšŒ
    for log_source in log_dirs + xcom_logs:
        if not log_source.exists():
            continue

        # ê°œë³„ íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ì²˜ë¦¬
        if log_source.is_dir():
            log_files = list(log_source.glob("*.log"))
        else:
            log_files = [log_source]

        for log_file in log_files:
            try:
                text = log_file.read_text(errors="ignore")
            except Exception as e:
                print(f"âš ï¸ Failed to read {log_file}: {e}")
                continue

            lines = text.splitlines()
            total_lines += len(lines)

            # 1ï¸âƒ£ ì—ëŸ¬ ë¬¸ìì—´ ê²€ì‚¬
            for kw in error_keywords:
                if re.search(kw, text, re.IGNORECASE):
                    found_issues.append((log_file, kw))

            # 2ï¸âƒ£ ë¡œê·¸ ì¤„ ìˆ˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ê²½ê³ 
            if len(lines) < 50:
                found_issues.append((log_file, f"Too short ({len(lines)} lines)"))

    # 3ï¸âƒ£ ë¬¸ì œ ìˆìœ¼ë©´ ì‹¤íŒ¨ ì²˜ë¦¬
    if found_issues:
        print("\nâŒ Issues found in MICA logs:")
        for f, msg in found_issues:
            print(f"  - {f}: {msg}")
        print("=" * 80)
        raise Exception("Detected errors or insufficient log content in MICA pipeline outputs.")

    # 4ï¸âƒ£ ë¡œê·¸ê°€ ë„ˆë¬´ ì—†ìœ¼ë©´ ì‹¤íŒ¨
    if total_lines == 0:
        raise Exception("No log content found â€” pipeline may have crashed early.")

    print("âœ… Log completion check passed successfully.")
    print("=" * 80)


default_args = {
    "owner": "mica_pipeline",
    "depends_on_past": False,
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 1,  # ì‹¤íŒ¨ ì‹œ 1ë²ˆ ì¬ì‹œë„
    "retry_delay": timedelta(minutes=5),
    "execution_timeout": timedelta(hours=6),  # ìµœëŒ€ 6ì‹œê°„
}

with DAG(
    dag_id=DAG_ID,
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,  # Manual trigger only
    catchup=False,
    default_args=default_args,
    tags=["mica", "neuroimaging", "production"],
    max_active_runs=5,  # ìµœëŒ€ 5ê°œì˜ DAG ë™ì‹œ ì‹¤í–‰
    concurrency=10,  # ìµœëŒ€ 10ê°œì˜ task ë™ì‹œ ì‹¤í–‰
    description="MICA Pipeline - Multi-user neuroimaging processing pipeline",
) as dag:

    # Task 1: ì‹œì‘ ë¡œê·¸
    start_task = PythonOperator(
        task_id="log_start",
        python_callable=log_start,
    )
    
    # Task 2: Docker ëª…ë ¹ì–´ ìƒì„±
    build_command_task = PythonOperator(
        task_id="build_command",
        python_callable=build_docker_command,
    )
    
    # Task 3: MICA Pipeline ì‹¤í–‰
    # ì£¼ì˜: Airflow ì»¨í…Œì´ë„ˆì—ì„œ í˜¸ìŠ¤íŠ¸ì˜ Dockerë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ Docker socket ë§ˆìš´íŠ¸ í•„ìš”
    run_micapipe_task = BashOperator(
        task_id="run_micapipe",
        bash_command="{{ ti.xcom_pull(key='docker_command', task_ids='build_command') }}",
        execution_timeout=timedelta(hours=6),
    )
    
    # Task 4: ì™„ë£Œ ë¡œê·¸
    complete_task = PythonOperator(
        task_id="log_completion",
        python_callable=log_completion,
    )
    
    # Task ì˜ì¡´ì„± ì„¤ì •
    start_task >> build_command_task >> run_micapipe_task >> complete_task

