services:
  postgres:
    image: postgres:13
    container_name: aimedpipeline_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5433:5432"
    volumes:
      - postgres_db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: unless-stopped

  streamlit:
    build: .
    container_name: aimedpipeline_streamlit
    ports:
      - "8502:8501"
    environment:
      STREAMLIT_SERVER_HEADLESS: "true"
      STREAMLIT_SERVER_ENABLE_CORS: "false"
      FASTAPI_SERVER_URL: http://backend:8000
    volumes:
      - ./:/app
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      - backend
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: aimedpipeline_backend
    ports:
      - "8003:8000"
    environment:
      AIRFLOW_BASE_URL: http://airflow:8080
      AIRFLOW_DAG_ID: mri_pipeline
      AIRFLOW_USER: admin
      AIRFLOW_PASSWORD: admin
      HOST_DATA_DIR: /data  # 컨테이너 내부 절대 경로
    volumes:
      - ./logs/backend:/app/logs
      - ./:/app/workspace
      - ./data:/app/data
      - /var/run/docker.sock:/var/run/docker.sock  # Docker 소켓 마운트
      - ./data:/data  # 호스트 상대경로 -> 컨테이너 내부 절대경로
    working_dir: /app/workspace/backend
    depends_on:
      - airflow
    restart: unless-stopped

  airflow:
    image: apache/airflow:2.9.1
    container_name: aimedpipeline_airflow
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "8081:8080"
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ""
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock  # Docker 소켓 마운트 (MICA Pipeline 실행용)
      - ./data:/data  # 호스트 상대경로 -> 컨테이너 내부 절대경로 (Docker 명령어에서 동일한 경로 사용)
    user: "0:0"
    command: >
      bash -c "
        set -e
        echo 'Installing dependencies...'
        apt-get update -qq && apt-get install -y -qq docker.io curl
        
        echo 'Creating log directory...'
        mkdir -p /opt/airflow/logs
        chmod -R 777 /opt/airflow/logs
        
        echo 'Initializing Airflow database...'
        airflow db migrate || airflow db init
        
        echo 'Creating admin user...'
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com 2>/dev/null || echo 'User already exists'
        
        echo 'Starting Airflow webserver...'
        airflow webserver -p 8080 &
        
        echo 'Starting Airflow scheduler...'
        exec airflow scheduler"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]  # 컨테이너 내부는 8080 유지
      interval: 10s
      timeout: 5s
      retries: 30
    restart: unless-stopped

volumes:
  postgres_db:
  airflow_db:
  airflow_logs:
