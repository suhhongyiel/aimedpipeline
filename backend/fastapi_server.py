import os
import uuid
import requests
from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime
import shutil
import json
from typing import List
import zipfile
import tarfile
# tokenize for shell command ì•ˆì „ ì²˜ë¦¬
import shlex
# db ê´€ë ¨
# --- DB ê´€ë ¨ import ìˆ˜ì • ---
from database import SessionLocal
from models import JobLog, CommandLog, MicaPipelineJob
import models
from database import engine
from sqlalchemy.orm import Session
from fastapi import Depends

# ===== í™˜ê²½ë³€ìˆ˜ ê¸°ë³¸ê°’ =====
AIRFLOW_BASE = os.getenv("AIRFLOW_BASE_URL", "http://airflow:8080")
#AIRFLOW_BASE = os.getenv("AIRFLOW_BASE_URL", "http://localhost:8080")
AIRFLOW_API  = f"{AIRFLOW_BASE}/api/v1"
AIRFLOW_DAG  = os.getenv("AIRFLOW_DAG_ID", "mri_pipeline")
AIRFLOW_USER = os.getenv("AIRFLOW_USER", "admin")
AIRFLOW_PASS = os.getenv("AIRFLOW_PASSWORD", "admin")

# --- ì„œë²„ ì‹œì‘ ì‹œ DB í…Œì´ë¸” ìƒì„± ---
# ì´ë¯¸ ìˆìœ¼ë©´ ë³„ ì‘ì—… ì•ˆí•¨
models.Base.metadata.create_all(bind=engine)
# --------------------------------
app = FastAPI(title="AIMed Pipeline Backend")

# Streamlit(ë³„ë„ ì»¨í…Œì´ë„ˆ)ì—ì„œ í˜¸ì¶œí•˜ë¯€ë¡œ CORS í—ˆìš©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

def _auth():
    return (AIRFLOW_USER, AIRFLOW_PASS)

# db session í•¨ìˆ˜
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@app.on_event("startup")
def on_startup():
    models.Base.metadata.create_all(bind=engine)

@app.get("/")
def root():
    return {"ok": True, "airflow": AIRFLOW_BASE, "dag": AIRFLOW_DAG}

@app.post("/run-job")
def run_job(job_type: str = "MRI ë¶„ì„", db: Session = Depends(get_db)):
    run_id = f"ui_{uuid.uuid4().hex[:8]}"
    payload = {"dag_run_id": run_id, "conf": {"job_type": job_type}}
    # Airflow mock (ì‹¤ì œ Airflow ì—†ì´)
    try:
        r = requests.post(f"{AIRFLOW_API}/dags/{AIRFLOW_DAG}/dagRuns", json=payload, auth=_auth(), timeout=2)
        if r.status_code not in (200, 201):
            raise HTTPException(status_code=r.status_code, detail=r.text)
    except Exception as e:
        print(f"[Mock] Airflow í˜¸ì¶œ ì‹¤íŒ¨, í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¬´ì‹œ: {e}")
        
    # ---- ë¡œê·¸ DBì— ì €ì¥ ----
    log = JobLog(
        job_id=run_id,
        job_type=job_type,
        status="requested",
        log="Job requested (mock)"
    )
    db.add(log)
    db.commit()
    return {"job_id": run_id, "note": "Airflow ë¯¸ì—°ë™ mock"}

@app.get("/job-status/{job_id}")
def job_status(job_id: str):
    try:
        # ì‹¤ì œ Airflow REST API í˜¸ì¶œ ë¶€ë¶„
        dr = requests.get(f"{AIRFLOW_API}/dags/{AIRFLOW_DAG}/dagRuns/{job_id}", auth=_auth(), timeout=2)
        if dr.status_code != 200:
            raise HTTPException(status_code=dr.status_code, detail=dr.text)
        drj = dr.json()
        state = drj.get("state", "unknown")

        tis = requests.get(
            f"{AIRFLOW_API}/dags/{AIRFLOW_DAG}/dagRuns/{job_id}/taskInstances",
            auth=_auth(), timeout=2
        )
        if tis.status_code != 200:
            raise HTTPException(status_code=tis.status_code, detail=tis.text)
        tasks = []
        for ti in tis.json().get("task_instances", []):
            tasks.append({
                "task_id": ti["task_id"],
                "state": ti["state"],
                "start_date": ti.get("start_date"),
                "end_date": ti.get("end_date"),
                "try_number": ti.get("try_number"),
            })

        total = max(len(tasks), 1)
        success = sum(1 for t in tasks if (t["state"] or "").lower() == "success")
        running = sum(1 for t in tasks if (t["state"] or "").lower() in ("queued", "running"))
        progress = int((success/total)*100 + (running/total)*25)
        progress = min(progress, 99 if state.lower() not in ("success", "failed") else 100)

        ui_url = f"{AIRFLOW_BASE}/dag/{AIRFLOW_DAG}/grid?dag_run_id={job_id}"
        return {
            "status": state, "tasks": tasks, "progress": progress,
            "airflow_ui": ui_url, "log": f"DagRun {job_id} is {state}"
        }
    except Exception as e:
        # Airflow ìš”ì²­ ì‹¤íŒ¨ ì‹œ mock ë°ì´í„° ë°˜í™˜ (ì—ëŸ¬ ì•ˆ ë‚˜ê²Œ í•¨)
        print(f"[Mock] Airflow ìƒíƒœì¡°íšŒ ì‹¤íŒ¨, ë”ë¯¸ê°’ ë°˜í™˜: {e}")
        return {
            "status": "mocked",
            "tasks": [{"task_id": "mocked_task", "state": "success"}],
            "progress": 100,
            "airflow_ui": "mock_url",
            "log": f"Mock DagRun {job_id} is mocked"
        }
    
@app.post("/run-command")
async def run_command(data: dict):
    """ì„œë²„ì—ì„œ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    cmd = data.get("cmd")
    if not cmd:
        raise HTTPException(status_code=400, detail="Command is required")
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì • (ë³¼ë¥¨ ë§ˆìš´íŠ¸ëœ ê²½ë¡œ ì‚¬ìš©)
    work_dir = data.get("work_dir", "/app/workspace")
    work_dir = os.path.abspath(work_dir) if not work_dir.startswith("/") else work_dir
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(work_dir, exist_ok=True)
    
    start_time = datetime.now()
    try:
        result = subprocess.run(
            cmd, 
            shell=True, 
            capture_output=True, 
            text=True,
            cwd=work_dir,
            timeout=data.get("timeout", 300)  # ê¸°ë³¸ 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # DBì— ë¡œê·¸ ì €ì¥
        session = SessionLocal()
        try:
            log = CommandLog(
                command=cmd,
                output=result.stdout,
                error=result.stderr
            )
            session.add(log)
            session.commit()
        finally:
            session.close()
        
        return {
            "success": result.returncode == 0,
            "output": result.stdout,
            "error": result.stderr,
            "returncode": result.returncode,
            "work_dir": work_dir,
            "duration": f"{duration:.2f}s",
            "timestamp": datetime.now().isoformat()
        }
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "output": "",
            "error": f"Command timeout after {data.get('timeout', 300)} seconds",
            "returncode": -1,
            "work_dir": work_dir,
            "duration": None,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "success": False,
            "output": "",
            "error": str(e),
            "returncode": -1,
            "work_dir": work_dir,
            "duration": None,
            "timestamp": datetime.now().isoformat()
        }

@app.get("/list-files")
async def list_files(path: str = "/app/workspace"):
    """ì§€ì •ëœ ê²½ë¡œì˜ íŒŒì¼ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        path_obj = Path(path)
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail=f"Path not found: {path}")
        
        files = []
        for item in path_obj.iterdir():
            try:
                stat = item.stat()
                files.append({
                    "name": item.name,
                    "path": str(item),
                    "type": "directory" if item.is_dir() else "file",
                    "size": stat.st_size if item.is_file() else None,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "permissions": oct(stat.st_mode)[-3:]
                })
            except PermissionError:
                continue
        
        # ì´ë¦„ìˆœ ì •ë ¬
        files.sort(key=lambda x: (x["type"] == "file", x["name"]))
        
        return {
            "path": path,
            "files": files,
            "count": len(files)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/create-file")
async def create_file(data: dict):
    """íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    file_path = data.get("file_path")
    content = data.get("content", "")
    work_dir = data.get("work_dir", "/app/workspace")
    
    if not file_path:
        raise HTTPException(status_code=400, detail="file_path is required")
    
    # ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹ˆë©´ work_dir ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
    if not file_path.startswith("/"):
        file_path = os.path.join(work_dir, file_path)
    
    try:
        path_obj = Path(file_path)
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        path_obj.parent.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ ìƒì„±
        path_obj.write_text(content, encoding="utf-8")
        
        return {
            "success": True,
            "file_path": str(path_obj),
            "size": path_obj.stat().st_size,
            "message": f"File created: {file_path}"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/delete-file")
async def delete_file(file_path: str, work_dir: str = "/app/workspace"):
    """íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    if not file_path:
        raise HTTPException(status_code=400, detail="file_path is required")
    
    # ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹ˆë©´ work_dir ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
    if not file_path.startswith("/"):
        file_path = os.path.join(work_dir, file_path)
    
    try:
        path_obj = Path(file_path)
        
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail=f"Path not found: {file_path}")
        
        # ì•ˆì „ì¥ì¹˜: ì‘ì—… ë””ë ‰í† ë¦¬ ì™¸ë¶€ ì‚­ì œ ë°©ì§€
        if not str(path_obj.resolve()).startswith("/app"):
            raise HTTPException(status_code=403, detail="Cannot delete files outside /app directory")
        
        if path_obj.is_dir():
            import shutil
            shutil.rmtree(path_obj)
            item_type = "directory"
        else:
            path_obj.unlink()
            item_type = "file"
        
        return {
            "success": True,
            "file_path": str(path_obj),
            "type": item_type,
            "message": f"{item_type.capitalize()} deleted: {file_path}"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/read-file")
async def read_file(file_path: str, work_dir: str = "/app/workspace"):
    """íŒŒì¼ ë‚´ìš©ì„ ì½ìŠµë‹ˆë‹¤."""
    if not file_path:
        raise HTTPException(status_code=400, detail="file_path is required")
    
    # ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹ˆë©´ work_dir ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
    if not file_path.startswith("/"):
        file_path = os.path.join(work_dir, file_path)
    
    try:
        path_obj = Path(file_path)
        
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail=f"File not found: {file_path}")
        
        if not path_obj.is_file():
            raise HTTPException(status_code=400, detail=f"Path is not a file: {file_path}")
        
        # ì•ˆì „ì¥ì¹˜: ì‘ì—… ë””ë ‰í† ë¦¬ ì™¸ë¶€ íŒŒì¼ ì½ê¸° ë°©ì§€
        if not str(path_obj.resolve()).startswith("/app"):
            raise HTTPException(status_code=403, detail="Cannot read files outside /app directory")
        
        content = path_obj.read_text(encoding="utf-8")
        stat = path_obj.stat()
        
        return {
            "file_path": str(path_obj),
            "content": content,
            "size": stat.st_size,
            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload-file")
async def upload_file(
    files: List[UploadFile] = File(...),
    destination: str = Form("/app/data"),
    extract_archives: bool = Form(True)
):
    """
    íŒŒì¼ì„ ì„œë²„ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    ì••ì¶• íŒŒì¼(.zip, .tar.gz, .tgz)ì€ ìë™ìœ¼ë¡œ ì••ì¶• í•´ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        # ëª©ì ì§€ ë””ë ‰í† ë¦¬ ìƒì„±
        dest_path = Path(destination)
        dest_path.mkdir(parents=True, exist_ok=True)
        
        uploaded_files = []
        extracted_files = []
        total_size = 0
        
        for file in files:
            # íŒŒì¼ ì €ì¥
            file_path = dest_path / file.filename
            
            # íŒŒì¼ ì“°ê¸°
            with file_path.open("wb") as buffer:
                content = await file.read()
                buffer.write(content)
            
            file_size = file_path.stat().st_size
            total_size += file_size
            
            file_info = {
                "filename": file.filename,
                "path": str(file_path),
                "size": file_size,
                "content_type": file.content_type,
                "extracted": False
            }
            
            # ì••ì¶• íŒŒì¼ ìë™ ì••ì¶• í•´ì œ
            if extract_archives:
                extracted = False
                
                # ZIP íŒŒì¼ ì²˜ë¦¬
                if file.filename.lower().endswith('.zip'):
                    try:
                        with zipfile.ZipFile(file_path, 'r') as zip_ref:
                            zip_ref.extractall(dest_path)
                        extracted = True
                        extracted_files.extend(zip_ref.namelist())
                        file_info["extracted"] = True
                        file_info["archive_type"] = "zip"
                    except Exception as e:
                        file_info["extraction_error"] = str(e)
                
                # TAR.GZ ë˜ëŠ” TGZ íŒŒì¼ ì²˜ë¦¬
                elif file.filename.lower().endswith(('.tar.gz', '.tgz', '.tar')):
                    try:
                        with tarfile.open(file_path, 'r:*') as tar_ref:
                            tar_ref.extractall(dest_path)
                        extracted = True
                        extracted_files.extend([m.name for m in tar_ref.getmembers()])
                        file_info["extracted"] = True
                        file_info["archive_type"] = "tar"
                    except Exception as e:
                        file_info["extraction_error"] = str(e)
                
                # ì••ì¶• í•´ì œ ì„±ê³µ ì‹œ ì›ë³¸ ì••ì¶• íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
                if extracted:
                    file_path.unlink()
                    file_info["original_removed"] = True
            
            uploaded_files.append(file_info)
        
        response = {
            "success": True,
            "uploaded_files": uploaded_files,
            "count": len(uploaded_files),
            "total_size": total_size,
            "destination": destination,
            "message": f"{len(uploaded_files)} file(s) uploaded successfully"
        }
        
        if extracted_files:
            response["extracted_files_count"] = len(extracted_files)
            response["extracted_files_sample"] = extracted_files[:10]  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
        
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/validate-bids")
async def validate_bids(directory: str = "/app/data/bids"):
    """BIDS í¬ë§· ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        dir_path = Path(directory)
        
        if not dir_path.exists():
            raise HTTPException(status_code=404, detail=f"Directory not found: {directory}")
        
        # ë¬´ì‹œí•  ì‹œìŠ¤í…œ í´ë”/íŒŒì¼
        ignore_list = {"__MACOSX", ".DS_Store", "Thumbs.db", ".git", ".gitignore"}
        
        # BIDS ê²€ì¦ ê²°ê³¼
        validation_result = {
            "is_valid": False,
            "errors": [],
            "warnings": [],
            "directory": directory,
            "structure": {},
            "details": []
        }
        
        # í•„ìˆ˜ íŒŒì¼/í´ë” ì²´í¬
        required_items = {
            "dataset_description.json": False,
            "README": False,
            "participants.tsv": False
        }
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„
        all_items = []
        subject_dirs = []
        
        for item in dir_path.iterdir():
            # ì‹œìŠ¤í…œ í´ë”/íŒŒì¼ ë¬´ì‹œ
            if item.name in ignore_list:
                validation_result["warnings"].append(f"Ignored system file/folder: {item.name}")
                continue
            
            all_items.append(item.name)
            
            if item.name in required_items:
                required_items[item.name] = True
                validation_result["structure"][item.name] = "found"
                validation_result["details"].append(f"âœ“ {item.name}")
            elif item.is_dir() and item.name.startswith("sub-"):
                subject_dirs.append(item.name)
                validation_result["structure"][item.name] = "subject_directory"
                
                # Subject í´ë” ë‚´ë¶€ í™•ì¸
                sub_folders = [f.name for f in item.iterdir() if f.is_dir() and f.name not in ignore_list]
                if sub_folders:
                    validation_result["details"].append(f"âœ“ {item.name}/ â†’ {', '.join(sub_folders)}")
                else:
                    validation_result["warnings"].append(f"{item.name} folder is empty")
        
        # í•„ìˆ˜ í•­ëª© ê²€ì‚¬
        missing_items = [k for k, v in required_items.items() if not v]
        
        if missing_items:
            for item in missing_items:
                validation_result["errors"].append(f"âœ— Missing required file: {item}")
                validation_result["details"].append(f"âœ— {item} (missing)")
        
        # subject í´ë” ê°œìˆ˜ í™•ì¸
        validation_result["subject_count"] = len(subject_dirs)
        validation_result["subject_list"] = subject_dirs[:10]  # ì²˜ìŒ 10ê°œë§Œ
        
        if len(subject_dirs) == 0:
            validation_result["errors"].append("No subject directories found (sub-*)")
        else:
            validation_result["details"].append(f"âœ“ Found {len(subject_dirs)} subject(s)")
        
        # dataset_description.json ê²€ì¦
        desc_file = dir_path / "dataset_description.json"
        if desc_file.exists():
            try:
                with desc_file.open("r", encoding="utf-8") as f:
                    desc_data = json.load(f)
                    required_fields = ["Name", "BIDSVersion"]
                    missing_fields = [f for f in required_fields if f not in desc_data]
                    
                    if missing_fields:
                        validation_result["errors"].append(
                            f"dataset_description.json missing fields: {', '.join(missing_fields)}"
                        )
                    else:
                        validation_result["dataset_info"] = {
                            "name": desc_data.get("Name"),
                            "version": desc_data.get("BIDSVersion"),
                            "dataset_type": desc_data.get("DatasetType", "unknown")
                        }
                        validation_result["details"].append(
                            f"âœ“ Dataset: {desc_data.get('Name')} (BIDS {desc_data.get('BIDSVersion')})"
                        )
            except json.JSONDecodeError as e:
                validation_result["errors"].append(f"dataset_description.json is not valid JSON: {str(e)}")
            except Exception as e:
                validation_result["errors"].append(f"Error reading dataset_description.json: {str(e)}")
        
        # README íŒŒì¼ í™•ì¸
        readme_file = dir_path / "README"
        if readme_file.exists():
            try:
                readme_size = readme_file.stat().st_size
                validation_result["details"].append(f"âœ“ README ({readme_size} bytes)")
            except:
                pass
        
        # participants.tsv í™•ì¸
        participants_file = dir_path / "participants.tsv"
        if participants_file.exists():
            try:
                with participants_file.open("r", encoding="utf-8") as f:
                    lines = f.readlines()
                    validation_result["details"].append(f"âœ“ participants.tsv ({len(lines)} lines)")
                    validation_result["participants_count"] = len(lines) - 1  # í—¤ë” ì œì™¸
            except:
                pass
        
        # ìµœì¢… ê²€ì¦ ê²°ê³¼
        if not validation_result["errors"]:
            validation_result["is_valid"] = True
            validation_result["message"] = "âœ… Valid BIDS dataset!"
        else:
            validation_result["is_valid"] = False
            validation_result["message"] = f"âŒ Invalid BIDS dataset ({len(validation_result['errors'])} error(s))"
        
        return validation_result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

async def run_mica_via_airflow(
    subject_id: str,
    session_id: str,
    processes: list,
    bids_dir: str,
    output_dir: str,
    fs_licence: str,
    threads: int,
    freesurfer: bool,
    user: str,
    proc_func_flags: list | None = None,
    dwi_flags: list | None = None,
    sc_flags: list | None = None,
):
    """Airflow DAGë¥¼ íŠ¸ë¦¬ê±°í•˜ì—¬ MICA Pipelineì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        # DAG Run ID ìƒì„±
        run_id = f"mica_{subject_id.replace('sub-', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Airflow API í˜ì´ë¡œë“œ
        payload = {
            "dag_run_id": run_id,
            "conf": {
                "subject_id": subject_id,
                "session_id": session_id,
                "processes": processes,
                "bids_dir": bids_dir,
                "output_dir": output_dir,
                "fs_licence": fs_licence,
                "threads": threads,
                "freesurfer": freesurfer,
                "user": user,
                "proc_func_flags": proc_func_flags or [],
                "dwi_flags": dwi_flags or [],
                "sc_flags": sc_flags or [],
            }
        }
        
        # Airflow API í˜¸ì¶œ
        response = requests.post(
            f"{AIRFLOW_API}/dags/mica_pipeline/dagRuns",
            json=payload,
            auth=_auth(),
            timeout=10
        )
        
        if response.status_code in (200, 201):
            airflow_data = response.json()
            
            # DBì— ì €ì¥
            session = SessionLocal()
            try:
                container_name = f"{subject_id}"
                if session_id:
                    container_name += f"_ses-{session_id}"
                if processes:
                    container_name += f"_{processes[0]}"
                
                mica_job = MicaPipelineJob(
                    job_id=run_id,
                    subject_id=subject_id,
                    session_id=session_id,
                    processes=",".join(processes),
                    container_name=container_name,
                    pid=None,  # Airflowê°€ ê´€ë¦¬
                    status="processing",
                    progress=0.0,
                    log_file=f"{output_dir}/logs/{processes[0]}/fin/{container_name}.log",
                    error_log_file=f"{output_dir}/logs/{processes[0]}/error/{container_name}_error.log"
                )
                session.add(mica_job)
                session.commit()
            finally:
                session.close()
            
            return {
                "success": True,
                "mode": "airflow",
                "dag_run_id": run_id,
                "subject_id": subject_id,
                "session_id": session_id,
                "processes": processes,
                "user": user,
                "airflow_url": f"{AIRFLOW_BASE}/dags/mica_pipeline/grid?dag_run_id={run_id}",
                "message": f"âœ… MICA Pipelineì´ Airflowë¥¼ í†µí•´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                          f"DAG Run ID: {run_id}\n"
                          f"User: {user}\n"
                          f"Subject: {subject_id}\n\n"
                          f"ğŸ’¡ Airflow UIì—ì„œ ì‹¤í–‰ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”: http://localhost:8080",
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Airflow API failed: {response.text}"
            )
    except requests.exceptions.RequestException as e:
        raise HTTPException(
            status_code=503,
            detail=f"Failed to connect to Airflow: {str(e)}"
        )


@app.post("/run-mica-pipeline")
async def run_mica_pipeline(data: dict):
    """mica-pipeline docker ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (ì§ì ‘ ì‹¤í–‰ ë˜ëŠ” Airflow í†µí•´ ì‹¤í–‰)"""
    try:
        # ì‹¤í–‰ ë°©ì‹ ì„ íƒ
        use_airflow = data.get("use_airflow", False)
        user = data.get("user", "anonymous")
        
        # í˜¸ìŠ¤íŠ¸ì˜ ì‹¤ì œ ë°ì´í„° ê²½ë¡œ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        host_data_dir = os.getenv("HOST_DATA_DIR", "/data")
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
        bids_dir = data.get("bids_dir", "./data/bids")
        output_dir = data.get("output_dir", "./data/derivatives")
        subject_id = data.get("subject_id")
        processes = data.get("processes", [])
        
        # ì¶”ê°€ íŒŒë¼ë¯¸í„°
        session_id = data.get("session_id", "")
        fs_licence = data.get("fs_licence", "./data/license.txt")
        threads = data.get("threads", 4)
        freesurfer = data.get("freesurfer", True)
    
        #í”„ë¡œì„¸ìŠ¤
        proc_func_flags = data.get("proc_func_flags", [])
        dwi_flags = data.get("dwi_flags", [])
        sc_flags = data.get("sc_flags", [])
        
        # Airflowë¥¼ í†µí•œ ì‹¤í–‰
        if use_airflow:
            return await run_mica_via_airflow(
                subject_id=subject_id,
                session_id=session_id,
                processes=processes,
                bids_dir=host_data_dir + "/bids",
                output_dir=host_data_dir + "/derivatives",
                fs_licence=host_data_dir + "/license.txt",
                threads=threads,
                freesurfer=freesurfer,
                user=user,
                proc_func_flags=proc_func_flags,
                dwi_flags=dwi_flags,
                sc_flags=sc_flags,
            )
        def join_tokens(tokens: list[str]) -> str:
        # ê° í† í°ì„ shlex.quoteë¡œ ê°ì‹¸ ì•ˆì „í•˜ê²Œ ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
            return " ".join(shlex.quote(t) for t in tokens if t is not None and str(t) != "")

        # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œë¥¼ í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ë³€í™˜
        def convert_to_host_path(container_path: str) -> str:
            """ì»¨í…Œì´ë„ˆ ê²½ë¡œë¥¼ í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ë³€í™˜"""
            if container_path.startswith("/app/data"):
                return container_path.replace("/app/data", host_data_dir)
            return container_path
        
        # í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ë³€í™˜
        host_bids_dir = convert_to_host_path(bids_dir)
        host_output_dir = convert_to_host_path(output_dir)
        host_fs_licence = convert_to_host_path(fs_licence)
        
        if not subject_id:
            raise HTTPException(status_code=400, detail="subject_id is required")
        
        if not processes:
            raise HTTPException(status_code=400, detail="At least one process must be selected")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # ì „ì²´ Subject ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
        if subject_id.lower() == "all":
            # BIDS ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  subject ì°¾ê¸°
            bids_path = Path(bids_dir)
            if not bids_path.exists():
                raise HTTPException(status_code=404, detail=f"BIDS directory not found: {bids_dir}")
            
            subjects = [d.name for d in bids_path.iterdir() 
                       if d.is_dir() and d.name.startswith("sub-") and d.name != "__MACOSX"]
            
            if not subjects:
                raise HTTPException(status_code=400, detail="No subjects found in BIDS directory")
            
            # ì „ì²´ subject ìˆœì°¨ ì‹¤í–‰
            all_results = []
            total_success = 0
            total_failed = 0
            
            for sub in subjects:
                try:
                    # subject IDì—ì„œ "sub-" ì œê±°
                    sub_id = sub.replace("sub-", "")
                    
                    # Session ìë™ ê°ì§€ (session_idê°€ ì—†ì„ ë•Œ)
                    sessions_to_process = []
                    if session_id:
                        # ëª…ì‹œì ìœ¼ë¡œ sessionì´ ì§€ì •ëœ ê²½ìš°
                        sessions_to_process = [session_id]
                    else:
                        # Session ìë™ ê°ì§€
                        subject_path = Path(bids_dir) / sub
                        if subject_path.exists():
                            # ses-* ë””ë ‰í† ë¦¬ ì°¾ê¸°
                            session_dirs = [d.name.replace("ses-", "") for d in subject_path.iterdir() 
                                          if d.is_dir() and d.name.startswith("ses-")]
                            if session_dirs:
                                sessions_to_process = session_dirs
                            else:
                                # sessionì´ ì—†ëŠ” ê²½ìš° (ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬)
                                sessions_to_process = [""]
                        else:
                            sessions_to_process = [""]
                    
                    # ê° sessionë³„ë¡œ ì‹¤í–‰
                    for ses in sessions_to_process:
                        # ì»¨í…Œì´ë„ˆ ì´ë¦„ ìƒì„±
                        container_name = f"{sub}"
                        if ses:
                            container_name += f"_ses-{ses}"
                        if processes:
                            container_name += f"_{processes[0]}"
                        
                        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± (í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ìƒì„± - micapipeê°€ ì ‘ê·¼ ê°€ëŠ¥)
                        host_log_dir = Path(host_output_dir) / "logs" / processes[0] if processes else Path(host_output_dir) / "logs"
                        
                        # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ë””ë ‰í† ë¦¬ ìƒì„± (ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¥¼ í†µí•´)
                        container_log_dir = Path(output_dir) / "logs" / processes[0] if processes else Path(output_dir) / "logs"
                        container_log_dir.mkdir(parents=True, exist_ok=True)
                        (container_log_dir / "fin").mkdir(exist_ok=True)
                        (container_log_dir / "error").mkdir(exist_ok=True)
                        
                        # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ - shell redirectionìš©)
                        container_log_file = container_log_dir / "fin" / f"{container_name}.log"
                        container_error_log_file = container_log_dir / "error" / f"{container_name}_error.log"
                        
                        # í”„ë¡œì„¸ìŠ¤ í”Œë˜ê·¸ (í•˜ì´í”ˆ í•˜ë‚˜)
                        base_switches = [f"-{p}" for p in processes]
                        extra_tokens = (proc_func_flags or []) + (dwi_flags or []) + (sc_flags or [])
                        process_flags = join_tokens(base_switches + extra_tokens)

                        
                        # FreeSurfer ë¼ì´ì„¼ìŠ¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ì»¨í…Œì´ë„ˆ ê²½ë¡œë¡œ)
                        fs_licence_mount = ""
                        if Path(fs_licence).exists():
                            fs_licence_mount = f"-v {host_fs_licence}:{host_fs_licence}"
                        
                        # micapipe ëª…ë ¹ì–´ ìƒì„± (í˜¸ìŠ¤íŠ¸ ê²½ë¡œ ì‚¬ìš©)
                        cmd = f"docker run --rm --name {container_name} " \
                              f"-v {host_bids_dir}:{host_bids_dir} " \
                              f"-v {host_output_dir}:{host_output_dir} "
                        
                        if fs_licence_mount:
                            cmd += f"{fs_licence_mount} "
                        
                        cmd += f"micalab/micapipe:v0.2.3 " \
                               f"-bids {host_bids_dir} " \
                               f"-out {host_output_dir} " \
                               f"-sub {sub_id} "
                        
                        if ses:  # ses ë³€ìˆ˜ ì‚¬ìš© (ìë™ ê°ì§€ëœ session)
                            cmd += f"-ses {ses} "
                        
                        if fs_licence_mount:
                            cmd += f"-fs_licence {host_fs_licence} "
                        
                        cmd += f"-threads {threads} " \
                               f"{process_flags} " \
                               f"-freesurfer {'TRUE' if freesurfer else 'FALSE'} " \
                               f"> {container_log_file} 2> {container_error_log_file}"
                        
                        # ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
                        process = subprocess.Popen(
                            cmd,
                            shell=True,
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            text=True
                        )
                        
                        # DBì— ë¡œê·¸ ì €ì¥ (ì‹œì‘ ì‹œì  ê¸°ë¡)
                        db_session = SessionLocal()
                        try:
                            # CommandLog ì €ì¥
                            log = CommandLog(
                                command=cmd,
                                output=f"Container started: {container_name} (PID: {process.pid})",
                                error=""
                            )
                            db_session.add(log)
                            
                            # MicaPipelineJob ì €ì¥
                            mica_job = MicaPipelineJob(
                                job_id=container_name,
                                subject_id=sub,
                                session_id=ses if ses else None,
                                processes=",".join(processes),
                                container_name=container_name,
                                pid=process.pid,
                                status="processing",
                                progress=0.0,
                                log_file=str(container_log_file),
                                error_log_file=str(container_error_log_file)
                            )
                            db_session.add(mica_job)
                            db_session.commit()
                        finally:
                            db_session.close()
                        
                        # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ì´ë¯€ë¡œ ì¼ë‹¨ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                        total_success += 1
                        
                        subject_with_session = f"{sub}" + (f"_ses-{ses}" if ses else "")
                        all_results.append({
                            "subject": subject_with_session,
                            "success": True,
                            "container_name": container_name,
                            "pid": process.pid,
                            "job_id": container_name,
                            "message": "ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë¨"
                        })
                    
                except Exception as e:
                    total_failed += 1
                    all_results.append({
                        "subject": sub,
                        "success": False,
                        "message": f"ì‹œì‘ ì‹¤íŒ¨: {str(e)}"
                    })
            
            return {
                "success": True,
                "mode": "all_subjects",
                "total_subjects": len(subjects),
                "successful": total_success,
                "failed": total_failed,
                "results": all_results,
                "processes": processes,
                "timestamp": datetime.now().isoformat(),
                "message": f"âœ… {total_success}ê°œì˜ ì»¨í…Œì´ë„ˆê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. (ì‹¤íŒ¨: {total_failed}ê°œ)\n\nğŸ’¡ 'ë¡œê·¸ í™•ì¸' íƒ­ì—ì„œ ì‹¤í–‰ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."
            }
        
        else:
            # ë‹¨ì¼ Subject ì‹¤í–‰
            # subject IDì—ì„œ "sub-" ì œê±°
            sub_id = subject_id.replace("sub-", "")
            
            # Session ìë™ ê°ì§€ (session_idê°€ ì—†ì„ ë•Œ)
            actual_session = session_id
            if not session_id:
                # Session ìë™ ê°ì§€
                subject_path = Path(bids_dir) / subject_id
                if subject_path.exists():
                    # ses-* ë””ë ‰í† ë¦¬ ì°¾ê¸°
                    session_dirs = [d.name.replace("ses-", "") for d in subject_path.iterdir() 
                                  if d.is_dir() and d.name.startswith("ses-")]
                    if session_dirs:
                        # ì²« ë²ˆì§¸ session ìë™ ì„ íƒ
                        actual_session = session_dirs[0]
            
            # ì»¨í…Œì´ë„ˆ ì´ë¦„ ìƒì„±
            container_name = f"{subject_id}"
            if actual_session:
                container_name += f"_ses-{actual_session}"
            if processes:
                container_name += f"_{processes[0]}"
            
            # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± (í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ìƒì„± - micapipeê°€ ì ‘ê·¼ ê°€ëŠ¥)
            host_log_dir = Path(host_output_dir) / "logs" / processes[0] if processes else Path(host_output_dir) / "logs"
            
            # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ë””ë ‰í† ë¦¬ ìƒì„± (ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¥¼ í†µí•´)
            container_log_dir = Path(output_dir) / "logs" / processes[0] if processes else Path(output_dir) / "logs"
            container_log_dir.mkdir(parents=True, exist_ok=True)
            (container_log_dir / "fin").mkdir(exist_ok=True)
            (container_log_dir / "error").mkdir(exist_ok=True)
            
            # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ - shell redirectionìš©)
            container_log_file = container_log_dir / "fin" / f"{container_name}.log"
            container_error_log_file = container_log_dir / "error" / f"{container_name}_error.log"
            
            # í”„ë¡œì„¸ìŠ¤ í”Œë˜ê·¸ (í•˜ì´í”ˆ í•˜ë‚˜)
            base_switches = [f"-{p}" for p in processes]
            extra_tokens = (proc_func_flags or []) + (dwi_flags or []) + (sc_flags or [])
            process_flags = join_tokens(base_switches + extra_tokens)

            # FreeSurfer ë¼ì´ì„¼ìŠ¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ì»¨í…Œì´ë„ˆ ê²½ë¡œë¡œ)
            fs_licence_mount = ""
            if Path(fs_licence).exists():
                fs_licence_mount = f"-v {host_fs_licence}:{host_fs_licence}"
            
            # micapipe ëª…ë ¹ì–´ ìƒì„± (í˜¸ìŠ¤íŠ¸ ê²½ë¡œ ì‚¬ìš©)
            cmd = f"docker run --rm --name {container_name} " \
                  f"-v {host_bids_dir}:{host_bids_dir} " \
                  f"-v {host_output_dir}:{host_output_dir} "
            
            if fs_licence_mount:
                cmd += f"{fs_licence_mount} "
            
            cmd += f"micalab/micapipe:v0.2.3 " \
                   f"-bids {host_bids_dir} " \
                   f"-out {host_output_dir} " \
                   f"-sub {sub_id} "
            
            if actual_session:  # ìë™ ê°ì§€ëœ session ì‚¬ìš©
                cmd += f"-ses {actual_session} "
            
            if fs_licence_mount:
                cmd += f"-fs_licence {host_fs_licence} "
            
            cmd += f"-threads {threads} " \
                   f"{process_flags} " \
                   f"-freesurfer {'TRUE' if freesurfer else 'FALSE'} " \
                   f"> {container_log_file} 2> {container_error_log_file}"
            
            # ëª…ë ¹ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰í•˜ê³  ì¦‰ì‹œ ë°˜í™˜)
            process = subprocess.Popen(
                cmd,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # DBì— ë¡œê·¸ ì €ì¥ (ì‹œì‘ ì‹œì  ê¸°ë¡)
            session = SessionLocal()
            try:
                # CommandLog ì €ì¥
                log = CommandLog(
                    command=cmd,
                    output=f"Container started: {container_name} (PID: {process.pid})",
                    error=""
                )
                session.add(log)
                
                # MicaPipelineJob ì €ì¥
                mica_job = MicaPipelineJob(
                    job_id=container_name,
                    subject_id=subject_id,
                    session_id=actual_session,
                    processes=",".join(processes),
                    container_name=container_name,
                    pid=process.pid,
                    status="processing",
                    progress=0.0,
                    log_file=str(container_log_file),
                    error_log_file=str(container_error_log_file)
                )
                session.add(mica_job)
                session.commit()
            finally:
                session.close()
            
            return {
                "success": True,
                "mode": "single_subject",
                "command": cmd,
                "output": f"âœ… MICA Pipelineì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\nì»¨í…Œì´ë„ˆ: {container_name}\nPID: {process.pid}\n\nğŸ’¡ 'ë¡œê·¸ í™•ì¸' íƒ­ ë˜ëŠ” 'Download Results'ì—ì„œ ì‹¤í–‰ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "error": "",
                "returncode": 0,
                "subject_id": subject_id,
                "session_id": actual_session,
                "session_auto_detected": bool(actual_session and not session_id),
                "processes": processes,
                "container_name": container_name,
                "pid": process.pid,
                "job_id": container_name,
                "timestamp": datetime.now().isoformat()
            }
            
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": f"Command timeout after {data.get('timeout', 3600)} seconds",
            "returncode": -1
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/mica-logs")
async def get_mica_logs(output_dir: str = "/app/data/derivatives"):
    """MICA Pipeline ë¡œê·¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        logs_dir = Path(output_dir) / "logs"
        
        if not logs_dir.exists():
            return {
                "success": True,
                "logs": [],
                "message": "ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        logs = []
        
        # ê° í”„ë¡œì„¸ìŠ¤ ë””ë ‰í† ë¦¬ ìˆœíšŒ
        for process_dir in logs_dir.iterdir():
            if not process_dir.is_dir():
                continue
            
            process_name = process_dir.name
            
            # fin ë””ë ‰í† ë¦¬ì˜ ë¡œê·¸ íŒŒì¼
            fin_dir = process_dir / "fin"
            error_dir = process_dir / "error"
            
            if fin_dir.exists():
                for log_file in fin_dir.iterdir():
                    if log_file.is_file() and log_file.suffix == ".log":
                        error_file = error_dir / f"{log_file.stem}_error.log"
                        
                        logs.append({
                            "process": process_name,
                            "subject": log_file.stem,
                            "log_file": str(log_file),
                            "error_file": str(error_file) if error_file.exists() else None,
                            "size": log_file.stat().st_size,
                            "modified": log_file.stat().st_mtime,
                            "has_error": error_file.exists() and error_file.stat().st_size > 0
                        })
        
        # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        logs.sort(key=lambda x: x["modified"], reverse=True)
        
        return {
            "success": True,
            "logs": logs,
            "count": len(logs)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/mica-log-content")
async def get_mica_log_content(log_file: str, lines: int = 100):
    """MICA Pipeline ë¡œê·¸ íŒŒì¼ ë‚´ìš©ì„ ì½ìŠµë‹ˆë‹¤."""
    try:
        log_path = Path(log_file)
        
        # ë³´ì•ˆ: /app/data ì™¸ë¶€ ê²½ë¡œ ì ‘ê·¼ ì°¨ë‹¨
        if not str(log_path).startswith("/app/data"):
            raise HTTPException(status_code=403, detail="Access denied")
        
        if not log_path.exists():
            raise HTTPException(status_code=404, detail="Log file not found")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = log_path.stat().st_size
        
        # ë§ˆì§€ë§‰ Nì¤„ ì½ê¸°
        with log_path.open("r", encoding="utf-8", errors="ignore") as f:
            all_lines = f.readlines()
            content_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
        
        return {
            "success": True,
            "file": str(log_path),
            "size": file_size,
            "total_lines": len(all_lines),
            "returned_lines": len(content_lines),
            "content": "".join(content_lines)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/mica-containers")
async def get_mica_containers():
    """ì‹¤í–‰ ì¤‘ì¸ micapipe ì»¨í…Œì´ë„ˆ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        # docker ps ëª…ë ¹ ì‹¤í–‰
        result = subprocess.run(
            "docker ps --filter 'name=sub-' --format '{{.Names}}\t{{.Status}}\t{{.Image}}\t{{.RunningFor}}'",
            shell=True,
            capture_output=True,
            text=True,
            timeout=10
        )
        
        containers = []
        if result.stdout.strip():
            for line in result.stdout.strip().split('\n'):
                parts = line.split('\t')
                if len(parts) >= 4:
                    containers.append({
                        "name": parts[0],
                        "status": parts[1],
                        "image": parts[2],
                        "running_for": parts[3]
                    })
        
        return {
            "success": True,
            "containers": containers,
            "count": len(containers)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/mica-container-stop")
async def stop_mica_container(container_name: str):
    """micapipe ì»¨í…Œì´ë„ˆë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
    try:
        # ë³´ì•ˆ: sub- ë¡œ ì‹œì‘í•˜ëŠ” ì»¨í…Œì´ë„ˆë§Œ ì¢…ë£Œ ê°€ëŠ¥
        if not container_name.startswith("sub-"):
            raise HTTPException(status_code=403, detail="Only sub-* containers can be stopped")
        
        # docker stop ëª…ë ¹ ì‹¤í–‰
        result = subprocess.run(
            f"docker stop {container_name}",
            shell=True,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        return {
            "success": result.returncode == 0,
            "container": container_name,
            "message": f"Container {container_name} stopped" if result.returncode == 0 else "Failed to stop container",
            "output": result.stdout,
            "error": result.stderr
        }
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Timeout while stopping container"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/mica-jobs")
async def get_mica_jobs(status: str = None):
    """MICA Pipeline Job ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        db = SessionLocal()
        try:
            query = db.query(MicaPipelineJob)
            
            # ìƒíƒœ í•„í„°ë§
            if status:
                query = query.filter(MicaPipelineJob.status == status)
            
            jobs = query.order_by(MicaPipelineJob.started_at.desc()).all()
            
            # ì‹¤ì‹œê°„ìœ¼ë¡œ ì»¨í…Œì´ë„ˆ/Airflow ìƒíƒœ í™•ì¸ ë° ì—…ë°ì´íŠ¸
            for job in jobs:
                if job.status == "processing":
                    # Airflowë¡œ ì‹¤í–‰ëœ jobì¸ì§€ í™•ì¸ (job_idê°€ "mica_"ë¡œ ì‹œì‘)
                    if job.job_id.startswith("mica_"):
                        # Airflow DAG Run ìƒíƒœ í™•ì¸
                        try:
                            airflow_response = requests.get(
                                f"{AIRFLOW_API}/dags/mica_pipeline/dagRuns/{job.job_id}",
                                auth=_auth(),
                                timeout=5
                            )
                            if airflow_response.status_code == 200:
                                airflow_data = airflow_response.json()
                                airflow_state = airflow_data.get("state")
                                
                                if airflow_state in ["success", "failed"]:
                                    job.status = "completed" if airflow_state == "success" else "failed"
                                    job.completed_at = datetime.utcnow()
                                    job.progress = 100.0
                                    
                                    if airflow_state == "failed":
                                        # Airflow ë¡œê·¸ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ì¶œ (ê°„ë‹¨íˆ ì²˜ë¦¬)
                                        job.error_message = "Airflow DAG execution failed. Check Airflow UI for details."
                                    
                                    db.commit()
                        except Exception as e:
                            print(f"Failed to check Airflow status: {e}")
                    
                    # ì§ì ‘ ì‹¤í–‰ëœ jobì˜ ê²½ìš° Docker ì»¨í…Œì´ë„ˆ í™•ì¸
                    else:
                        result = subprocess.run(
                            f"docker inspect {job.container_name}",
                            shell=True,
                            capture_output=True,
                            text=True,
                            timeout=5
                        )
                        
                        if result.returncode != 0:
                            # ì»¨í…Œì´ë„ˆê°€ ì—†ìŒ = ì™„ë£Œ ë˜ëŠ” ì‹¤íŒ¨
                            # ë¡œê·¸ íŒŒì¼ì—ì„œ ì—ëŸ¬ í™•ì¸
                            has_error = False
                            error_message = None
                            
                            # 1. ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ í™•ì¸
                            error_log_path = Path(job.error_log_file) if job.error_log_file else None
                            if error_log_path and error_log_path.exists():
                                error_size = error_log_path.stat().st_size
                                if error_size > 0:
                                    has_error = True
                                    with open(error_log_path, 'r', encoding='utf-8', errors='ignore') as f:
                                        error_message = f.read()[-500:]
                            
                            # 2. í‘œì¤€ ì¶œë ¥ ë¡œê·¸ì—ì„œ "[ ERROR ]" í™•ì¸ (MICA Pipelineì€ exit 0ë¡œ ì¢…ë£Œí•´ë„ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥)
                            if not has_error:
                                log_path = Path(job.log_file) if job.log_file else None
                                if log_path and log_path.exists():
                                    with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
                                        log_content = f.read()
                                        if "[ ERROR ]" in log_content or "ERROR" in log_content:
                                            has_error = True
                                            # ì—ëŸ¬ ë¶€ë¶„ ì¶”ì¶œ
                                            error_lines = [line for line in log_content.split('\n') if 'ERROR' in line]
                                            error_message = '\n'.join(error_lines[-5:]) if error_lines else log_content[-500:]
                            
                            # ìƒíƒœ ì—…ë°ì´íŠ¸
                            job.status = "failed" if has_error else "completed"
                            job.completed_at = datetime.utcnow()
                            job.progress = 100.0
                            if has_error:
                                job.error_message = error_message
                            db.commit()
            
            # JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            jobs_data = []
            for job in jobs:
                jobs_data.append({
                    "id": job.id,
                    "job_id": job.job_id,
                    "subject_id": job.subject_id,
                    "session_id": job.session_id,
                    "processes": job.processes,
                    "container_name": job.container_name,
                    "pid": job.pid,
                    "status": job.status,
                    "progress": job.progress,
                    "log_file": job.log_file,
                    "error_log_file": job.error_log_file,
                    "started_at": job.started_at.isoformat() if job.started_at else None,
                    "completed_at": job.completed_at.isoformat() if job.completed_at else None,
                    "error_message": job.error_message,
                    "duration": (job.completed_at - job.started_at).total_seconds() if job.completed_at else None
                })
            
            return {
                "success": True,
                "jobs": jobs_data,
                "count": len(jobs_data),
                "summary": {
                    "processing": sum(1 for j in jobs if j.status == "processing"),
                    "completed": sum(1 for j in jobs if j.status == "completed"),
                    "failed": sum(1 for j in jobs if j.status == "failed")
                }
            }
        finally:
            db.close()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/mica-job-update")
async def update_mica_job_status(data: dict):
    """MICA Pipeline Job ìƒíƒœë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        job_id = data.get("job_id")
        status = data.get("status")
        progress = data.get("progress")
        error_message = data.get("error_message")
        
        if not job_id:
            raise HTTPException(status_code=400, detail="job_id is required")
        
        db = SessionLocal()
        try:
            job = db.query(MicaPipelineJob).filter(MicaPipelineJob.job_id == job_id).first()
            
            if not job:
                raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
            
            if status:
                job.status = status
                if status in ["completed", "failed"]:
                    job.completed_at = datetime.utcnow()
                    job.progress = 100.0
            
            if progress is not None:
                job.progress = progress
            
            if error_message:
                job.error_message = error_message
            
            db.commit()
            
            return {
                "success": True,
                "job_id": job_id,
                "status": job.status,
                "message": "Job status updated"
            }
        finally:
            db.close()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# --- ìœ í‹¸: ë°ì´í„° ë£¨íŠ¸ ê²°ì • (/app/data ê¸°ë³¸) ---
def pick_existing_data_root() -> Path:
    root = Path(os.getenv("HOST_DATA_DIR", "/app/data"))
    if not (root / "derivatives").exists():
        raise HTTPException(status_code=404, detail=f"Derivatives not found: {root / 'derivatives'}")
    return root

# --- ë³´ì•ˆìš©: derivatives ë°”ê¹¥ ì ‘ê·¼ ë°©ì§€ ---
def _ensure_inside(root: Path, target: Path) -> Path:
    root_r = root.resolve()
    target_r = target.resolve()
    if not str(target_r).startswith(str(root_r)):
        raise HTTPException(status_code=403, detail="Path traversal detected")
    return target_r

# --- ì „ì²´ derivativesë¥¼ ZIPìœ¼ë¡œ ë°˜í™˜ ---
@app.get("/download-derivatives")
def download_derivatives():
    data_root = pick_existing_data_root()
    derivatives_root = (data_root / "derivatives").resolve()

    if not derivatives_root.exists():
        raise HTTPException(status_code=404, detail=f"Target not found: {derivatives_root}")

    # ZIP íŒŒì¼ ê²½ë¡œ ì¤€ë¹„
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"derivatives_all_{ts}.zip"
    tmp_dir = Path("/app/tmp")
    tmp_dir.mkdir(parents=True, exist_ok=True)
    zip_path = tmp_dir / zip_name

    # ZIP ìƒì„±
    import zipfile
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
        for p in derivatives_root.rglob("*"):
            if p.is_file():
                arcname = p.relative_to(derivatives_root)
                zf.write(p, arcname=str(arcname))

    return FileResponse(
        path=str(zip_path),
        filename=zip_name,
        media_type="application/zip",
    )