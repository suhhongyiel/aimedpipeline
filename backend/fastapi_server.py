import os
import uuid
import requests
from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime
import shutil
import json
from typing import List
import zipfile
import tarfile
import time
# tokenize for shell command ì•ˆì „ ì²˜ë¦¬
import shlex
# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
from system_resources import get_system_resources
# db ê´€ë ¨
# --- DB ê´€ë ¨ import ìˆ˜ì • ---
from database import SessionLocal
from models import JobLog, CommandLog, MicaPipelineJob
import models
from database import engine
from sqlalchemy.orm import Session
from fastapi import Depends

# ===== í™˜ê²½ë³€ìˆ˜ ê¸°ë³¸ê°’ =====
AIRFLOW_BASE = os.getenv("AIRFLOW_BASE_URL", "http://airflow:8080")
#AIRFLOW_BASE = os.getenv("AIRFLOW_BASE_URL", "http://localhost:8080")
AIRFLOW_API  = f"{AIRFLOW_BASE}/api/v1"
AIRFLOW_DAG  = os.getenv("AIRFLOW_DAG_ID", "mica_pipeline")
AIRFLOW_USER = os.getenv("AIRFLOW_USER", "admin")
AIRFLOW_PASS = os.getenv("AIRFLOW_PASSWORD", "admin")

# --- ì„œë²„ ì‹œì‘ ì‹œ DB í…Œì´ë¸” ìƒì„± ---
# ì´ë¯¸ ìˆìœ¼ë©´ ë³„ ì‘ì—… ì•ˆí•¨
models.Base.metadata.create_all(bind=engine)
# --------------------------------
app = FastAPI(title="AIMed Pipeline Backend")

# Streamlit(ë³„ë„ ì»¨í…Œì´ë„ˆ)ì—ì„œ í˜¸ì¶œí•˜ë¯€ë¡œ CORS í—ˆìš©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

def _auth():
    return (AIRFLOW_USER, AIRFLOW_PASS)

# db session í•¨ìˆ˜
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@app.on_event("startup")
def on_startup():
    models.Base.metadata.create_all(bind=engine)

@app.get("/")
def root():
    return {"ok": True, "airflow": AIRFLOW_BASE, "dag": AIRFLOW_DAG}

@app.post("/run-job")
def run_job(job_type: str = "MRI ë¶„ì„", db: Session = Depends(get_db)):
    run_id = f"ui_{uuid.uuid4().hex[:8]}"
    payload = {"dag_run_id": run_id, "conf": {"job_type": job_type}}
    # Airflow mock (ì‹¤ì œ Airflow ì—†ì´)
    try:
        r = requests.post(f"{AIRFLOW_API}/dags/{AIRFLOW_DAG}/dagRuns", json=payload, auth=_auth(), timeout=2)
        if r.status_code not in (200, 201):
            raise HTTPException(status_code=r.status_code, detail=r.text)
    except Exception as e:
        print(f"[Mock] Airflow í˜¸ì¶œ ì‹¤íŒ¨, í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¬´ì‹œ: {e}")
        
    # ---- ë¡œê·¸ DBì— ì €ì¥ ----
    log = JobLog(
        job_id=run_id,
        job_type=job_type,
        status="requested",
        log="Job requested (mock)"
    )
    db.add(log)
    db.commit()
    return {"job_id": run_id, "note": "Airflow ë¯¸ì—°ë™ mock"}

@app.get("/job-status/{job_id}")
def job_status(job_id: str):
    try:
        # ì‹¤ì œ Airflow REST API í˜¸ì¶œ ë¶€ë¶„
        dr = requests.get(f"{AIRFLOW_API}/dags/{AIRFLOW_DAG}/dagRuns/{job_id}", auth=_auth(), timeout=2)
        if dr.status_code != 200:
            raise HTTPException(status_code=dr.status_code, detail=dr.text)
        drj = dr.json()
        state = drj.get("state", "unknown")

        tis = requests.get(
            f"{AIRFLOW_API}/dags/{AIRFLOW_DAG}/dagRuns/{job_id}/taskInstances",
            auth=_auth(), timeout=2
        )
        if tis.status_code != 200:
            raise HTTPException(status_code=tis.status_code, detail=tis.text)
        tasks = []
        for ti in tis.json().get("task_instances", []):
            tasks.append({
                "task_id": ti["task_id"],
                "state": ti["state"],
                "start_date": ti.get("start_date"),
                "end_date": ti.get("end_date"),
                "try_number": ti.get("try_number"),
            })

        total = max(len(tasks), 1)
        success = sum(1 for t in tasks if (t["state"] or "").lower() == "success")
        running = sum(1 for t in tasks if (t["state"] or "").lower() in ("queued", "running"))
        progress = int((success/total)*100 + (running/total)*25)
        progress = min(progress, 99 if state.lower() not in ("success", "failed") else 100)

        ui_url = f"{AIRFLOW_BASE}/dag/{AIRFLOW_DAG}/grid?dag_run_id={job_id}"
        return {
            "status": state, "tasks": tasks, "progress": progress,
            "airflow_ui": ui_url, "log": f"DagRun {job_id} is {state}"
        }
    except Exception as e:
        # Airflow ìš”ì²­ ì‹¤íŒ¨ ì‹œ mock ë°ì´í„° ë°˜í™˜ (ì—ëŸ¬ ì•ˆ ë‚˜ê²Œ í•¨)
        print(f"[Mock] Airflow ìƒíƒœì¡°íšŒ ì‹¤íŒ¨, ë”ë¯¸ê°’ ë°˜í™˜: {e}")
        return {
            "status": "mocked",
            "tasks": [{"task_id": "mocked_task", "state": "success"}],
            "progress": 100,
            "airflow_ui": "mock_url",
            "log": f"Mock DagRun {job_id} is mocked"
        }
    
@app.post("/run-command")
async def run_command(data: dict):
    """ì„œë²„ì—ì„œ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    cmd = data.get("cmd")
    if not cmd:
        raise HTTPException(status_code=400, detail="Command is required")
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì • (ë³¼ë¥¨ ë§ˆìš´íŠ¸ëœ ê²½ë¡œ ì‚¬ìš©)
    work_dir = data.get("work_dir", "/app/workspace")
    work_dir = os.path.abspath(work_dir) if not work_dir.startswith("/") else work_dir
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(work_dir, exist_ok=True)
    
    start_time = datetime.now()
    try:
        result = subprocess.run(
            cmd, 
            shell=True, 
            capture_output=True, 
            text=True,
            cwd=work_dir,
            timeout=data.get("timeout", 300)  # ê¸°ë³¸ 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # DBì— ë¡œê·¸ ì €ì¥
        session = SessionLocal()
        try:
            log = CommandLog(
                command=cmd,
                output=result.stdout,
                error=result.stderr
            )
            session.add(log)
            session.commit()
        finally:
            session.close()
        
        return {
            "success": result.returncode == 0,
            "output": result.stdout,
            "error": result.stderr,
            "returncode": result.returncode,
            "work_dir": work_dir,
            "duration": f"{duration:.2f}s",
            "timestamp": datetime.now().isoformat()
        }
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "output": "",
            "error": f"Command timeout after {data.get('timeout', 300)} seconds",
            "returncode": -1,
            "work_dir": work_dir,
            "duration": None,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "success": False,
            "output": "",
            "error": str(e),
            "returncode": -1,
            "work_dir": work_dir,
            "duration": None,
            "timestamp": datetime.now().isoformat()
        }

@app.get("/list-files")
async def list_files(path: str = "/app/workspace"):
    """ì§€ì •ëœ ê²½ë¡œì˜ íŒŒì¼ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        path_obj = Path(path)
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail=f"Path not found: {path}")
        
        files = []
        for item in path_obj.iterdir():
            try:
                stat = item.stat()
                files.append({
                    "name": item.name,
                    "path": str(item),
                    "type": "directory" if item.is_dir() else "file",
                    "size": stat.st_size if item.is_file() else None,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "permissions": oct(stat.st_mode)[-3:]
                })
            except PermissionError:
                continue
        
        # ì´ë¦„ìˆœ ì •ë ¬
        files.sort(key=lambda x: (x["type"] == "file", x["name"]))
        
        return {
            "path": path,
            "files": files,
            "count": len(files)
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.post("/create-file")
async def create_file(data: dict):
    """íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    file_path = data.get("file_path")
    content = data.get("content", "")
    work_dir = data.get("work_dir", "/app/workspace")
    
    if not file_path:
        raise HTTPException(status_code=400, detail="file_path is required")
    
    # ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹ˆë©´ work_dir ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
    if not file_path.startswith("/"):
        file_path = os.path.join(work_dir, file_path)
    
    try:
        path_obj = Path(file_path)
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        path_obj.parent.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ ìƒì„±
        path_obj.write_text(content, encoding="utf-8")
        
        return {
            "success": True,
            "file_path": str(path_obj),
            "size": path_obj.stat().st_size,
            "message": f"File created: {file_path}"
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.delete("/delete-file")
async def delete_file(file_path: str, work_dir: str = "/app/workspace"):
    """íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    if not file_path:
        raise HTTPException(status_code=400, detail="file_path is required")
    
    # ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹ˆë©´ work_dir ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
    if not file_path.startswith("/"):
        file_path = os.path.join(work_dir, file_path)
    
    try:
        path_obj = Path(file_path)
        
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail=f"Path not found: {file_path}")
        
        # ì•ˆì „ì¥ì¹˜: ì‘ì—… ë””ë ‰í† ë¦¬ ì™¸ë¶€ ì‚­ì œ ë°©ì§€
        if not str(path_obj.resolve()).startswith("/app"):
            raise HTTPException(status_code=403, detail="Cannot delete files outside /app directory")
        
        if path_obj.is_dir():
            import shutil
            shutil.rmtree(path_obj)
            item_type = "directory"
        else:
            path_obj.unlink()
            item_type = "file"
        
        return {
            "success": True,
            "file_path": str(path_obj),
            "type": item_type,
            "message": f"{item_type.capitalize()} deleted: {file_path}"
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.get("/read-file")
async def read_file(file_path: str, work_dir: str = "/app/workspace"):
    """íŒŒì¼ ë‚´ìš©ì„ ì½ìŠµë‹ˆë‹¤."""
    if not file_path:
        raise HTTPException(status_code=400, detail="file_path is required")
    
    # ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹ˆë©´ work_dir ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
    if not file_path.startswith("/"):
        file_path = os.path.join(work_dir, file_path)
    
    try:
        path_obj = Path(file_path)
        
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail=f"File not found: {file_path}")
        
        if not path_obj.is_file():
            raise HTTPException(status_code=400, detail=f"Path is not a file: {file_path}")
        
        # ì•ˆì „ì¥ì¹˜: ì‘ì—… ë””ë ‰í† ë¦¬ ì™¸ë¶€ íŒŒì¼ ì½ê¸° ë°©ì§€
        if not str(path_obj.resolve()).startswith("/app"):
            raise HTTPException(status_code=403, detail="Cannot read files outside /app directory")
        
        content = path_obj.read_text(encoding="utf-8")
        stat = path_obj.stat()
        
        return {
            "file_path": str(path_obj),
            "content": content,
            "size": stat.st_size,
            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.post("/upload-file")
async def upload_file(
    files: List[UploadFile] = File(...),
    destination: str = Form("/app/data"),
    extract_archives: bool = Form(True)
):
    """
    íŒŒì¼ì„ ì„œë²„ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    ì••ì¶• íŒŒì¼(.zip, .tar.gz, .tgz)ì€ ìë™ìœ¼ë¡œ ì••ì¶• í•´ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì‚¬ìš©ìë³„ ê²½ë¡œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
    """
    try:
        # ëª©ì ì§€ ë””ë ‰í† ë¦¬ ìƒì„± (ì‚¬ìš©ìë³„ ê²½ë¡œ ìë™ ìƒì„±)
        dest_path = Path(destination)
        
        # ì‚¬ìš©ìë³„ ê²½ë¡œì¸ ê²½ìš° (ì˜ˆ: /app/data/{username}/bids) ìƒìœ„ ë””ë ‰í† ë¦¬ë„ ìƒì„±
        if "/data/" in str(dest_path) and dest_path.parts[-1] in ["bids", "derivatives"]:
            # ìƒìœ„ ë””ë ‰í† ë¦¬(ì‚¬ìš©ì ë””ë ‰í† ë¦¬)ë„ ìƒì„±
            dest_path.parent.mkdir(parents=True, exist_ok=True)
        
        dest_path.mkdir(parents=True, exist_ok=True)
        
        uploaded_files = []
        extracted_files = []
        total_size = 0
        
        for file in files:
            # íŒŒì¼ ì €ì¥
            file_path = dest_path / file.filename
            
            # íŒŒì¼ ì“°ê¸°
            with file_path.open("wb") as buffer:
                content = await file.read()
                buffer.write(content)
            
            file_size = file_path.stat().st_size
            total_size += file_size
            
            file_info = {
                "filename": file.filename,
                "path": str(file_path),
                "size": file_size,
                "content_type": file.content_type,
                "extracted": False
            }
            
            # ì••ì¶• íŒŒì¼ ìë™ ì••ì¶• í•´ì œ
            if extract_archives:
                extracted = False
                
                # ZIP íŒŒì¼ ì²˜ë¦¬
                if file.filename.lower().endswith('.zip'):
                    try:
                        with zipfile.ZipFile(file_path, 'r') as zip_ref:
                            zip_ref.extractall(dest_path)
                        extracted = True
                        extracted_files.extend(zip_ref.namelist())
                        file_info["extracted"] = True
                        file_info["archive_type"] = "zip"
                    except Exception as e:
                        file_info["extraction_error"] = str(e)
                
                # TAR.GZ ë˜ëŠ” TGZ íŒŒì¼ ì²˜ë¦¬
                elif file.filename.lower().endswith(('.tar.gz', '.tgz', '.tar')):
                    try:
                        with tarfile.open(file_path, 'r:*') as tar_ref:
                            tar_ref.extractall(dest_path)
                        extracted = True
                        extracted_files.extend([m.name for m in tar_ref.getmembers()])
                        file_info["extracted"] = True
                        file_info["archive_type"] = "tar"
                    except Exception as e:
                        file_info["extraction_error"] = str(e)
                
                # ì••ì¶• í•´ì œ ì„±ê³µ ì‹œ ì›ë³¸ ì••ì¶• íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
                if extracted:
                    file_path.unlink()
                    file_info["original_removed"] = True
            
            uploaded_files.append(file_info)
        
        response = {
            "success": True,
            "uploaded_files": uploaded_files,
            "count": len(uploaded_files),
            "total_size": total_size,
            "destination": destination,
            "message": f"{len(uploaded_files)} file(s) uploaded successfully"
        }
        
        if extracted_files:
            response["extracted_files_count"] = len(extracted_files)
            response["extracted_files_sample"] = extracted_files[:10]  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
        
        return response
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.post("/validate-bids")
async def validate_bids(directory: str = "/app/data/bids"):
    """BIDS í¬ë§· ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ì„±ëŠ¥ ìµœì í™”)"""
    try:
        dir_path = Path(directory)
        
        if not dir_path.exists():
            raise HTTPException(status_code=404, detail=f"Directory not found: {directory}")
        
        # ë¬´ì‹œí•  ì‹œìŠ¤í…œ í´ë”/íŒŒì¼
        ignore_list = {"__MACOSX", ".DS_Store", "Thumbs.db", ".git", ".gitignore"}
        
        # BIDS ê²€ì¦ ê²°ê³¼
        validation_result = {
            "is_valid": False,
            "errors": [],
            "warnings": [],
            "directory": directory,
            "structure": {},
            "details": []
        }
        
        # í•„ìˆ˜ íŒŒì¼/í´ë” ì²´í¬
        required_items = {
            "dataset_description.json": False,
            "README": False,
            "participants.tsv": False
        }
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„ (ì„±ëŠ¥ ìµœì í™”: í•œ ë²ˆë§Œ ìˆœíšŒ)
        all_items = []
        subject_dirs = []
        max_subjects_to_check = 100  # ìµœëŒ€ 100ê°œ subjectë§Œ ìƒì„¸ í™•ì¸
        
        try:
            items = list(dir_path.iterdir())
        except PermissionError:
            raise HTTPException(status_code=403, detail=f"Permission denied: {directory}")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error reading directory: {str(e)}")
        
        for item in items:
            # ì‹œìŠ¤í…œ í´ë”/íŒŒì¼ ë¬´ì‹œ
            if item.name in ignore_list:
                continue
            
            all_items.append(item.name)
            
            if item.name in required_items:
                required_items[item.name] = True
                validation_result["structure"][item.name] = "found"
                validation_result["details"].append(f"âœ“ {item.name}")
            elif item.is_dir() and item.name.startswith("sub-"):
                subject_dirs.append(item.name)
                validation_result["structure"][item.name] = "subject_directory"
                
                # Subject í´ë” ë‚´ë¶€ í™•ì¸ (ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ ìƒì„¸ í™•ì¸)
                if len(subject_dirs) <= max_subjects_to_check:
                    try:
                        # ë¹ ë¥¸ í™•ì¸: ì²« ë²ˆì§¸ ë””ë ‰í† ë¦¬ë§Œ í™•ì¸
                        sub_items = list(item.iterdir())
                        sub_folders = [f.name for f in sub_items[:5] if f.is_dir() and f.name not in ignore_list]  # ìµœëŒ€ 5ê°œë§Œ í™•ì¸
                        if sub_folders:
                            validation_result["details"].append(f"âœ“ {item.name}/ â†’ {', '.join(sub_folders[:3])}{'...' if len(sub_folders) > 3 else ''}")
                    except Exception:
                        # ì ‘ê·¼ ê¶Œí•œ ë¬¸ì œ ë“±ì€ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                        pass
        
        # í•„ìˆ˜ í•­ëª© ê²€ì‚¬
        missing_items = [k for k, v in required_items.items() if not v]
        
        if missing_items:
            for item in missing_items:
                validation_result["errors"].append(f"âœ— Missing required file: {item}")
                validation_result["details"].append(f"âœ— {item} (missing)")
        
        # subject í´ë” ê°œìˆ˜ í™•ì¸
        validation_result["subject_count"] = len(subject_dirs)
        validation_result["subject_list"] = subject_dirs[:10]  # ì²˜ìŒ 10ê°œë§Œ
        
        if len(subject_dirs) == 0:
            validation_result["errors"].append("No subject directories found (sub-*)")
        else:
            validation_result["details"].append(f"âœ“ Found {len(subject_dirs)} subject(s)")
            if len(subject_dirs) > max_subjects_to_check:
                validation_result["warnings"].append(f"Large dataset: {len(subject_dirs)} subjects found. Only first {max_subjects_to_check} were checked in detail.")
        
        # dataset_description.json ê²€ì¦ (ë¹ ë¥¸ ì½ê¸°)
        desc_file = dir_path / "dataset_description.json"
        if desc_file.exists():
            try:
                # íŒŒì¼ í¬ê¸° ì œí•œ (10MB)
                if desc_file.stat().st_size > 10 * 1024 * 1024:
                    validation_result["errors"].append("dataset_description.json is too large (>10MB)")
                else:
                    with desc_file.open("r", encoding="utf-8") as f:
                        desc_data = json.load(f)
                        required_fields = ["Name", "BIDSVersion"]
                        missing_fields = [f for f in required_fields if f not in desc_data]
                        
                        if missing_fields:
                            validation_result["errors"].append(
                                f"dataset_description.json missing fields: {', '.join(missing_fields)}"
                            )
                        else:
                            validation_result["dataset_info"] = {
                                "name": desc_data.get("Name"),
                                "version": desc_data.get("BIDSVersion"),
                                "dataset_type": desc_data.get("DatasetType", "unknown")
                            }
                            validation_result["details"].append(
                                f"âœ“ Dataset: {desc_data.get('Name')} (BIDS {desc_data.get('BIDSVersion')})"
                            )
            except json.JSONDecodeError as e:
                validation_result["errors"].append(f"dataset_description.json is not valid JSON: {str(e)}")
            except Exception as e:
                validation_result["errors"].append(f"Error reading dataset_description.json: {str(e)}")
        
        # README íŒŒì¼ í™•ì¸ (ë¹ ë¥¸ í™•ì¸)
        readme_file = dir_path / "README"
        if readme_file.exists():
            try:
                readme_size = readme_file.stat().st_size
                validation_result["details"].append(f"âœ“ README ({readme_size} bytes)")
            except Exception:
                pass
        
        # participants.tsv í™•ì¸ (ë¹ ë¥¸ í™•ì¸: ì²« 100ì¤„ë§Œ)
        participants_file = dir_path / "participants.tsv"
        if participants_file.exists():
            try:
                # íŒŒì¼ í¬ê¸° ì œí•œ (100MB)
                if participants_file.stat().st_size > 100 * 1024 * 1024:
                    validation_result["warnings"].append("participants.tsv is very large (>100MB), counting may be slow")
                    validation_result["participants_count"] = "unknown"
                else:
                    with participants_file.open("r", encoding="utf-8") as f:
                        lines = f.readlines()[:1000]  # ìµœëŒ€ 1000ì¤„ë§Œ ì½ê¸°
                        validation_result["details"].append(f"âœ“ participants.tsv ({len(lines)} lines checked)")
                        validation_result["participants_count"] = len(lines) - 1 if len(lines) > 0 else 0  # í—¤ë” ì œì™¸
            except Exception as e:
                validation_result["warnings"].append(f"Could not read participants.tsv: {str(e)}")
        
        # ìµœì¢… ê²€ì¦ ê²°ê³¼
        if not validation_result["errors"]:
            validation_result["is_valid"] = True
            validation_result["message"] = "âœ… Valid BIDS dataset!"
        else:
            validation_result["is_valid"] = False
            validation_result["message"] = f"âŒ Invalid BIDS dataset ({len(validation_result['errors'])} error(s))"
        
        return validation_result
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /validate-bids: {error_detail}")
        raise HTTPException(status_code=500, detail=f"BIDS validation error: {str(e)}")

@app.get("/get-sessions")
async def get_sessions(subject_id: str, bids_dir: str = "/app/data/bids"):
    """íŠ¹ì • Subjectì˜ Session ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        # í˜¸ìŠ¤íŠ¸ ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
        host_data_dir = os.getenv("HOST_DATA_DIR", "/home/admin1/Documents/aimedpipeline/data")
        
        # Subject ë””ë ‰í† ë¦¬ ê²½ë¡œ êµ¬ì„±
        sub_dirname = subject_id if subject_id.startswith("sub-") else f"sub-{subject_id}"
        
        # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ ìš°ì„ , í˜¸ìŠ¤íŠ¸ ê²½ë¡œëŠ” ë°±ì—…)
        possible_paths = []
        
        # 1. ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ (ë§ˆìš´íŠ¸ëœ ê²½ë¡œ)
        if bids_dir.startswith('/app/data/'):
            possible_paths.append(Path(bids_dir) / sub_dirname)
            possible_paths.append(Path(bids_dir) / subject_id)
        
        # 2. ì§ì ‘ ì»¨í…Œì´ë„ˆ ê²½ë¡œ ì‹œë„
        possible_paths.append(Path("/app/data/bids") / sub_dirname)
        possible_paths.append(Path("/app/data/bids") / subject_id)
        
        # 3. í˜¸ìŠ¤íŠ¸ ê²½ë¡œ (Backendê°€ í˜¸ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°)
        possible_paths.append(Path(host_data_dir) / "bids" / sub_dirname)
        possible_paths.append(Path(host_data_dir) / "bids" / subject_id)
        
        # 4. ìƒëŒ€ ê²½ë¡œë„ ì‹œë„ (Backendê°€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°)
        possible_paths.append(Path("./data/bids") / sub_dirname)
        possible_paths.append(Path("./data/bids") / subject_id)
        
        found_path = None
        for path in possible_paths:
            try:
                if path.exists():
                    found_path = path
                    break
            except Exception:
                continue
        
        if not found_path:
            # ì—ëŸ¬ ë©”ì‹œì§€ì— ì‹œë„í•œ ê²½ë¡œë“¤ í¬í•¨
            tried_paths = [str(p) for p in possible_paths[:4]]  # ì²˜ìŒ 4ê°œë§Œ í‘œì‹œ
            return {
                "success": False,
                "sessions": [],
                "message": f"Subject directory not found. Tried: {', '.join(tried_paths)}"
            }
        
        # Session ë””ë ‰í† ë¦¬ ì°¾ê¸°
        session_dirs = []
        try:
            for item in found_path.iterdir():
                if item.is_dir() and item.name.startswith("ses-"):
                    session_id = item.name.replace("ses-", "")
                    session_dirs.append({
                        "session_id": session_id,
                        "display_name": item.name,  # ses-M126
                        "full_name": item.name      # ses-M126
                    })
        except Exception as e:
            return {
                "success": False,
                "sessions": [],
                "message": f"Error reading directory {found_path}: {str(e)}"
            }
        
        # Session IDë¡œ ì •ë ¬
        session_dirs.sort(key=lambda x: x["session_id"])
        
        return {
            "success": True,
            "subject_id": subject_id,
            "sessions": session_dirs,
            "count": len(session_dirs),
            "message": f"Found {len(session_dirs)} session(s) for {subject_id}",
            "debug_path": str(found_path)  # ë””ë²„ê¹…ìš©
        }
    except Exception as e:
        import traceback
        return {
            "success": False,
            "sessions": [],
            "message": f"Error: {str(e)}",
            "traceback": traceback.format_exc()
        }

async def run_mica_via_airflow(
    subject_id: str,
    session_id: str,
    processes: list,
    bids_dir: str,
    output_dir: str,
    fs_licence: str,
    threads: int,
    freesurfer: bool,
    user: str,
    proc_structural_flags: list | None = None,
    proc_surf_flags: list | None = None,
    post_structural_flags: list | None = None,
    proc_func_flags: list | None = None,
    dwi_flags: list | None = None,
    sc_flags: list | None = None,
):
    """Airflow DAGë¥¼ íŠ¸ë¦¬ê±°í•˜ì—¬ MICA Pipelineì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        # DAG Run ID ìƒì„±
        run_id = f"mica_{subject_id.replace('sub-', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ë””ë²„ê¹…: Airflowë¡œ ì „ë‹¬í•  session_id í™•ì¸
        print(f"ğŸ” DEBUG (run_mica_via_airflow) - session_id: '{session_id}' (type: {type(session_id)})")
        
        # Airflow API í˜ì´ë¡œë“œ
        payload = {
            "dag_run_id": run_id,
            "conf": {
                "subject_id": subject_id,
                "session_id": session_id,
                "processes": processes,
                "bids_dir": bids_dir,
                "output_dir": output_dir,
                "fs_licence": fs_licence,
                "threads": threads,
                "freesurfer": freesurfer,
                "user": user,
                "proc_structural_flags": proc_structural_flags or [],
                "proc_surf_flags": proc_surf_flags or [],
                "post_structural_flags": post_structural_flags or [],
                "proc_func_flags": proc_func_flags or [],
                "dwi_flags": dwi_flags or [],
                "sc_flags": sc_flags or [],
            }
        }
        
        # Airflow API í˜¸ì¶œ
        response = requests.post(
            f"{AIRFLOW_API}/dags/mica_pipeline/dagRuns",
            json=payload,
            auth=_auth(),
            timeout=10
        )
        
        if response.status_code in (200, 201):
            airflow_data = response.json()
            
            # DBì— ì €ì¥
            session = SessionLocal()
            try:
                container_name = f"{subject_id}"
                if session_id:
                    container_name += f"_ses-{session_id}"
                if processes:
                    container_name += f"_{processes[0]}"
                
                mica_job = MicaPipelineJob(
                    job_id=run_id,
                    subject_id=subject_id,
                    session_id=session_id,
                    processes=",".join(processes),
                    container_name=container_name,
                    pid=None,  # Airflowê°€ ê´€ë¦¬
                    status="processing",
                    progress=0.0,
                    log_file=f"{output_dir}/logs/{processes[0]}/fin/{container_name}.log",
                    error_log_file=f"{output_dir}/logs/{processes[0]}/error/{container_name}_error.log",
                    user=user  # ì‚¬ìš©ì ì •ë³´ ì¶”ê°€
                )
                session.add(mica_job)
                session.commit()
            finally:
                session.close()
            
            return {
                "success": True,
                "mode": "airflow",
                "dag_run_id": run_id,
                "subject_id": subject_id,
                "session_id": session_id,
                "processes": processes,
                "user": user,
                "airflow_url": f"{AIRFLOW_BASE}/dags/mica_pipeline/grid?dag_run_id={run_id}",
                "message": f"âœ… MICA Pipelineì´ Airflowë¥¼ í†µí•´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                          f"DAG Run ID: {run_id}\n"
                          f"User: {user}\n"
                          f"Subject: {subject_id}\n\n"
                          f"ğŸ’¡ Airflow UIì—ì„œ ì‹¤í–‰ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”: http://localhost:8080",
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Airflow API failed: {response.text}"
            )
    except requests.exceptions.RequestException as e:
        raise HTTPException(
            status_code=503,
            detail=f"Failed to connect to Airflow: {str(e)}"
        )


@app.post("/run-mica-pipeline")
async def run_mica_pipeline(data: dict):
    """mica-pipeline docker ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (ì§ì ‘ ì‹¤í–‰ ë˜ëŠ” Airflow í†µí•´ ì‹¤í–‰)"""
    try:
        # ì‹¤í–‰ ë°©ì‹ ì„ íƒ
        use_airflow = data.get("use_airflow", False)
        user = data.get("user", "anonymous")
        
        # í˜¸ìŠ¤íŠ¸ì˜ ì‹¤ì œ ë°ì´í„° ê²½ë¡œ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        base_host_data_dir = os.getenv("HOST_DATA_DIR", "/home/admin1/Documents/aimedpipeline/data")
        # ì‚¬ìš©ìë³„ ê²½ë¡œ ìƒì„±
        host_data_dir = os.path.join(base_host_data_dir, user)
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
        bids_dir = data.get("bids_dir", f"./data/{user}/bids")
        output_dir = data.get("output_dir", f"./data/{user}/derivatives")
        subject_id = data.get("subject_id")
        processes = data.get("processes", [])

        # âœ… proc_structural ë‹¨ë… ì„ íƒ ì—¬ë¶€ (ë¯¸ë‹ˆë©€ ëª¨ë“œ ìŠ¤ìœ„ì¹˜)
        simple_structural = (processes == ["proc_structural"])
        
        # ì¶”ê°€ íŒŒë¼ë¯¸í„°
        session_id = data.get("session_id", "")
        # session_idì—ì„œ "ses-" ì ‘ë‘ì‚¬ ì œê±° (ì‚¬ìš©ìê°€ "ses-01" í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•  ìˆ˜ ìˆìŒ)
        if session_id:
            session_id = session_id.replace("ses-", "").strip()
        fs_licence = data.get("fs_licence", "/app/data/license.txt")
        threads = data.get("threads", 4)
        freesurfer = data.get("freesurfer", True)
    
        # í”„ë¡œì„¸ìŠ¤ë³„ ì„¸ë¶€ í”Œë˜ê·¸
        # (ìš”êµ¬ì‚¬í•­: proc_structural/proc_surfëŠ” ì˜µì…˜ ë¯¸ì‚¬ìš©. post_structuralë§Œ atlas í—ˆìš©)
        proc_structural_flags = data.get("proc_structural_flags", [])
        proc_surf_flags = data.get("proc_surf_flags", [])
        post_structural_flags = data.get("post_structural_flags", [])
        proc_func_flags = data.get("proc_func_flags", [])
        dwi_flags = data.get("dwi_flags", [])
        sc_flags = data.get("sc_flags", [])

        # ìœ í‹¸ í•¨ìˆ˜ë“¤ -----------------------------------------------------------
        def join_tokens(tokens: list[str]) -> str:
            # ê° í† í°ì„ shlex.quoteë¡œ ê°ì‹¸ ì•ˆì „í•˜ê²Œ ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
            return " ".join(shlex.quote(t) for t in tokens if t is not None and str(t) != "")

        def normalize_flags(tokens: list[str]) -> list[str]:
            """
            - ê°’ ë™ë°˜ ì˜µì…˜ì€ 'ë§ˆì§€ë§‰ ê°’ ìš°ì„ 'ìœ¼ë¡œ 1íšŒë§Œ ë‚¨ê¹€
            - í† ê¸€í˜• í”Œë˜ê·¸ëŠ” ì¤‘ë³µ ì œê±°
            - -freesurfer/-fs_licence ëŠ” ì—¬ê¸°ì„œ ì œê±°(ì „ì—­ì—ì„œ 1íšŒë§Œ ì‚½ì…)
            """
            with_val = {
                "-T1wStr", "-fs_licence", "-surf_dir", "-T1", "-atlas",
                "-mainScanStr", "-func_pe", "-func_rpe", "-mainScanRun",
                "-phaseReversalRun", "-topupConfig", "-icafixTraining",
                "-sesAnat"
            }
            kv = {}               # option -> value (ë§ˆì§€ë§‰ ê°’ì´ ë®ì–´ì”€)
            toggles = set()       # í† ê¸€í˜• ëª¨ìŒ
            passthrough = []      # ê¸°íƒ€(ê°’ ì—†ëŠ”) í† í° ë³´ê´€

            it = iter(tokens)
            for t in it:
                if t in with_val:
                    v = next(it, None)
                    if v is None or (isinstance(v, str) and v.startswith("-")):
                        continue
                    kv[t] = v
                else:
                    # -freesurfer/-fs_licence ëŠ” ì „ì—­ì—ì„œë§Œ ë„£ê¸°: ì—¬ê¸°ì„œ ì œê±°
                    if t in ("-freesurfer",):
                        continue
                    if t == "-fs_licence":
                        _ = next(it, None)  # ê°’ ì†Œëª¨ë§Œ í•˜ê³  ë²„ë¦¼
                        continue
                    toggles.add(t) if t.startswith("-") else passthrough.append(t)

            out = []
            for k, v in kv.items():
                if k == "-fs_licence":
                    continue
                out += [k, v]

            out += sorted(t for t in toggles if t not in ("-freesurfer",))
            out += passthrough
            return out

        def convert_to_host_path(container_path: str) -> str:
            """ì»¨í…Œì´ë„ˆ ê²½ë¡œë¥¼ í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ë³€í™˜"""
            if container_path.startswith("/app/data"):
                return container_path.replace("/app/data", host_data_dir)
            return container_path
        # ---------------------------------------------------------------------

        # í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ë³€í™˜
        host_bids_dir = convert_to_host_path(bids_dir)
        host_output_dir = convert_to_host_path(output_dir)
        host_fs_licence = convert_to_host_path(fs_licence)
        
        if not subject_id:
            raise HTTPException(status_code=400, detail="subject_id is required")
        if not processes:
            raise HTTPException(status_code=400, detail="At least one process must be selected")
        
        # ì‚¬ìš©ìë³„ ë””ë ‰í† ë¦¬ ìƒì„± (í˜¸ìŠ¤íŠ¸ ê²½ë¡œ)
        Path(host_data_dir).mkdir(parents=True, exist_ok=True)
        Path(host_data_dir + "/bids").mkdir(parents=True, exist_ok=True)
        Path(host_data_dir + "/derivatives").mkdir(parents=True, exist_ok=True)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # Airflowë¡œ ë„˜ê¸°ëŠ” ê²½ìš°ëŠ” ê¸°ì¡´ ê·¸ëŒ€ë¡œ ìœ ì§€
        if use_airflow:
            # ë¼ì´ì„¼ìŠ¤ ê²½ë¡œ ë³€í™˜ (ê³µí†µ ê²½ë¡œ ì‚¬ìš©)
            # fs_licenceê°€ /app/dataë¡œ ì‹œì‘í•˜ë©´ í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ë³€í™˜
            if fs_licence.startswith("/app/data/"):
                # /app/data/license.txt -> /home/admin1/Documents/aimedpipeline/data/license.txt
                fs_licence_host = fs_licence.replace("/app/data", base_host_data_dir)
            elif fs_licence.startswith("/app/"):
                # ë‹¤ë¥¸ /app ê²½ë¡œë„ ë³€í™˜
                fs_licence_host = fs_licence.replace("/app", base_host_data_dir.replace("/data", ""))
            else:
                # ì´ë¯¸ í˜¸ìŠ¤íŠ¸ ê²½ë¡œì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                fs_licence_host = fs_licence
            
            # ë””ë²„ê¹…: session_id í™•ì¸
            print(f"ğŸ” DEBUG (backend) - session_id before sending to Airflow: '{session_id}' (type: {type(session_id)})")
            print(f"ğŸ” DEBUG (backend) - host_data_dir: {host_data_dir}")
            print(f"ğŸ” DEBUG (backend) - subject_id: {subject_id}")
            print(f"ğŸ” DEBUG (backend) - use_airflow: {use_airflow}")
            
            # session_idê°€ ë¹ˆ ë¬¸ìì—´ì´ë©´ ëª¨ë“  ì„¸ì…˜ì— ëŒ€í•´ ë³„ë„ ì‘ì—… ìƒì„±
            if not session_id or session_id == "":
                print(f"ğŸ” DEBUG - session_id is empty, attempting to detect sessions...")
                # ì„¸ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                try:
                    # ì‚¬ìš©ìë³„ ê²½ë¡œ ì‚¬ìš©
                    # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œì™€ í˜¸ìŠ¤íŠ¸ ê²½ë¡œ ëª¨ë‘ ì‹œë„
                    bids_path_host = Path(host_data_dir) / "bids"
                    bids_path_container = Path("/app/data") / user / "bids"
                    
                    sub_dirname = subject_id if subject_id.startswith("sub-") else f"sub-{subject_id}"
                    subject_path_host = bids_path_host / sub_dirname
                    subject_path_container = bids_path_container / sub_dirname
                    
                    print(f"ğŸ” DEBUG - Checking for sessions:")
                    print(f"  Host path: {subject_path_host} (exists: {subject_path_host.exists()})")
                    print(f"  Container path: {subject_path_container} (exists: {subject_path_container.exists()})")
                    
                    # í˜¸ìŠ¤íŠ¸ ê²½ë¡œ ë˜ëŠ” ì»¨í…Œì´ë„ˆ ê²½ë¡œ ì¤‘ ì¡´ì¬í•˜ëŠ” ê²ƒ ì‚¬ìš©
                    subject_path = None
                    if subject_path_host.exists():
                        subject_path = subject_path_host
                        print(f"âœ… Using host path: {subject_path}")
                    elif subject_path_container.exists():
                        subject_path = subject_path_container
                        print(f"âœ… Using container path: {subject_path}")
                    
                    if not subject_path:
                        print(f"âš ï¸ Neither path exists, trying to find subject directory...")
                        # ì¶”ê°€ ê²½ë¡œ ì‹œë„
                        possible_paths = [
                            Path(host_data_dir) / "bids" / sub_dirname,
                            Path("/app/data") / user / "bids" / sub_dirname,
                            Path("/app/data/bids") / sub_dirname,
                            Path(host_data_dir.replace("/data", "")) / "data" / user / "bids" / sub_dirname,
                        ]
                        for path in possible_paths:
                            if path.exists():
                                subject_path = path
                                print(f"âœ… Found subject at: {subject_path}")
                                break
                    
                    if subject_path and subject_path.exists():
                        # ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
                        all_items = list(subject_path.iterdir())
                        print(f"ğŸ” DEBUG - Items in subject directory: {[item.name for item in all_items]}")
                        
                        session_dirs = [d.name.replace("ses-", "") for d in subject_path.iterdir()
                                      if d.is_dir() and d.name.startswith("ses-")]
                        
                        print(f"ğŸ” DEBUG - Found sessions: {session_dirs}")
                        
                        if session_dirs:
                            print(f"ğŸ” Found {len(session_dirs)} sessions: {session_dirs}")
                            # ê° ì„¸ì…˜ì— ëŒ€í•´ ë³„ë„ì˜ Airflow DAG Run ìƒì„±
                            results = []
                            for ses in session_dirs:
                                print(f"ğŸš€ Creating DAG Run for session: {ses}")
                                result = await run_mica_via_airflow(
                                    subject_id=subject_id,
                                    session_id=ses,  # ê° ì„¸ì…˜ì— ëŒ€í•´ ë³„ë„ ì‹¤í–‰
                                    processes=processes,
                                    bids_dir=host_data_dir + "/bids",
                                    output_dir=host_data_dir + "/derivatives",
                                    fs_licence=fs_licence_host,
                                    threads=threads,
                                    freesurfer=freesurfer,
                                    user=user,
                                    proc_structural_flags=proc_structural_flags,
                                    proc_surf_flags=proc_surf_flags,
                                    post_structural_flags=post_structural_flags,
                                    proc_func_flags=proc_func_flags,
                                    dwi_flags=dwi_flags,
                                    sc_flags=sc_flags,
                                )
                                results.append(result)
                            
                            # ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë˜, ëª¨ë“  ì„¸ì…˜ì´ ì‹¤í–‰ ì¤‘ì„ì„ í‘œì‹œ
                            if results:
                                first_result = results[0]
                                first_result["message"] = f"âœ… MICA Pipelineì´ {len(session_dirs)}ê°œ ì„¸ì…˜ì— ëŒ€í•´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n" + \
                                                          f"ì‹¤í–‰ ì¤‘ì¸ ì„¸ì…˜: {', '.join(session_dirs)}\n" + \
                                                          f"ê° ì„¸ì…˜ì€ Airflowì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤."
                                first_result["sessions"] = session_dirs
                                first_result["total_sessions"] = len(session_dirs)
                                return first_result
                        else:
                            print(f"âš ï¸ No sessions found for {subject_id}, running with empty session_id")
                    else:
                        print(f"âš ï¸ Subject path not found: {subject_path}, running with empty session_id")
                except Exception as e:
                    print(f"âš ï¸ Error detecting sessions: {e}, running with empty session_id")
                    import traceback
                    traceback.print_exc()
            
            # session_idê°€ ìˆê±°ë‚˜ ì„¸ì…˜ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            print(f"âš ï¸ DEBUG - Falling back to single DAG Run with session_id: '{session_id}'")
            return await run_mica_via_airflow(
                subject_id=subject_id,
                session_id=session_id if session_id else "",  # ë¹ˆ ë¬¸ìì—´ì´ë©´ ì „ì²´ ì„¸ì…˜ ì²˜ë¦¬ (Airflowì—ì„œ ìë™ ê°ì§€)
                processes=processes,
                bids_dir=host_data_dir + "/bids",
                output_dir=host_data_dir + "/derivatives",
                fs_licence=fs_licence_host,
                threads=threads,
                freesurfer=freesurfer,
                user=user,
                # ì•„ë˜ ë‘ í”Œë˜ê·¸ëŠ” ë¬´ì‹œë˜ì§€ë§Œ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€
                proc_structural_flags=proc_structural_flags,
                proc_surf_flags=proc_surf_flags,
                post_structural_flags=post_structural_flags,
                proc_func_flags=proc_func_flags,
                dwi_flags=dwi_flags,
                sc_flags=sc_flags,
            )

        # =========================
        # ì „ì²´ Subject ì‹¤í–‰ (ALL)
        # =========================
        if subject_id.lower() == "all":
            bids_path = Path(bids_dir)
            if not bids_path.exists():
                raise HTTPException(status_code=404, detail=f"BIDS directory not found: {bids_dir}")
            
            subjects = [d.name for d in bids_path.iterdir() 
                        if d.is_dir() and d.name.startswith("sub-") and d.name != "__MACOSX"]
            if not subjects:
                raise HTTPException(status_code=400, detail="No subjects found in BIDS directory")
            
            all_results, total_success, total_failed = [], 0, 0
            
            for sub in subjects:
                try:
                    sub_id = sub.replace("sub-", "")
                    
                    # ì„¸ì…˜ ëª©ë¡ ìˆ˜ì§‘
                    if session_id:
                        sessions_to_process = [session_id]
                    else:
                        # bids_dirê°€ ì»¨í…Œì´ë„ˆ ê²½ë¡œì¸ì§€ í˜¸ìŠ¤íŠ¸ ê²½ë¡œì¸ì§€ í™•ì¸
                        if bids_dir.startswith('/app/data/'):
                            check_bids_dir = bids_dir.replace('/app/data/', f'{host_data_dir}/')
                        else:
                            check_bids_dir = bids_dir
                        
                        subject_path = Path(check_bids_dir) / sub
                        if subject_path.exists():
                            session_dirs = [d.name.replace("ses-", "") for d in subject_path.iterdir() 
                                            if d.is_dir() and d.name.startswith("ses-")]
                            sessions_to_process = session_dirs if session_dirs else [""]
                        else:
                            sessions_to_process = [""]

                    for ses in sessions_to_process:
                        container_name = f"{sub}" + (f"_ses-{ses}" if ses else "")
                        if processes:
                            container_name += f"_{processes[0]}"

                        container_log_dir = Path(output_dir) / "logs" / (processes[0] if processes else "")
                        container_log_dir.mkdir(parents=True, exist_ok=True)
                        (container_log_dir / "fin").mkdir(exist_ok=True)
                        (container_log_dir / "error").mkdir(exist_ok=True)

                        container_log_file = container_log_dir / "fin" / f"{container_name}.log"
                        container_error_log_file = container_log_dir / "error" / f"{container_name}_error.log"

                        # ---- ë¶„ê¸°: ë¯¸ë‹ˆë©€ vs ì¼ë°˜ ----
                        if simple_structural:
                            use_fs_licence_min = Path(fs_licence).exists() and ('proc_structural' in processes)

                            # docker run ë³¼ë¥¨ ë§ˆìš´íŠ¸
                            cmd = (
                                f"docker run --rm --name {container_name} "
                                f"-v {host_bids_dir}:{host_bids_dir} "
                                f"-v {host_output_dir}:{host_output_dir} "
                            )
                            if use_fs_licence_min:
                                cmd += f"-v {host_fs_licence}:{host_fs_licence} "

                            cmd += (
                                "micalab/micapipe:v0.2.3 "
                                f"-bids {host_bids_dir} "
                                f"-out {host_output_dir} "
                                f"-sub {sub_id} "
                            )
                            if ses:  # ses ë³€ìˆ˜ ì‚¬ìš©
                                cmd += f"-ses {ses} "

                            cmd += "-proc_structural "
                            if use_fs_licence_min:
                                cmd += f"-fs_licence {host_fs_licence} "

                            cmd += f"> {container_log_file} 2> {container_error_log_file}"
                        else:
                            # ì¼ë°˜: ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ ì¡°í•©
                            base_switches = [f"-{p}" for p in processes]

                            # ì˜µì…˜ í”Œë˜ê·¸: post_structural + func + dwi + sc (struct/surf ì˜µì…˜ì€ ìƒëµ)
                            extra_tokens = (
                                #(proc_structural_flags or []) + 
                                #(proc_surf_flags or []) + 
                                (post_structural_flags or []) +
                                (proc_func_flags or []) +
                                (dwi_flags or []) +
                                (sc_flags or [])
                            )
                            normalized = normalize_flags(extra_tokens)
                            process_flags = join_tokens(base_switches + normalized)

                            use_fs_licence = Path(fs_licence).exists() and (
                                ('proc_structural' in processes) or
                                ('proc_surf' in processes and freesurfer)
                            )

                            fs_licence_mount = ""
                            if use_fs_licence:
                                fs_licence_mount = f"-v {host_fs_licence}:{host_fs_licence}"

                            cmd = (
                                f"docker run --rm --name {container_name} "
                                f"-v {host_bids_dir}:{host_bids_dir} "
                                f"-v {host_output_dir}:{host_output_dir} "
                            )
                            if fs_licence_mount:
                                cmd += f"{fs_licence_mount} "

                            cmd += (
                                "micalab/micapipe:v0.2.3 "
                                f"-bids {host_bids_dir} "
                                f"-out {host_output_dir} "
                                f"-sub {sub_id} "
                            )
                            if ses:
                                cmd += f"-ses {ses} "

                            cmd += f"-threads {threads} {process_flags} "

                            if 'proc_surf' in processes:
                                cmd += f"-freesurfer {'TRUE' if freesurfer else 'FALSE'} "

                            # ë¼ì´ì„ ìŠ¤ëŠ” ìœ„ ì¡°ê±´(use_fs_licence)ì¼ ë•Œ í•­ìƒ ë„˜ê¹€
                            if use_fs_licence:
                                cmd += f"-fs_licence {host_fs_licence} "

                            cmd += f"> {container_log_file} 2> {container_error_log_file}"

                        # ì‹¤í–‰
                        process = subprocess.Popen(
                            cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
                        )

                        # DB ê¸°ë¡
                        db_session = SessionLocal()
                        try:
                            log = CommandLog(
                                command=cmd,
                                output=f"Container started: {container_name} (PID: {process.pid})",
                                error=""
                            )
                            db_session.add(log)
                            mica_job = MicaPipelineJob(
                                job_id=container_name,
                                subject_id=sub,
                                session_id=ses if ses else None,
                                processes=",".join(processes),
                                container_name=container_name,
                                pid=process.pid,
                                status="processing",
                                progress=0.0,
                                log_file=str(container_log_file),
                                error_log_file=str(container_error_log_file)
                            )
                            db_session.add(mica_job)
                            db_session.commit()
                        finally:
                            db_session.close()

                        total_success += 1
                        subject_with_session = f"{sub}" + (f"_ses-{ses}" if ses else "")
                        all_results.append({
                            "subject": subject_with_session,
                            "success": True,
                            "container_name": container_name,
                            "pid": process.pid,
                            "job_id": container_name,
                            "message": "ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë¨"
                        })

                except Exception as e:
                    total_failed += 1
                    all_results.append({
                        "subject": sub,
                        "success": False,
                        "message": f"ì‹œì‘ ì‹¤íŒ¨: {str(e)}"
                    })
            
            return {
                "success": True,
                "mode": "all_subjects",
                "total_subjects": len(subjects),
                "successful": total_success,
                "failed": total_failed,
                "results": all_results,
                "processes": processes,
                "timestamp": datetime.now().isoformat(),
                "message": f"âœ… {total_success}ê°œì˜ ì»¨í…Œì´ë„ˆê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. (ì‹¤íŒ¨: {total_failed}ê°œ)\n\nğŸ’¡ 'ë¡œê·¸ í™•ì¸' íƒ­ì—ì„œ ì‹¤í–‰ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."
            }

        # =========================
        # ë‹¨ì¼ Subject ì‹¤í–‰
        # =========================
        else:
            sub_id = subject_id.replace("sub-", "")
            
            # ì„¸ì…˜ ìë™ ê°ì§€
            actual_session = session_id
            if not session_id:
                # bids_dirê°€ ì»¨í…Œì´ë„ˆ ê²½ë¡œì¸ì§€ í˜¸ìŠ¤íŠ¸ ê²½ë¡œì¸ì§€ í™•ì¸
                # ì»¨í…Œì´ë„ˆ ê²½ë¡œ(/app/data)ë©´ í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ë³€í™˜
                if bids_dir.startswith('/app/data/'):
                    check_bids_dir = bids_dir.replace('/app/data/', f'{host_data_dir}/')
                else:
                    # ì´ë¯¸ í˜¸ìŠ¤íŠ¸ ê²½ë¡œì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    check_bids_dir = bids_dir
                
                subject_path = Path(check_bids_dir) / subject_id
                print(f"[Backend] Checking for sessions in: {subject_path}")
                
                if subject_path.exists():
                    session_dirs = [d.name.replace("ses-", "") for d in subject_path.iterdir() 
                                    if d.is_dir() and d.name.startswith("ses-")]
                    if session_dirs:
                        actual_session = session_dirs[0]
                        print(f"[Backend] âœ… Auto-detected session: {actual_session}")
                    else:
                        print(f"[Backend] âš ï¸ No session directories found in: {subject_path}")
                else:
                    print(f"[Backend] âš ï¸ Warning: Subject path not found: {subject_path}")
            
            container_name = f"{subject_id}" + (f"_ses-{actual_session}" if actual_session else "")
            if processes:
                container_name += f"_{processes[0]}"

            container_log_dir = Path(output_dir) / "logs" / (processes[0] if processes else "")
            container_log_dir.mkdir(parents=True, exist_ok=True)
            (container_log_dir / "fin").mkdir(exist_ok=True)
            (container_log_dir / "error").mkdir(exist_ok=True)

            container_log_file = container_log_dir / "fin" / f"{container_name}.log"
            container_error_log_file = container_log_dir / "error" / f"{container_name}_error.log"

            # ---- ë¶„ê¸°: ë¯¸ë‹ˆë©€ vs ì¼ë°˜ ----
            if simple_structural:
                use_fs_licence_min = Path(fs_licence).exists() and ('proc_structural' in processes)

                # docker run ë³¼ë¥¨ ë§ˆìš´íŠ¸
                cmd = (
                    f"docker run --rm --name {container_name} "
                    f"-v {host_bids_dir}:{host_bids_dir} "
                    f"-v {host_output_dir}:{host_output_dir} "
                )
                if use_fs_licence_min:
                    cmd += f"-v {host_fs_licence}:{host_fs_licence} "

                cmd += (
                    "micalab/micapipe:v0.2.3 "
                    f"-bids {host_bids_dir} "
                    f"-out {host_output_dir} "
                    f"-sub {sub_id} "
                )
                if actual_session:  # ë˜ëŠ” ses
                    cmd += f"-ses {actual_session} "

                cmd += "-proc_structural "
                if use_fs_licence_min:
                    cmd += f"-fs_licence {host_fs_licence} "

                cmd += f"> {container_log_file} 2> {container_error_log_file}"
            else:
                # ì¼ë°˜
                base_switches = [f"-{p}" for p in processes]

                extra_tokens = (
                    #(proc_structural_flags or []) + 
                    #(proc_surf_flags or []) + 
                    (post_structural_flags or []) +
                    (proc_func_flags or []) +
                    (dwi_flags or []) +
                    (sc_flags or [])
                )
                normalized = normalize_flags(extra_tokens)
                process_flags = join_tokens(base_switches + normalized)

                use_fs_licence = Path(fs_licence).exists() and (
                    ('proc_structural' in processes) or
                    ('proc_surf' in processes and freesurfer)
                )

                fs_licence_mount = ""
                if use_fs_licence:
                    fs_licence_mount = f"-v {host_fs_licence}:{host_fs_licence}"

                cmd = (
                    f"docker run --rm --name {container_name} "
                    f"-v {host_bids_dir}:{host_bids_dir} "
                    f"-v {host_output_dir}:{host_output_dir} "
                )
                if fs_licence_mount:
                    cmd += f"{fs_licence_mount} "

                cmd += (
                    "micalab/micapipe:v0.2.3 "
                    f"-bids {host_bids_dir} "
                    f"-out {host_output_dir} "
                    f"-sub {sub_id} "
                )
                if actual_session:
                    cmd += f"-ses {actual_session} "

                cmd += f"-threads {threads} {process_flags} "

                if 'proc_surf' in processes:
                    cmd += f"-freesurfer {'TRUE' if freesurfer else 'FALSE'} "

                # ë¼ì´ì„ ìŠ¤ëŠ” ìœ„ ì¡°ê±´(use_fs_licence)ì¼ ë•Œ í•­ìƒ ë„˜ê¹€
                if use_fs_licence:
                    cmd += f"-fs_licence {host_fs_licence} "

                cmd += f"> {container_log_file} 2> {container_error_log_file}"

            # ì‹¤í–‰
            process = subprocess.Popen(
                cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
            )
            
            # DB ê¸°ë¡
            session = SessionLocal()
            try:
                log = CommandLog(
                    command=cmd,
                    output=f"Container started: {container_name} (PID: {process.pid})",
                    error=""
                )
                session.add(log)
                mica_job = MicaPipelineJob(
                    job_id=container_name,
                    subject_id=subject_id,
                    session_id=actual_session,
                    processes=",".join(processes),
                    container_name=container_name,
                    pid=process.pid,
                    status="processing",
                    progress=0.0,
                    log_file=str(container_log_file),
                    error_log_file=str(container_error_log_file)
                )
                session.add(mica_job)
                session.commit()
            finally:
                session.close()
            
            return {
                "success": True,
                "mode": "single_subject",
                "command": cmd,
                "output": f"âœ… MICA Pipelineì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\nì»¨í…Œì´ë„ˆ: {container_name}\nPID: {process.pid}\n\nğŸ’¡ 'ë¡œê·¸ í™•ì¸' íƒ­ ë˜ëŠ” 'Download Results'ì—ì„œ ì‹¤í–‰ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "error": "",
                "returncode": 0,
                "subject_id": subject_id,
                "session_id": actual_session,
                "session_auto_detected": bool(actual_session and not session_id),
                "processes": processes,
                "container_name": container_name,
                "pid": process.pid,
                "job_id": container_name,
                "timestamp": datetime.now().isoformat()
            }
            
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": f"Command timeout after {data.get('timeout', 3600)} seconds",
            "returncode": -1
        }
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì „ë‹¬
        raise
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)


@app.get("/mica-logs")
async def get_mica_logs(output_dir: str = "/app/data/derivatives"):
    """MICA Pipeline ë¡œê·¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        logs_dir = Path(output_dir) / "logs"
        
        if not logs_dir.exists():
            return {
                "success": True,
                "logs": [],
                "message": "ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        # ë¡œê·¸ ì•ˆì—ì„œ ì—ëŸ¬ë¡œ ê°„ì£¼í•  í‚¤ì›Œë“œë“¤
        error_keywords = [
            "[ ERROR ]",       # micapipe ê¸°ë³¸ ì—ëŸ¬ í¬ë§·
            "ERROR",
            "Error",
            "Exception",
            "FAILED",
            "Failed",
        ]
        # ë¬´ì‹œí•´ì•¼ í•˜ëŠ” ì—ëŸ¬ í‚¤ì›Œë“œë“¤
        ignore_error_keywords = [
            "realpath: invalid option -- 'f'",
            "Try 'realpath --help' for more information"
        ]
        logs = []
        
        # ê° í”„ë¡œì„¸ìŠ¤ ë””ë ‰í† ë¦¬ ìˆœíšŒ
        for process_dir in logs_dir.iterdir():
            if not process_dir.is_dir():
                continue
            
            process_name = process_dir.name
            
            fin_dir = process_dir / "fin"
            error_dir = process_dir / "error"
            
            if fin_dir.exists():
                for log_file in fin_dir.iterdir():
                    if not (log_file.is_file() and log_file.suffix == ".log"):
                        continue
                    
                    error_file = error_dir / f"{log_file.stem}_error.log"
                    
                    # 1ì°¨: ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ í¬ê¸°ë¡œ íŒë‹¨
                    has_error = error_file.exists() and error_file.stat().st_size > 0
                    
                    # â”€â”€ ignore rule ì ìš© 1: ì—ëŸ¬ íŒŒì¼ ì•ˆì—ì„œ ë¬´ì‹œ ê°€ëŠ¥í•œ ì—ëŸ¬ì¸ì§€ í™•ì¸ â”€â”€
                    if has_error and error_file.exists():
                        try:
                            with error_file.open("r", errors="ignore") as f:
                                err_content = f.read()
                            if any(kw in err_content for kw in ignore_error_keywords):
                                has_error = False
                        except:
                            pass
                    # 2ì°¨: ì—ëŸ¬ íŒŒì¼ì´ ë¹„ì–´ ìˆìœ¼ë©´ ë©”ì¸ ë¡œê·¸ ë‚´ìš©ì—ì„œ [ ERROR ] ë“± ê²€ìƒ‰
                    if not has_error:
                        try:
                            # ë„ˆë¬´ í° íŒŒì¼ ëŒ€ë¹„í•´ì„œ ëë¶€ë¶„ë§Œ ì½ê¸° (ë§ˆì§€ë§‰ 4000ì ì •ë„)
                            with log_file.open("r", encoding="utf-8", errors="ignore") as f:
                                tail = f.read()[-4000:]
                            
                            if any(kw in tail for kw in error_keywords):
                                has_error = True
                        except Exception:
                            # ë¡œê·¸ ì½ê¸° ì‹¤íŒ¨í•˜ë©´ ì—¬ê¸°ì„œ ì¶”ê°€ ì²˜ë¦¬ ì—†ì´ íŒ¨ìŠ¤
                            pass
                    
                    logs.append({
                        "process": process_name,
                        "subject": log_file.stem,
                        "log_file": str(log_file),
                        "error_file": str(error_file) if error_file.exists() else None,
                        "size": log_file.stat().st_size,
                        "modified": log_file.stat().st_mtime,
                        "has_error": has_error,
                    })
        
        # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        logs.sort(key=lambda x: x["modified"], reverse=True)
        
        return {
            "success": True,
            "logs": logs,
            "count": len(logs)
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.get("/mica-log-content")
async def get_mica_log_content(log_file: str, lines: int = 100):
    """MICA Pipeline ë¡œê·¸ íŒŒì¼ ë‚´ìš©ì„ ì½ìŠµë‹ˆë‹¤."""
    try:
        log_path = Path(log_file)
        
        # ë³´ì•ˆ: /app/data ì™¸ë¶€ ê²½ë¡œ ì ‘ê·¼ ì°¨ë‹¨
        if not str(log_path).startswith("/app/data"):
            raise HTTPException(status_code=403, detail="Access denied")
        
        if not log_path.exists():
            raise HTTPException(status_code=404, detail="Log file not found")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = log_path.stat().st_size
        
        # ë§ˆì§€ë§‰ Nì¤„ ì½ê¸°
        with log_path.open("r", encoding="utf-8", errors="ignore") as f:
            all_lines = f.readlines()
            content_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
        
        return {
            "success": True,
            "file": str(log_path),
            "size": file_size,
            "total_lines": len(all_lines),
            "returned_lines": len(content_lines),
            "content": "".join(content_lines)
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.get("/mica-containers")
async def get_mica_containers():
    """ì‹¤í–‰ ì¤‘ì¸ micapipe ì»¨í…Œì´ë„ˆ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        # docker ps ëª…ë ¹ ì‹¤í–‰
        result = subprocess.run(
            "docker ps --filter 'name=sub-' --format '{{.Names}}\t{{.Status}}\t{{.Image}}\t{{.RunningFor}}'",
            shell=True,
            capture_output=True,
            text=True,
            timeout=10
        )
        
        containers = []
        if result.stdout.strip():
            for line in result.stdout.strip().split('\n'):
                parts = line.split('\t')
                if len(parts) >= 4:
                    containers.append({
                        "name": parts[0],
                        "status": parts[1],
                        "image": parts[2],
                        "running_for": parts[3]
                    })
        
        return {
            "success": True,
            "containers": containers,
            "count": len(containers)
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.post("/mica-container-stop")
async def stop_mica_container(container_name: str):
    """micapipe ì»¨í…Œì´ë„ˆë¥¼ ì¢…ë£Œí•˜ê³  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        # ë³´ì•ˆ: sub- ë¡œ ì‹œì‘í•˜ëŠ” ì»¨í…Œì´ë„ˆë§Œ ì¢…ë£Œ ê°€ëŠ¥
        if not container_name.startswith("sub-"):
            raise HTTPException(status_code=403, detail="Only sub-* containers can be stopped")
        
        # docker stop ëª…ë ¹ ì‹¤í–‰
        result = subprocess.run(
            f"docker stop {container_name}",
            shell=True,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        # ì»¨í…Œì´ë„ˆ ì¤‘ì§€ ì„±ê³µ ì‹œ DB ìƒíƒœ ì—…ë°ì´íŠ¸
        if result.returncode == 0:
            db = SessionLocal()
            try:
                # í•´ë‹¹ ì»¨í…Œì´ë„ˆ ì´ë¦„ìœ¼ë¡œ job ì°¾ê¸°
                job = db.query(MicaPipelineJob).filter(
                    MicaPipelineJob.container_name == container_name,
                    MicaPipelineJob.status == "processing"
                ).first()
                
                if job:
                    job.status = "failed"
                    job.completed_at = datetime.utcnow()
                    job.progress = 100.0
                    job.error_message = f"Container stopped by user: {container_name}"
                    db.commit()
                    print(f"âœ… Updated job status for stopped container: {container_name}")
            except Exception as e:
                print(f"âš ï¸ Failed to update job status: {e}")
            finally:
                db.close()
        
        return {
            "success": result.returncode == 0,
            "container": container_name,
            "message": f"Container {container_name} stopped" if result.returncode == 0 else "Failed to stop container",
            "output": result.stdout,
            "error": result.stderr
        }
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Timeout while stopping container"
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /mica-container-stop: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.get("/mica-jobs")
async def get_mica_jobs(status: str = None, user: str = None):
    """MICA Pipeline Job ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤. (ì‚¬ìš©ìë³„ í•„í„°ë§ ì§€ì›)"""
    try:
        db = SessionLocal()
        try:
            query = db.query(MicaPipelineJob)
            
            # ì‚¬ìš©ìë³„ í•„í„°ë§ (user íŒŒë¼ë¯¸í„°ê°€ ì œê³µëœ ê²½ìš°)
            if user:
                query = query.filter(MicaPipelineJob.user == user)
            
            # ìƒíƒœ í•„í„°ë§
            if status:
                query = query.filter(MicaPipelineJob.status == status)
            
            jobs = query.order_by(MicaPipelineJob.started_at.desc()).all()
            
            # Airflowì—ì„œ ëª¨ë“  ìµœê·¼ DAG Run í™•ì¸í•˜ì—¬ ëˆ„ë½ëœ ì‹¤íŒ¨ job ì°¾ê¸°
            try:
                # ìµœê·¼ 100ê°œì˜ DAG Run í™•ì¸
                airflow_dag_runs_response = requests.get(
                    f"{AIRFLOW_API}/dags/mica_pipeline/dagRuns",
                    params={"limit": 100, "order_by": "-start_date"},
                    auth=_auth(),
                    timeout=10
                )
                if airflow_dag_runs_response.status_code == 200:
                    airflow_dag_runs = airflow_dag_runs_response.json().get("dag_runs", [])
                    # DBì— ìˆëŠ” job_id ëª©ë¡
                    db_job_ids = {job.job_id for job in jobs if job.job_id.startswith("mica_")}
                    
                    # Airflowì—ëŠ” ìˆì§€ë§Œ DBì— ì—†ê±°ë‚˜ ìƒíƒœê°€ ë‹¤ë¥¸ ê²½ìš° ì—…ë°ì´íŠ¸
                    for dag_run in airflow_dag_runs:
                        dag_run_id = dag_run.get("dag_run_id")
                        dag_run_state = dag_run.get("state")
                        
                        if dag_run_id.startswith("mica_") and dag_run_state in ["success", "failed"]:
                            # DBì—ì„œ í•´ë‹¹ job ì°¾ê¸°
                            db_job = next((j for j in jobs if j.job_id == dag_run_id), None)
                            if db_job:
                                # ìƒíƒœê°€ ë‹¤ë¥´ë©´ ì—…ë°ì´íŠ¸
                                if db_job.status == "processing" and dag_run_state in ["success", "failed"]:
                                    db_job.status = "completed" if dag_run_state == "success" else "failed"
                                    db_job.completed_at = datetime.utcnow()
                                    db_job.progress = 100.0
                                    if dag_run_state == "failed":
                                        db_job.error_message = "Airflow DAG execution failed. Check Airflow UI for details."
                                    db.commit()
                                    print(f"âœ… Synced job {dag_run_id} status from Airflow: {dag_run_state}")
            except Exception as e:
                print(f"Failed to sync Airflow DAG runs: {e}")
            
            # ì‹¤ì‹œê°„ìœ¼ë¡œ ì»¨í…Œì´ë„ˆ/Airflow ìƒíƒœ í™•ì¸ ë° ì—…ë°ì´íŠ¸
            # ëª¨ë“  processing ìƒíƒœì˜ job í™•ì¸ (ì‹¤íŒ¨í•œ jobë„ í¬í•¨)
            processing_jobs = [job for job in jobs if job.status == "processing"]
            
            for job in processing_jobs:
                if job.status == "processing":
                    # Airflowë¡œ ì‹¤í–‰ëœ jobì¸ì§€ í™•ì¸ (job_idê°€ "mica_"ë¡œ ì‹œì‘)
                    if job.job_id.startswith("mica_"):
                        # Airflow DAG Run ìƒíƒœ í™•ì¸ (ë” ê°•í™”ëœ ì²´í¬)
                        try:
                            airflow_response = requests.get(
                                f"{AIRFLOW_API}/dags/mica_pipeline/dagRuns/{job.job_id}",
                                auth=_auth(),
                                timeout=5  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
                            )
                            if airflow_response.status_code == 200:
                                airflow_data = airflow_response.json()
                                airflow_state = airflow_data.get("state")
                                
                                # Airflow ìƒíƒœì— ë”°ë¼ DB ì—…ë°ì´íŠ¸
                                if airflow_state in ["success", "failed"]:
                                    job.status = "completed" if airflow_state == "success" else "failed"
                                    job.completed_at = datetime.utcnow()
                                    job.progress = 100.0
                                    
                                    if airflow_state == "failed":
                                        # Task Instances í™•ì¸í•˜ì—¬ ë” ìì„¸í•œ ì—ëŸ¬ ì •ë³´ ì¶”ì¶œ
                                        try:
                                            task_response = requests.get(
                                                f"{AIRFLOW_API}/dags/mica_pipeline/dagRuns/{job.job_id}/taskInstances",
                                                auth=_auth(),
                                                timeout=5
                                            )
                                            if task_response.status_code == 200:
                                                tasks = task_response.json().get("task_instances", [])
                                                failed_tasks = [t for t in tasks if t.get("state") == "failed"]
                                                if failed_tasks:
                                                    job.error_message = f"Airflow DAG execution failed. Failed tasks: {', '.join([t.get('task_id', 'unknown') for t in failed_tasks])}"
                                                else:
                                                    job.error_message = "Airflow DAG execution failed. Check Airflow UI for details."
                                        except Exception:
                                            job.error_message = "Airflow DAG execution failed. Check Airflow UI for details."
                                    
                                    db.commit()
                                    print(f"âœ… Updated job {job.job_id} status from Airflow: {airflow_state}")
                                elif airflow_state == "running":
                                    # running ìƒíƒœì´ì§€ë§Œ ì˜¤ë˜ ì‹¤í–‰ ì¤‘ì´ë©´ ì²´í¬
                                    start_date = airflow_data.get("start_date")
                                    if start_date:
                                        try:
                                            from datetime import datetime as dt
                                            start_dt = dt.fromisoformat(start_date.replace('Z', '+00:00'))
                                            elapsed = (datetime.utcnow() - start_dt.replace(tzinfo=None)).total_seconds()
                                            # 2ì‹œê°„ ì´ìƒ ì‹¤í–‰ ì¤‘ì´ë©´ ë¡œê·¸ í™•ì¸
                                            if elapsed > 7200:  # 2ì‹œê°„
                                                # ë¡œê·¸ íŒŒì¼ í™•ì¸
                                                log_path = Path(job.log_file) if job.log_file else None
                                                if log_path and log_path.exists():
                                                    try:
                                                        with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
                                                            log_content = f.read()
                                                            error_keywords = ["[ ERROR ]", "ERROR", "FATAL", "failed", "Failed", "FAILED"]
                                                            if any(keyword in log_content for keyword in error_keywords):
                                                                # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
                                                                job.status = "failed"
                                                                job.completed_at = datetime.utcnow()
                                                                job.progress = 100.0
                                                                error_lines = [line for line in log_content.split('\n') if any(kw in line for kw in error_keywords)]
                                                                job.error_message = '\n'.join(error_lines[-5:]) if error_lines else "Pipeline execution failed (stalled with errors)"
                                                                db.commit()
                                                                print(f"âš ï¸ Job {job.job_id} stalled with errors, marked as failed")
                                                    except Exception:
                                                        pass
                                        except Exception:
                                            pass
                            elif airflow_response.status_code == 404:
                                # DAG Runì´ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
                                job.status = "failed"
                                job.completed_at = datetime.utcnow()
                                job.progress = 100.0
                                job.error_message = "Airflow DAG Run not found (may have been deleted)"
                                db.commit()
                                print(f"âš ï¸ Job {job.job_id} DAG Run not found in Airflow, marked as failed")
                        except Exception as e:
                            print(f"Failed to check Airflow status for {job.job_id}: {e}")
                    
                    # ì§ì ‘ ì‹¤í–‰ëœ jobì˜ ê²½ìš° Docker ì»¨í…Œì´ë„ˆ í™•ì¸
                    else:
                        # ë¨¼ì € ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
                        container_check = subprocess.run(
                            f"docker ps --filter 'name={job.container_name}' --format '{{{{.Names}}}}'",
                            shell=True,
                            capture_output=True,
                            text=True,
                            timeout=3  # íƒ€ì„ì•„ì›ƒì„ 3ì´ˆë¡œ ë‹¨ì¶•
                        )
                        
                        container_running = container_check.returncode == 0 and container_check.stdout.strip() == job.container_name
                        
                        if not container_running:
                            # ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹˜ = ì™„ë£Œ ë˜ëŠ” ì‹¤íŒ¨
                            # ë¡œê·¸ íŒŒì¼ì—ì„œ ì—ëŸ¬ í™•ì¸
                            has_error = False
                            error_message = None
                            
                            # 1. ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ í™•ì¸
                            error_log_path = Path(job.error_log_file) if job.error_log_file else None
                            if error_log_path and error_log_path.exists():
                                error_size = error_log_path.stat().st_size
                                if error_size > 0:
                                    has_error = True
                                    try:
                                        with open(error_log_path, 'r', encoding='utf-8', errors='ignore') as f:
                                            error_message = f.read()[-500:]
                                    except Exception:
                                        error_message = "Error log file exists but cannot be read"
                            
                            # 2. í‘œì¤€ ì¶œë ¥ ë¡œê·¸ì—ì„œ "[ ERROR ]" í™•ì¸ (MICA Pipelineì€ exit 0ë¡œ ì¢…ë£Œí•´ë„ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥)
                            if not has_error:
                                log_path = Path(job.log_file) if job.log_file else None
                                if log_path and log_path.exists():
                                    try:
                                        with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
                                            log_content = f.read()
                                            # ë” ì—„ê²©í•œ ì—ëŸ¬ ê²€ì‚¬
                                            error_keywords = ["[ ERROR ]", "ERROR", "FATAL", "failed", "Failed", "FAILED"]
                                            if any(keyword in log_content for keyword in error_keywords):
                                                has_error = True
                                                # ì—ëŸ¬ ë¶€ë¶„ ì¶”ì¶œ
                                                error_lines = [line for line in log_content.split('\n') if any(kw in line for kw in error_keywords)]
                                                error_message = '\n'.join(error_lines[-10:]) if error_lines else log_content[-500:]
                                    except Exception:
                                        pass
                            
                            # ìƒíƒœ ì—…ë°ì´íŠ¸
                            job.status = "failed" if has_error else "completed"
                            job.completed_at = datetime.utcnow()
                            job.progress = 100.0
                            if has_error:
                                job.error_message = error_message or "Pipeline execution failed (check logs for details)"
                            db.commit()
                            print(f"âœ… Updated job {job.job_id} status: {job.status} (container stopped)")
                        else:
                            # ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì´ì§€ë§Œ, ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ì‹¤íŒ¨ ì—¬ë¶€ ì²´í¬
                            log_path = Path(job.log_file) if job.log_file else None
                            if log_path and log_path.exists():
                                try:
                                    # ë¡œê·¸ íŒŒì¼ì´ ìµœê·¼ì— ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸ (5ë¶„ ì´ìƒ ì—…ë°ì´íŠ¸ ì•ˆ ë˜ë©´ ë¬¸ì œ ê°€ëŠ¥)
                                    log_mtime = log_path.stat().st_mtime
                                    time_since_update = time.time() - log_mtime
                                    
                                    # 5ë¶„ ì´ìƒ ë¡œê·¸ê°€ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šì•˜ê³ , ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
                                    if time_since_update > 300:  # 5ë¶„
                                        with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
                                            log_content = f.read()
                                            error_keywords = ["[ ERROR ]", "ERROR", "FATAL", "failed", "Failed", "FAILED"]
                                            if any(keyword in log_content for keyword in error_keywords):
                                                # ì»¨í…Œì´ë„ˆë¥¼ ì¤‘ì§€í•˜ê³  ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
                                                subprocess.run(f"docker stop {job.container_name}", shell=True, timeout=10)
                                                job.status = "failed"
                                                job.completed_at = datetime.utcnow()
                                                job.progress = 100.0
                                                error_lines = [line for line in log_content.split('\n') if any(kw in line for kw in error_keywords)]
                                                job.error_message = '\n'.join(error_lines[-10:]) if error_lines else "Pipeline execution failed (stalled)"
                                                db.commit()
                                                print(f"âš ï¸ Job {job.job_id} stalled with errors, marked as failed")
                                except Exception as e:
                                    print(f"Error checking log for {job.job_id}: {e}")
            
            # JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            jobs_data = []
            for job in jobs:
                jobs_data.append({
                    "id": job.id,
                    "job_id": job.job_id,
                    "subject_id": job.subject_id,
                    "session_id": job.session_id,
                    "processes": job.processes,
                    "container_name": job.container_name,
                    "pid": job.pid,
                    "status": job.status,
                    "progress": job.progress,
                    "log_file": job.log_file,
                    "error_log_file": job.error_log_file,
                    "started_at": job.started_at.isoformat() if job.started_at else None,
                    "completed_at": job.completed_at.isoformat() if job.completed_at else None,
                    "error_message": job.error_message,
                    "duration": (job.completed_at - job.started_at).total_seconds() if job.completed_at else None
                })
            
            return {
                "success": True,
                "jobs": jobs_data,
                "count": len(jobs_data),
                "summary": {
                    "processing": sum(1 for j in jobs if j.status == "processing"),
                    "completed": sum(1 for j in jobs if j.status == "completed"),
                    "failed": sum(1 for j in jobs if j.status == "failed")
                }
            }
        finally:
            db.close()
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.post("/mica-job-update")
async def update_mica_job_status(data: dict):
    """MICA Pipeline Job ìƒíƒœë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        job_id = data.get("job_id")
        status = data.get("status")
        progress = data.get("progress")
        error_message = data.get("error_message")
        
        if not job_id:
            raise HTTPException(status_code=400, detail="job_id is required")
        
        db = SessionLocal()
        try:
            job = db.query(MicaPipelineJob).filter(MicaPipelineJob.job_id == job_id).first()
            
            if not job:
                raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
            
            if status:
                job.status = status
                if status in ["completed", "failed"]:
                    job.completed_at = datetime.utcnow()
                    job.progress = 100.0
            
            if progress is not None:
                job.progress = progress
            
            if error_message:
                job.error_message = error_message
            
            db.commit()
            
            return {
                "success": True,
                "job_id": job_id,
                "status": job.status,
                "message": "Job status updated"
            }
        finally:
            db.close()
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

# --- ìœ í‹¸: ë°ì´í„° ë£¨íŠ¸ ê²°ì • (/app/data ê¸°ë³¸) ---
def pick_existing_data_root() -> Path:
    # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ ìš°ì„ )
    possible_roots = [
        Path("/app/data"),  # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ (ë§ˆìš´íŠ¸ë¨)
        Path(os.getenv("HOST_DATA_DIR", "/home/admin1/Documents/aimedpipeline/data")),  # í˜¸ìŠ¤íŠ¸ ê²½ë¡œ
    ]
    
    for root in possible_roots:
        derivatives_path = root / "derivatives"
        if derivatives_path.exists():
            return root
    
    # ëª¨ë“  ê²½ë¡œê°€ ì‹¤íŒ¨í•œ ê²½ìš°
    tried_paths = [str(r / "derivatives") for r in possible_roots]
    raise HTTPException(
        status_code=404, 
        detail=f"Derivatives not found. Tried: {', '.join(tried_paths)}"
    )

# --- ë³´ì•ˆìš©: derivatives ë°”ê¹¥ ì ‘ê·¼ ë°©ì§€ ---
def _ensure_inside(root: Path, target: Path) -> Path:
    root_r = root.resolve()
    target_r = target.resolve()
    if not str(target_r).startswith(str(root_r)):
        raise HTTPException(status_code=403, detail="Path traversal detected")
    return target_r

# --- ì „ì²´ derivativesë¥¼ ZIPìœ¼ë¡œ ë°˜í™˜ ---
@app.get("/download-derivatives")
def download_derivatives():
    data_root = pick_existing_data_root()
    derivatives_root = (data_root / "derivatives").resolve()

    if not derivatives_root.exists():
        raise HTTPException(status_code=404, detail=f"Target not found: {derivatives_root}")

    # ZIP íŒŒì¼ ê²½ë¡œ ì¤€ë¹„
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"derivatives_all_{ts}.zip"
    tmp_dir = Path("/app/tmp")
    tmp_dir.mkdir(parents=True, exist_ok=True)
    zip_path = tmp_dir / zip_name

    # ZIP ìƒì„±
    import zipfile
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
        for p in derivatives_root.rglob("*"):
            if p.is_file():
                arcname = p.relative_to(derivatives_root)
                zf.write(p, arcname=str(arcname))

    return FileResponse(
        path=str(zip_path),
        filename=zip_name,
        media_type="application/zip",
    )

@app.get("/system-resources")
async def get_system_resources_endpoint():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        return get_system_resources()
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /system-resources: {error_detail}")
        return {
            "success": False,
            "error": str(e)
        }