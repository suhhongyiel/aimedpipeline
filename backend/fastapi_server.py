import os
import uuid
import requests
from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime
import shutil
import json
from typing import List
import zipfile
import tarfile
# tokenize for shell command ÏïàÏ†Ñ Ï≤òÎ¶¨
import shlex
# db Í¥ÄÎ†®
# --- DB Í¥ÄÎ†® import ÏàòÏ†ï ---
from database import SessionLocal
from models import JobLog, CommandLog, MicaPipelineJob
import models
from database import engine
from sqlalchemy.orm import Session
from fastapi import Depends

# ===== ÌôòÍ≤ΩÎ≥ÄÏàò Í∏∞Î≥∏Í∞í =====
AIRFLOW_BASE = os.getenv("AIRFLOW_BASE_URL", "http://airflow:8080")
#AIRFLOW_BASE = os.getenv("AIRFLOW_BASE_URL", "http://localhost:8080")
AIRFLOW_API  = f"{AIRFLOW_BASE}/api/v1"
AIRFLOW_DAG  = os.getenv("AIRFLOW_DAG_ID", "mica_pipeline")
AIRFLOW_USER = os.getenv("AIRFLOW_USER", "admin")
AIRFLOW_PASS = os.getenv("AIRFLOW_PASSWORD", "admin")

# --- ÏÑúÎ≤Ñ ÏãúÏûë Ïãú DB ÌÖåÏù¥Î∏î ÏÉùÏÑ± ---
# Ïù¥ÎØ∏ ÏûàÏúºÎ©¥ Î≥Ñ ÏûëÏóÖ ÏïàÌï®
models.Base.metadata.create_all(bind=engine)
# --------------------------------
app = FastAPI(title="AIMed Pipeline Backend")

# Streamlit(Î≥ÑÎèÑ Ïª®ÌÖåÏù¥ÎÑà)ÏóêÏÑú Ìò∏Ï∂úÌïòÎØÄÎ°ú CORS ÌóàÏö©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

def _auth():
    return (AIRFLOW_USER, AIRFLOW_PASS)

# db session Ìï®Ïàò
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@app.on_event("startup")
def on_startup():
    models.Base.metadata.create_all(bind=engine)

@app.get("/")
def root():
    return {"ok": True, "airflow": AIRFLOW_BASE, "dag": AIRFLOW_DAG}

@app.post("/run-job")
def run_job(job_type: str = "MRI Î∂ÑÏÑù", db: Session = Depends(get_db)):
    run_id = f"ui_{uuid.uuid4().hex[:8]}"
    payload = {"dag_run_id": run_id, "conf": {"job_type": job_type}}
    # Airflow mock (Ïã§Ï†ú Airflow ÏóÜÏù¥)
    try:
        r = requests.post(f"{AIRFLOW_API}/dags/{AIRFLOW_DAG}/dagRuns", json=payload, auth=_auth(), timeout=2)
        if r.status_code not in (200, 201):
            raise HTTPException(status_code=r.status_code, detail=r.text)
    except Exception as e:
        print(f"[Mock] Airflow Ìò∏Ï∂ú Ïã§Ìå®, ÌÖåÏä§Ìä∏Ïö©ÏúºÎ°ú Î¨¥Ïãú: {e}")
        
    # ---- Î°úÍ∑∏ DBÏóê Ï†ÄÏû• ----
    log = JobLog(
        job_id=run_id,
        job_type=job_type,
        status="requested",
        log="Job requested (mock)"
    )
    db.add(log)
    db.commit()
    return {"job_id": run_id, "note": "Airflow ÎØ∏Ïó∞Îèô mock"}

@app.get("/job-status/{job_id}")
def job_status(job_id: str):
    try:
        # Ïã§Ï†ú Airflow REST API Ìò∏Ï∂ú Î∂ÄÎ∂Ñ
        dr = requests.get(f"{AIRFLOW_API}/dags/{AIRFLOW_DAG}/dagRuns/{job_id}", auth=_auth(), timeout=2)
        if dr.status_code != 200:
            raise HTTPException(status_code=dr.status_code, detail=dr.text)
        drj = dr.json()
        state = drj.get("state", "unknown")

        tis = requests.get(
            f"{AIRFLOW_API}/dags/{AIRFLOW_DAG}/dagRuns/{job_id}/taskInstances",
            auth=_auth(), timeout=2
        )
        if tis.status_code != 200:
            raise HTTPException(status_code=tis.status_code, detail=tis.text)
        tasks = []
        for ti in tis.json().get("task_instances", []):
            tasks.append({
                "task_id": ti["task_id"],
                "state": ti["state"],
                "start_date": ti.get("start_date"),
                "end_date": ti.get("end_date"),
                "try_number": ti.get("try_number"),
            })

        total = max(len(tasks), 1)
        success = sum(1 for t in tasks if (t["state"] or "").lower() == "success")
        running = sum(1 for t in tasks if (t["state"] or "").lower() in ("queued", "running"))
        progress = int((success/total)*100 + (running/total)*25)
        progress = min(progress, 99 if state.lower() not in ("success", "failed") else 100)

        ui_url = f"{AIRFLOW_BASE}/dag/{AIRFLOW_DAG}/grid?dag_run_id={job_id}"
        return {
            "status": state, "tasks": tasks, "progress": progress,
            "airflow_ui": ui_url, "log": f"DagRun {job_id} is {state}"
        }
    except Exception as e:
        # Airflow ÏöîÏ≤≠ Ïã§Ìå® Ïãú mock Îç∞Ïù¥ÌÑ∞ Î∞òÌôò (ÏóêÎü¨ Ïïà ÎÇòÍ≤å Ìï®)
        print(f"[Mock] Airflow ÏÉÅÌÉúÏ°∞Ìöå Ïã§Ìå®, ÎçîÎØ∏Í∞í Î∞òÌôò: {e}")
        return {
            "status": "mocked",
            "tasks": [{"task_id": "mocked_task", "state": "success"}],
            "progress": 100,
            "airflow_ui": "mock_url",
            "log": f"Mock DagRun {job_id} is mocked"
        }
    
@app.post("/run-command")
async def run_command(data: dict):
    """ÏÑúÎ≤ÑÏóêÏÑú Î™ÖÎ†πÏñ¥Î•º Ïã§ÌñâÌïòÍ≥† Í≤∞Í≥ºÎ•º Î∞òÌôòÌï©ÎãàÎã§."""
    cmd = data.get("cmd")
    if not cmd:
        raise HTTPException(status_code=400, detail="Command is required")
    
    # ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï (Î≥ºÎ•® ÎßàÏö¥Ìä∏Îêú Í≤ΩÎ°ú ÏÇ¨Ïö©)
    work_dir = data.get("work_dir", "/app/workspace")
    work_dir = os.path.abspath(work_dir) if not work_dir.startswith("/") else work_dir
    
    # ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏúºÎ©¥ ÏÉùÏÑ±
    os.makedirs(work_dir, exist_ok=True)
    
    start_time = datetime.now()
    try:
        result = subprocess.run(
            cmd, 
            shell=True, 
            capture_output=True, 
            text=True,
            cwd=work_dir,
            timeout=data.get("timeout", 300)  # Í∏∞Î≥∏ 5Î∂Ñ ÌÉÄÏûÑÏïÑÏõÉ
        )
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # DBÏóê Î°úÍ∑∏ Ï†ÄÏû•
        session = SessionLocal()
        try:
            log = CommandLog(
                command=cmd,
                output=result.stdout,
                error=result.stderr
            )
            session.add(log)
            session.commit()
        finally:
            session.close()
        
        return {
            "success": result.returncode == 0,
            "output": result.stdout,
            "error": result.stderr,
            "returncode": result.returncode,
            "work_dir": work_dir,
            "duration": f"{duration:.2f}s",
            "timestamp": datetime.now().isoformat()
        }
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "output": "",
            "error": f"Command timeout after {data.get('timeout', 300)} seconds",
            "returncode": -1,
            "work_dir": work_dir,
            "duration": None,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "success": False,
            "output": "",
            "error": str(e),
            "returncode": -1,
            "work_dir": work_dir,
            "duration": None,
            "timestamp": datetime.now().isoformat()
        }

@app.get("/list-files")
async def list_files(path: str = "/app/workspace"):
    """ÏßÄÏ†ïÎêú Í≤ΩÎ°úÏùò ÌååÏùº Î™©Î°ùÏùÑ Î∞òÌôòÌï©ÎãàÎã§."""
    try:
        path_obj = Path(path)
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail=f"Path not found: {path}")
        
        files = []
        for item in path_obj.iterdir():
            try:
                stat = item.stat()
                files.append({
                    "name": item.name,
                    "path": str(item),
                    "type": "directory" if item.is_dir() else "file",
                    "size": stat.st_size if item.is_file() else None,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "permissions": oct(stat.st_mode)[-3:]
                })
            except PermissionError:
                continue
        
        # Ïù¥Î¶ÑÏàú Ï†ïÎ†¨
        files.sort(key=lambda x: (x["type"] == "file", x["name"]))
        
        return {
            "path": path,
            "files": files,
            "count": len(files)
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.post("/create-file")
async def create_file(data: dict):
    """ÌååÏùºÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§."""
    file_path = data.get("file_path")
    content = data.get("content", "")
    work_dir = data.get("work_dir", "/app/workspace")
    
    if not file_path:
        raise HTTPException(status_code=400, detail="file_path is required")
    
    # Ï†àÎåÄ Í≤ΩÎ°úÍ∞Ä ÏïÑÎãàÎ©¥ work_dir Í∏∞Ï§ÄÏúºÎ°ú ÏÉÅÎåÄ Í≤ΩÎ°ú Ï≤òÎ¶¨
    if not file_path.startswith("/"):
        file_path = os.path.join(work_dir, file_path)
    
    try:
        path_obj = Path(file_path)
        # ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏúºÎ©¥ ÏÉùÏÑ±
        path_obj.parent.mkdir(parents=True, exist_ok=True)
        
        # ÌååÏùº ÏÉùÏÑ±
        path_obj.write_text(content, encoding="utf-8")
        
        return {
            "success": True,
            "file_path": str(path_obj),
            "size": path_obj.stat().st_size,
            "message": f"File created: {file_path}"
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.delete("/delete-file")
async def delete_file(file_path: str, work_dir: str = "/app/workspace"):
    """ÌååÏùº ÎòêÎäî ÎîîÎ†âÌÜ†Î¶¨Î•º ÏÇ≠Ï†úÌï©ÎãàÎã§."""
    if not file_path:
        raise HTTPException(status_code=400, detail="file_path is required")
    
    # Ï†àÎåÄ Í≤ΩÎ°úÍ∞Ä ÏïÑÎãàÎ©¥ work_dir Í∏∞Ï§ÄÏúºÎ°ú ÏÉÅÎåÄ Í≤ΩÎ°ú Ï≤òÎ¶¨
    if not file_path.startswith("/"):
        file_path = os.path.join(work_dir, file_path)
    
    try:
        path_obj = Path(file_path)
        
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail=f"Path not found: {file_path}")
        
        # ÏïàÏ†ÑÏû•Ïπò: ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨ Ïô∏Î∂Ä ÏÇ≠Ï†ú Î∞©ÏßÄ
        if not str(path_obj.resolve()).startswith("/app"):
            raise HTTPException(status_code=403, detail="Cannot delete files outside /app directory")
        
        if path_obj.is_dir():
            import shutil
            shutil.rmtree(path_obj)
            item_type = "directory"
        else:
            path_obj.unlink()
            item_type = "file"
        
        return {
            "success": True,
            "file_path": str(path_obj),
            "type": item_type,
            "message": f"{item_type.capitalize()} deleted: {file_path}"
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.get("/read-file")
async def read_file(file_path: str, work_dir: str = "/app/workspace"):
    """ÌååÏùº ÎÇ¥Ïö©ÏùÑ ÏùΩÏäµÎãàÎã§."""
    if not file_path:
        raise HTTPException(status_code=400, detail="file_path is required")
    
    # Ï†àÎåÄ Í≤ΩÎ°úÍ∞Ä ÏïÑÎãàÎ©¥ work_dir Í∏∞Ï§ÄÏúºÎ°ú ÏÉÅÎåÄ Í≤ΩÎ°ú Ï≤òÎ¶¨
    if not file_path.startswith("/"):
        file_path = os.path.join(work_dir, file_path)
    
    try:
        path_obj = Path(file_path)
        
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail=f"File not found: {file_path}")
        
        if not path_obj.is_file():
            raise HTTPException(status_code=400, detail=f"Path is not a file: {file_path}")
        
        # ÏïàÏ†ÑÏû•Ïπò: ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨ Ïô∏Î∂Ä ÌååÏùº ÏùΩÍ∏∞ Î∞©ÏßÄ
        if not str(path_obj.resolve()).startswith("/app"):
            raise HTTPException(status_code=403, detail="Cannot read files outside /app directory")
        
        content = path_obj.read_text(encoding="utf-8")
        stat = path_obj.stat()
        
        return {
            "file_path": str(path_obj),
            "content": content,
            "size": stat.st_size,
            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.post("/upload-file")
async def upload_file(
    files: List[UploadFile] = File(...),
    destination: str = Form("/app/data"),
    extract_archives: bool = Form(True)
):
    """
    ÌååÏùºÏùÑ ÏÑúÎ≤ÑÏóê ÏóÖÎ°úÎìúÌï©ÎãàÎã§.
    ÏïïÏ∂ï ÌååÏùº(.zip, .tar.gz, .tgz)ÏùÄ ÏûêÎèôÏúºÎ°ú ÏïïÏ∂ï Ìï¥Ï†úÌï† Ïàò ÏûàÏäµÎãàÎã§.
    """
    try:
        # Î™©Ï†ÅÏßÄ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        dest_path = Path(destination)
        dest_path.mkdir(parents=True, exist_ok=True)
        
        uploaded_files = []
        extracted_files = []
        total_size = 0
        
        for file in files:
            # ÌååÏùº Ï†ÄÏû•
            file_path = dest_path / file.filename
            
            # ÌååÏùº Ïì∞Í∏∞
            with file_path.open("wb") as buffer:
                content = await file.read()
                buffer.write(content)
            
            file_size = file_path.stat().st_size
            total_size += file_size
            
            file_info = {
                "filename": file.filename,
                "path": str(file_path),
                "size": file_size,
                "content_type": file.content_type,
                "extracted": False
            }
            
            # ÏïïÏ∂ï ÌååÏùº ÏûêÎèô ÏïïÏ∂ï Ìï¥Ï†ú
            if extract_archives:
                extracted = False
                
                # ZIP ÌååÏùº Ï≤òÎ¶¨
                if file.filename.lower().endswith('.zip'):
                    try:
                        with zipfile.ZipFile(file_path, 'r') as zip_ref:
                            zip_ref.extractall(dest_path)
                        extracted = True
                        extracted_files.extend(zip_ref.namelist())
                        file_info["extracted"] = True
                        file_info["archive_type"] = "zip"
                    except Exception as e:
                        file_info["extraction_error"] = str(e)
                
                # TAR.GZ ÎòêÎäî TGZ ÌååÏùº Ï≤òÎ¶¨
                elif file.filename.lower().endswith(('.tar.gz', '.tgz', '.tar')):
                    try:
                        with tarfile.open(file_path, 'r:*') as tar_ref:
                            tar_ref.extractall(dest_path)
                        extracted = True
                        extracted_files.extend([m.name for m in tar_ref.getmembers()])
                        file_info["extracted"] = True
                        file_info["archive_type"] = "tar"
                    except Exception as e:
                        file_info["extraction_error"] = str(e)
                
                # ÏïïÏ∂ï Ìï¥Ï†ú ÏÑ±Í≥µ Ïãú ÏõêÎ≥∏ ÏïïÏ∂ï ÌååÏùº ÏÇ≠Ï†ú (ÏÑ†ÌÉùÏÇ¨Ìï≠)
                if extracted:
                    file_path.unlink()
                    file_info["original_removed"] = True
            
            uploaded_files.append(file_info)
        
        response = {
            "success": True,
            "uploaded_files": uploaded_files,
            "count": len(uploaded_files),
            "total_size": total_size,
            "destination": destination,
            "message": f"{len(uploaded_files)} file(s) uploaded successfully"
        }
        
        if extracted_files:
            response["extracted_files_count"] = len(extracted_files)
            response["extracted_files_sample"] = extracted_files[:10]  # Ï≤òÏùå 10Í∞úÎßå ÌëúÏãú
        
        return response
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.post("/validate-bids")
async def validate_bids(directory: str = "/app/data/bids"):
    """BIDS Ìè¨Îß∑ Í≤ÄÏ¶ùÏùÑ ÏàòÌñâÌï©ÎãàÎã§."""
    try:
        dir_path = Path(directory)
        
        if not dir_path.exists():
            raise HTTPException(status_code=404, detail=f"Directory not found: {directory}")
        
        # Î¨¥ÏãúÌï† ÏãúÏä§ÌÖú Ìè¥Îçî/ÌååÏùº
        ignore_list = {"__MACOSX", ".DS_Store", "Thumbs.db", ".git", ".gitignore"}
        
        # BIDS Í≤ÄÏ¶ù Í≤∞Í≥º
        validation_result = {
            "is_valid": False,
            "errors": [],
            "warnings": [],
            "directory": directory,
            "structure": {},
            "details": []
        }
        
        # ÌïÑÏàò ÌååÏùº/Ìè¥Îçî Ï≤¥ÌÅ¨
        required_items = {
            "dataset_description.json": False,
            "README": False,
            "participants.tsv": False
        }
        
        # ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞ Î∂ÑÏÑù
        all_items = []
        subject_dirs = []
        
        for item in dir_path.iterdir():
            # ÏãúÏä§ÌÖú Ìè¥Îçî/ÌååÏùº Î¨¥Ïãú
            if item.name in ignore_list:
                validation_result["warnings"].append(f"Ignored system file/folder: {item.name}")
                continue
            
            all_items.append(item.name)
            
            if item.name in required_items:
                required_items[item.name] = True
                validation_result["structure"][item.name] = "found"
                validation_result["details"].append(f"‚úì {item.name}")
            elif item.is_dir() and item.name.startswith("sub-"):
                subject_dirs.append(item.name)
                validation_result["structure"][item.name] = "subject_directory"
                
                # Subject Ìè¥Îçî ÎÇ¥Î∂Ä ÌôïÏù∏
                sub_folders = [f.name for f in item.iterdir() if f.is_dir() and f.name not in ignore_list]
                if sub_folders:
                    validation_result["details"].append(f"‚úì {item.name}/ ‚Üí {', '.join(sub_folders)}")
                else:
                    validation_result["warnings"].append(f"{item.name} folder is empty")
        
        # ÌïÑÏàò Ìï≠Î™© Í≤ÄÏÇ¨
        missing_items = [k for k, v in required_items.items() if not v]
        
        if missing_items:
            for item in missing_items:
                validation_result["errors"].append(f"‚úó Missing required file: {item}")
                validation_result["details"].append(f"‚úó {item} (missing)")
        
        # subject Ìè¥Îçî Í∞úÏàò ÌôïÏù∏
        validation_result["subject_count"] = len(subject_dirs)
        validation_result["subject_list"] = subject_dirs[:10]  # Ï≤òÏùå 10Í∞úÎßå
        
        if len(subject_dirs) == 0:
            validation_result["errors"].append("No subject directories found (sub-*)")
        else:
            validation_result["details"].append(f"‚úì Found {len(subject_dirs)} subject(s)")
        
        # dataset_description.json Í≤ÄÏ¶ù
        desc_file = dir_path / "dataset_description.json"
        if desc_file.exists():
            try:
                with desc_file.open("r", encoding="utf-8") as f:
                    desc_data = json.load(f)
                    required_fields = ["Name", "BIDSVersion"]
                    missing_fields = [f for f in required_fields if f not in desc_data]
                    
                    if missing_fields:
                        validation_result["errors"].append(
                            f"dataset_description.json missing fields: {', '.join(missing_fields)}"
                        )
                    else:
                        validation_result["dataset_info"] = {
                            "name": desc_data.get("Name"),
                            "version": desc_data.get("BIDSVersion"),
                            "dataset_type": desc_data.get("DatasetType", "unknown")
                        }
                        validation_result["details"].append(
                            f"‚úì Dataset: {desc_data.get('Name')} (BIDS {desc_data.get('BIDSVersion')})"
                        )
            except json.JSONDecodeError as e:
                validation_result["errors"].append(f"dataset_description.json is not valid JSON: {str(e)}")
            except Exception as e:
                validation_result["errors"].append(f"Error reading dataset_description.json: {str(e)}")
        
        # README ÌååÏùº ÌôïÏù∏
        readme_file = dir_path / "README"
        if readme_file.exists():
            try:
                readme_size = readme_file.stat().st_size
                validation_result["details"].append(f"‚úì README ({readme_size} bytes)")
            except:
                pass
        
        # participants.tsv ÌôïÏù∏
        participants_file = dir_path / "participants.tsv"
        if participants_file.exists():
            try:
                with participants_file.open("r", encoding="utf-8") as f:
                    lines = f.readlines()
                    validation_result["details"].append(f"‚úì participants.tsv ({len(lines)} lines)")
                    validation_result["participants_count"] = len(lines) - 1  # Ìó§Îçî Ï†úÏô∏
            except:
                pass
        
        # ÏµúÏ¢Ö Í≤ÄÏ¶ù Í≤∞Í≥º
        if not validation_result["errors"]:
            validation_result["is_valid"] = True
            validation_result["message"] = "‚úÖ Valid BIDS dataset!"
        else:
            validation_result["is_valid"] = False
            validation_result["message"] = f"‚ùå Invalid BIDS dataset ({len(validation_result['errors'])} error(s))"
        
        return validation_result
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.get("/get-sessions")
async def get_sessions(subject_id: str, bids_dir: str = "/app/data/bids"):
    """ÌäπÏ†ï SubjectÏùò Session Î™©Î°ùÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§."""
    try:
        # Ìò∏Ïä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨ ÌôïÏù∏
        host_data_dir = os.getenv("HOST_DATA_DIR", "/home/admin1/Documents/aimedpipeline/data")
        
        # Subject ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú Íµ¨ÏÑ±
        sub_dirname = subject_id if subject_id.startswith("sub-") else f"sub-{subject_id}"
        
        # Ïó¨Îü¨ Í≤ΩÎ°ú ÏãúÎèÑ (Ïª®ÌÖåÏù¥ÎÑà ÎÇ¥Î∂Ä Í≤ΩÎ°ú Ïö∞ÏÑ†, Ìò∏Ïä§Ìä∏ Í≤ΩÎ°úÎäî Î∞±ÏóÖ)
        possible_paths = []
        
        # 1. Ïª®ÌÖåÏù¥ÎÑà ÎÇ¥Î∂Ä Í≤ΩÎ°ú (ÎßàÏö¥Ìä∏Îêú Í≤ΩÎ°ú)
        if bids_dir.startswith('/app/data/'):
            possible_paths.append(Path(bids_dir) / sub_dirname)
            possible_paths.append(Path(bids_dir) / subject_id)
        
        # 2. ÏßÅÏ†ë Ïª®ÌÖåÏù¥ÎÑà Í≤ΩÎ°ú ÏãúÎèÑ
        possible_paths.append(Path("/app/data/bids") / sub_dirname)
        possible_paths.append(Path("/app/data/bids") / subject_id)
        
        # 3. Ìò∏Ïä§Ìä∏ Í≤ΩÎ°ú (BackendÍ∞Ä Ìò∏Ïä§Ìä∏ÏóêÏÑú Ïã§Ìñâ Ï§ëÏù∏ Í≤ΩÏö∞)
        possible_paths.append(Path(host_data_dir) / "bids" / sub_dirname)
        possible_paths.append(Path(host_data_dir) / "bids" / subject_id)
        
        # 4. ÏÉÅÎåÄ Í≤ΩÎ°úÎèÑ ÏãúÎèÑ (BackendÍ∞Ä ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ÏóêÏÑú Ïã§Ìñâ Ï§ëÏù∏ Í≤ΩÏö∞)
        possible_paths.append(Path("./data/bids") / sub_dirname)
        possible_paths.append(Path("./data/bids") / subject_id)
        
        found_path = None
        for path in possible_paths:
            try:
                if path.exists():
                    found_path = path
                    break
            except Exception:
                continue
        
        if not found_path:
            # ÏóêÎü¨ Î©îÏãúÏßÄÏóê ÏãúÎèÑÌïú Í≤ΩÎ°úÎì§ Ìè¨Ìï®
            tried_paths = [str(p) for p in possible_paths[:4]]  # Ï≤òÏùå 4Í∞úÎßå ÌëúÏãú
            return {
                "success": False,
                "sessions": [],
                "message": f"Subject directory not found. Tried: {', '.join(tried_paths)}"
            }
        
        # Session ÎîîÎ†âÌÜ†Î¶¨ Ï∞æÍ∏∞
        session_dirs = []
        try:
            for item in found_path.iterdir():
                if item.is_dir() and item.name.startswith("ses-"):
                    session_id = item.name.replace("ses-", "")
                    session_dirs.append({
                        "session_id": session_id,
                        "display_name": item.name,  # ses-M126
                        "full_name": item.name      # ses-M126
                    })
        except Exception as e:
            return {
                "success": False,
                "sessions": [],
                "message": f"Error reading directory {found_path}: {str(e)}"
            }
        
        # Session IDÎ°ú Ï†ïÎ†¨
        session_dirs.sort(key=lambda x: x["session_id"])
        
        return {
            "success": True,
            "subject_id": subject_id,
            "sessions": session_dirs,
            "count": len(session_dirs),
            "message": f"Found {len(session_dirs)} session(s) for {subject_id}",
            "debug_path": str(found_path)  # ÎîîÎ≤ÑÍπÖÏö©
        }
    except Exception as e:
        import traceback
        return {
            "success": False,
            "sessions": [],
            "message": f"Error: {str(e)}",
            "traceback": traceback.format_exc()
        }

async def run_mica_via_airflow(
    subject_id: str,
    session_id: str,
    processes: list,
    bids_dir: str,
    output_dir: str,
    fs_licence: str,
    threads: int,
    freesurfer: bool,
    user: str,
    proc_structural_flags: list | None = None,
    proc_surf_flags: list | None = None,
    post_structural_flags: list | None = None,
    proc_func_flags: list | None = None,
    dwi_flags: list | None = None,
    sc_flags: list | None = None,
):
    """Airflow DAGÎ•º Ìä∏Î¶¨Í±∞ÌïòÏó¨ MICA PipelineÏùÑ Ïã§ÌñâÌï©ÎãàÎã§."""
    try:
        # DAG Run ID ÏÉùÏÑ±
        run_id = f"mica_{subject_id.replace('sub-', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Airflow API ÌéòÏù¥Î°úÎìú
        payload = {
            "dag_run_id": run_id,
            "conf": {
                "subject_id": subject_id,
                "session_id": session_id,
                "processes": processes,
                "bids_dir": bids_dir,
                "output_dir": output_dir,
                "fs_licence": fs_licence,
                "threads": threads,
                "freesurfer": freesurfer,
                "user": user,
                "proc_structural_flags": proc_structural_flags or [],
                "proc_surf_flags": proc_surf_flags or [],
                "post_structural_flags": post_structural_flags or [],
                "proc_func_flags": proc_func_flags or [],
                "dwi_flags": dwi_flags or [],
                "sc_flags": sc_flags or [],
            }
        }
        
        # Airflow API Ìò∏Ï∂ú
        response = requests.post(
            f"{AIRFLOW_API}/dags/mica_pipeline/dagRuns",
            json=payload,
            auth=_auth(),
            timeout=10
        )
        
        if response.status_code in (200, 201):
            airflow_data = response.json()
            
            # DBÏóê Ï†ÄÏû•
            session = SessionLocal()
            try:
                container_name = f"{subject_id}"
                if session_id:
                    container_name += f"_ses-{session_id}"
                if processes:
                    container_name += f"_{processes[0]}"
                
                mica_job = MicaPipelineJob(
                    job_id=run_id,
                    subject_id=subject_id,
                    session_id=session_id,
                    processes=",".join(processes),
                    container_name=container_name,
                    pid=None,  # AirflowÍ∞Ä Í¥ÄÎ¶¨
                    status="processing",
                    progress=0.0,
                    log_file=f"{output_dir}/logs/{processes[0]}/fin/{container_name}.log",
                    error_log_file=f"{output_dir}/logs/{processes[0]}/error/{container_name}_error.log"
                )
                session.add(mica_job)
                session.commit()
            finally:
                session.close()
            
            return {
                "success": True,
                "mode": "airflow",
                "dag_run_id": run_id,
                "subject_id": subject_id,
                "session_id": session_id,
                "processes": processes,
                "user": user,
                "airflow_url": f"{AIRFLOW_BASE}/dags/mica_pipeline/grid?dag_run_id={run_id}",
                "message": f"‚úÖ MICA PipelineÏù¥ AirflowÎ•º ÌÜµÌï¥ ÏãúÏûëÎêòÏóàÏäµÎãàÎã§.\n\n"
                          f"DAG Run ID: {run_id}\n"
                          f"User: {user}\n"
                          f"Subject: {subject_id}\n\n"
                          f"üí° Airflow UIÏóêÏÑú Ïã§Ìñâ ÏÉÅÌÉúÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî: http://localhost:8080",
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Airflow API failed: {response.text}"
            )
    except requests.exceptions.RequestException as e:
        raise HTTPException(
            status_code=503,
            detail=f"Failed to connect to Airflow: {str(e)}"
        )


@app.post("/run-mica-pipeline")
async def run_mica_pipeline(data: dict):
    """mica-pipeline docker Î™ÖÎ†πÏùÑ Ïã§ÌñâÌï©ÎãàÎã§. (ÏßÅÏ†ë Ïã§Ìñâ ÎòêÎäî Airflow ÌÜµÌï¥ Ïã§Ìñâ)"""
    try:
        # Ïã§Ìñâ Î∞©Ïãù ÏÑ†ÌÉù
        use_airflow = data.get("use_airflow", False)
        user = data.get("user", "anonymous")
        
        # Ìò∏Ïä§Ìä∏Ïùò Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú (ÌôòÍ≤Ω Î≥ÄÏàòÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞)
        host_data_dir = os.getenv("HOST_DATA_DIR", "/home/admin1/Documents/aimedpipeline/data")
        
        # ÌïÑÏàò ÌååÎùºÎØ∏ÌÑ∞ ÌôïÏù∏ (Ïª®ÌÖåÏù¥ÎÑà ÎÇ¥Î∂Ä Í≤ΩÎ°ú)
        bids_dir = data.get("bids_dir", "./data/bids")
        output_dir = data.get("output_dir", "./data/derivatives")
        subject_id = data.get("subject_id")
        processes = data.get("processes", [])

        # ‚úÖ proc_structural Îã®ÎèÖ ÏÑ†ÌÉù Ïó¨Î∂Ä (ÎØ∏ÎãàÎ©Ä Î™®Îìú Ïä§ÏúÑÏπò)
        simple_structural = (processes == ["proc_structural"])
        
        # Ï∂îÍ∞Ä ÌååÎùºÎØ∏ÌÑ∞
        session_id = data.get("session_id", "")
        # session_idÏóêÏÑú "ses-" Ï†ëÎëêÏÇ¨ Ï†úÍ±∞ (ÏÇ¨Ïö©ÏûêÍ∞Ä "ses-01" ÌòïÏãùÏúºÎ°ú ÏûÖÎ†•Ìï† Ïàò ÏûàÏùå)
        if session_id:
            session_id = session_id.replace("ses-", "").strip()
        fs_licence = data.get("fs_licence", "./home/admin1/Documents/aimedpipeline/data/license.txt")
        threads = data.get("threads", 4)
        freesurfer = data.get("freesurfer", True)
    
        # ÌîÑÎ°úÏÑ∏Ïä§Î≥Ñ ÏÑ∏Î∂Ä ÌîåÎûòÍ∑∏
        # (ÏöîÍµ¨ÏÇ¨Ìï≠: proc_structural/proc_surfÎäî ÏòµÏÖò ÎØ∏ÏÇ¨Ïö©. post_structuralÎßå atlas ÌóàÏö©)
        proc_structural_flags = data.get("proc_structural_flags", [])
        proc_surf_flags = data.get("proc_surf_flags", [])
        post_structural_flags = data.get("post_structural_flags", [])
        proc_func_flags = data.get("proc_func_flags", [])
        dwi_flags = data.get("dwi_flags", [])
        sc_flags = data.get("sc_flags", [])

        # Ïú†Ìã∏ Ìï®ÏàòÎì§ -----------------------------------------------------------
        def join_tokens(tokens: list[str]) -> str:
            # Í∞Å ÌÜ†ÌÅ∞ÏùÑ shlex.quoteÎ°ú Í∞êÏã∏ ÏïàÏ†ÑÌïòÍ≤å Í≥µÎ∞±/ÌäπÏàòÎ¨∏Ïûê Ï≤òÎ¶¨
            return " ".join(shlex.quote(t) for t in tokens if t is not None and str(t) != "")

        def normalize_flags(tokens: list[str]) -> list[str]:
            """
            - Í∞í ÎèôÎ∞ò ÏòµÏÖòÏùÄ 'ÎßàÏßÄÎßâ Í∞í Ïö∞ÏÑ†'ÏúºÎ°ú 1ÌöåÎßå ÎÇ®ÍπÄ
            - ÌÜ†Í∏ÄÌòï ÌîåÎûòÍ∑∏Îäî Ï§ëÎ≥µ Ï†úÍ±∞
            - -freesurfer/-fs_licence Îäî Ïó¨Í∏∞ÏÑú Ï†úÍ±∞(Ï†ÑÏó≠ÏóêÏÑú 1ÌöåÎßå ÏÇΩÏûÖ)
            """
            with_val = {
                "-T1wStr", "-fs_licence", "-surf_dir", "-T1", "-atlas",
                "-mainScanStr", "-func_pe", "-func_rpe", "-mainScanRun",
                "-phaseReversalRun", "-topupConfig", "-icafixTraining",
                "-sesAnat"
            }
            kv = {}               # option -> value (ÎßàÏßÄÎßâ Í∞íÏù¥ ÎçÆÏñ¥ÏîÄ)
            toggles = set()       # ÌÜ†Í∏ÄÌòï Î™®Ïùå
            passthrough = []      # Í∏∞ÌÉÄ(Í∞í ÏóÜÎäî) ÌÜ†ÌÅ∞ Î≥¥Í¥Ä

            it = iter(tokens)
            for t in it:
                if t in with_val:
                    v = next(it, None)
                    if v is None or (isinstance(v, str) and v.startswith("-")):
                        continue
                    kv[t] = v
                else:
                    # -freesurfer/-fs_licence Îäî Ï†ÑÏó≠ÏóêÏÑúÎßå ÎÑ£Í∏∞: Ïó¨Í∏∞ÏÑú Ï†úÍ±∞
                    if t in ("-freesurfer",):
                        continue
                    if t == "-fs_licence":
                        _ = next(it, None)  # Í∞í ÏÜåÎ™®Îßå ÌïòÍ≥† Î≤ÑÎ¶º
                        continue
                    toggles.add(t) if t.startswith("-") else passthrough.append(t)

            out = []
            for k, v in kv.items():
                if k == "-fs_licence":
                    continue
                out += [k, v]

            out += sorted(t for t in toggles if t not in ("-freesurfer",))
            out += passthrough
            return out

        def convert_to_host_path(container_path: str) -> str:
            """Ïª®ÌÖåÏù¥ÎÑà Í≤ΩÎ°úÎ•º Ìò∏Ïä§Ìä∏ Í≤ΩÎ°úÎ°ú Î≥ÄÌôò"""
            if container_path.startswith("/app/data"):
                return container_path.replace("/app/data", host_data_dir)
            return container_path
        # ---------------------------------------------------------------------

        # Ìò∏Ïä§Ìä∏ Í≤ΩÎ°úÎ°ú Î≥ÄÌôò
        host_bids_dir = convert_to_host_path(bids_dir)
        host_output_dir = convert_to_host_path(output_dir)
        host_fs_licence = convert_to_host_path(fs_licence)
        
        if not subject_id:
            raise HTTPException(status_code=400, detail="subject_id is required")
        if not processes:
            raise HTTPException(status_code=400, detail="At least one process must be selected")
        
        # Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ± (Ïª®ÌÖåÏù¥ÎÑà ÎÇ¥Î∂Ä Í≤ΩÎ°ú)
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # AirflowÎ°ú ÎÑòÍ∏∞Îäî Í≤ΩÏö∞Îäî Í∏∞Ï°¥ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
        if use_airflow:
            return await run_mica_via_airflow(
                subject_id=subject_id,
                session_id=session_id,
                processes=processes,
                bids_dir=host_data_dir + "/bids",
                output_dir=host_data_dir + "/derivatives",
                fs_licence=host_data_dir + "/license.txt",
                threads=threads,
                freesurfer=freesurfer,
                user=user,
                # ÏïÑÎûò Îëê ÌîåÎûòÍ∑∏Îäî Î¨¥ÏãúÎêòÏßÄÎßå Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïú†ÏßÄ
                proc_structural_flags=proc_structural_flags,
                proc_surf_flags=proc_surf_flags,
                post_structural_flags=post_structural_flags,
                proc_func_flags=proc_func_flags,
                dwi_flags=dwi_flags,
                sc_flags=sc_flags,
            )

        # =========================
        # Ï†ÑÏ≤¥ Subject Ïã§Ìñâ (ALL)
        # =========================
        if subject_id.lower() == "all":
            bids_path = Path(bids_dir)
            if not bids_path.exists():
                raise HTTPException(status_code=404, detail=f"BIDS directory not found: {bids_dir}")
            
            subjects = [d.name for d in bids_path.iterdir() 
                        if d.is_dir() and d.name.startswith("sub-") and d.name != "__MACOSX"]
            if not subjects:
                raise HTTPException(status_code=400, detail="No subjects found in BIDS directory")
            
            all_results, total_success, total_failed = [], 0, 0
            
            for sub in subjects:
                try:
                    sub_id = sub.replace("sub-", "")
                    
                    # ÏÑ∏ÏÖò Î™©Î°ù ÏàòÏßë
                    if session_id:
                        sessions_to_process = [session_id]
                    else:
                        # bids_dirÍ∞Ä Ïª®ÌÖåÏù¥ÎÑà Í≤ΩÎ°úÏù∏ÏßÄ Ìò∏Ïä§Ìä∏ Í≤ΩÎ°úÏù∏ÏßÄ ÌôïÏù∏
                        if bids_dir.startswith('/app/data/'):
                            check_bids_dir = bids_dir.replace('/app/data/', f'{host_data_dir}/')
                        else:
                            check_bids_dir = bids_dir
                        
                        subject_path = Path(check_bids_dir) / sub
                        if subject_path.exists():
                            session_dirs = [d.name.replace("ses-", "") for d in subject_path.iterdir() 
                                            if d.is_dir() and d.name.startswith("ses-")]
                            sessions_to_process = session_dirs if session_dirs else [""]
                        else:
                            sessions_to_process = [""]

                    for ses in sessions_to_process:
                        container_name = f"{sub}" + (f"_ses-{ses}" if ses else "")
                        if processes:
                            container_name += f"_{processes[0]}"

                        container_log_dir = Path(output_dir) / "logs" / (processes[0] if processes else "")
                        container_log_dir.mkdir(parents=True, exist_ok=True)
                        (container_log_dir / "fin").mkdir(exist_ok=True)
                        (container_log_dir / "error").mkdir(exist_ok=True)

                        container_log_file = container_log_dir / "fin" / f"{container_name}.log"
                        container_error_log_file = container_log_dir / "error" / f"{container_name}_error.log"

                        # ---- Î∂ÑÍ∏∞: ÎØ∏ÎãàÎ©Ä vs ÏùºÎ∞ò ----
                        if simple_structural:
                            use_fs_licence_min = Path(fs_licence).exists() and ('proc_structural' in processes)

                            # docker run Î≥ºÎ•® ÎßàÏö¥Ìä∏
                            cmd = (
                                f"docker run --rm --name {container_name} "
                                f"-v {host_bids_dir}:{host_bids_dir} "
                                f"-v {host_output_dir}:{host_output_dir} "
                            )
                            if use_fs_licence_min:
                                cmd += f"-v {host_fs_licence}:{host_fs_licence} "

                            cmd += (
                                "micalab/micapipe:v0.2.3 "
                                f"-bids {host_bids_dir} "
                                f"-out {host_output_dir} "
                                f"-sub {sub_id} "
                            )
                            if ses:  # ses Î≥ÄÏàò ÏÇ¨Ïö©
                                cmd += f"-ses {ses} "

                            cmd += "-proc_structural "
                            if use_fs_licence_min:
                                cmd += f"-fs_licence {host_fs_licence} "

                            cmd += f"> {container_log_file} 2> {container_error_log_file}"
                        else:
                            # ÏùºÎ∞ò: Ïó¨Îü¨ ÌîÑÎ°úÏÑ∏Ïä§ Ï°∞Ìï©
                            base_switches = [f"-{p}" for p in processes]

                            # ÏòµÏÖò ÌîåÎûòÍ∑∏: post_structural + func + dwi + sc (struct/surf ÏòµÏÖòÏùÄ ÏÉùÎûµ)
                            extra_tokens = (
                                #(proc_structural_flags or []) + 
                                #(proc_surf_flags or []) + 
                                (post_structural_flags or []) +
                                (proc_func_flags or []) +
                                (dwi_flags or []) +
                                (sc_flags or [])
                            )
                            normalized = normalize_flags(extra_tokens)
                            process_flags = join_tokens(base_switches + normalized)

                            use_fs_licence = Path(fs_licence).exists() and (
                                ('proc_structural' in processes) or
                                ('proc_surf' in processes and freesurfer)
                            )

                            fs_licence_mount = ""
                            if use_fs_licence:
                                fs_licence_mount = f"-v {host_fs_licence}:{host_fs_licence}"

                            cmd = (
                                f"docker run --rm --name {container_name} "
                                f"-v {host_bids_dir}:{host_bids_dir} "
                                f"-v {host_output_dir}:{host_output_dir} "
                            )
                            if fs_licence_mount:
                                cmd += f"{fs_licence_mount} "

                            cmd += (
                                "micalab/micapipe:v0.2.3 "
                                f"-bids {host_bids_dir} "
                                f"-out {host_output_dir} "
                                f"-sub {sub_id} "
                            )
                            if ses:
                                cmd += f"-ses {ses} "

                            cmd += f"-threads {threads} {process_flags} "

                            if 'proc_surf' in processes:
                                cmd += f"-freesurfer {'TRUE' if freesurfer else 'FALSE'} "

                            # ÎùºÏù¥ÏÑ†Ïä§Îäî ÏúÑ Ï°∞Í±¥(use_fs_licence)Ïùº Îïå Ìï≠ÏÉÅ ÎÑòÍπÄ
                            if use_fs_licence:
                                cmd += f"-fs_licence {host_fs_licence} "

                            cmd += f"> {container_log_file} 2> {container_error_log_file}"

                        # Ïã§Ìñâ
                        process = subprocess.Popen(
                            cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
                        )

                        # DB Í∏∞Î°ù
                        db_session = SessionLocal()
                        try:
                            log = CommandLog(
                                command=cmd,
                                output=f"Container started: {container_name} (PID: {process.pid})",
                                error=""
                            )
                            db_session.add(log)
                            mica_job = MicaPipelineJob(
                                job_id=container_name,
                                subject_id=sub,
                                session_id=ses if ses else None,
                                processes=",".join(processes),
                                container_name=container_name,
                                pid=process.pid,
                                status="processing",
                                progress=0.0,
                                log_file=str(container_log_file),
                                error_log_file=str(container_error_log_file)
                            )
                            db_session.add(mica_job)
                            db_session.commit()
                        finally:
                            db_session.close()

                        total_success += 1
                        subject_with_session = f"{sub}" + (f"_ses-{ses}" if ses else "")
                        all_results.append({
                            "subject": subject_with_session,
                            "success": True,
                            "container_name": container_name,
                            "pid": process.pid,
                            "job_id": container_name,
                            "message": "Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú ÏãúÏûëÎê®"
                        })

                except Exception as e:
                    total_failed += 1
                    all_results.append({
                        "subject": sub,
                        "success": False,
                        "message": f"ÏãúÏûë Ïã§Ìå®: {str(e)}"
                    })
            
            return {
                "success": True,
                "mode": "all_subjects",
                "total_subjects": len(subjects),
                "successful": total_success,
                "failed": total_failed,
                "results": all_results,
                "processes": processes,
                "timestamp": datetime.now().isoformat(),
                "message": f"‚úÖ {total_success}Í∞úÏùò Ïª®ÌÖåÏù¥ÎÑàÍ∞Ä Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú ÏãúÏûëÎêòÏóàÏäµÎãàÎã§. (Ïã§Ìå®: {total_failed}Í∞ú)\n\nüí° 'Î°úÍ∑∏ ÌôïÏù∏' ÌÉ≠ÏóêÏÑú Ïã§Ìñâ ÏÉÅÌÉúÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî."
            }

        # =========================
        # Îã®Ïùº Subject Ïã§Ìñâ
        # =========================
        else:
            sub_id = subject_id.replace("sub-", "")
            
            # ÏÑ∏ÏÖò ÏûêÎèô Í∞êÏßÄ
            actual_session = session_id
            if not session_id:
                # bids_dirÍ∞Ä Ïª®ÌÖåÏù¥ÎÑà Í≤ΩÎ°úÏù∏ÏßÄ Ìò∏Ïä§Ìä∏ Í≤ΩÎ°úÏù∏ÏßÄ ÌôïÏù∏
                # Ïª®ÌÖåÏù¥ÎÑà Í≤ΩÎ°ú(/app/data)Î©¥ Ìò∏Ïä§Ìä∏ Í≤ΩÎ°úÎ°ú Î≥ÄÌôò
                if bids_dir.startswith('/app/data/'):
                    check_bids_dir = bids_dir.replace('/app/data/', f'{host_data_dir}/')
                else:
                    # Ïù¥ÎØ∏ Ìò∏Ïä§Ìä∏ Í≤ΩÎ°úÏù∏ Í≤ΩÏö∞ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
                    check_bids_dir = bids_dir
                
                subject_path = Path(check_bids_dir) / subject_id
                print(f"[Backend] Checking for sessions in: {subject_path}")
                
                if subject_path.exists():
                    session_dirs = [d.name.replace("ses-", "") for d in subject_path.iterdir() 
                                    if d.is_dir() and d.name.startswith("ses-")]
                    if session_dirs:
                        actual_session = session_dirs[0]
                        print(f"[Backend] ‚úÖ Auto-detected session: {actual_session}")
                    else:
                        print(f"[Backend] ‚ö†Ô∏è No session directories found in: {subject_path}")
                else:
                    print(f"[Backend] ‚ö†Ô∏è Warning: Subject path not found: {subject_path}")
            
            container_name = f"{subject_id}" + (f"_ses-{actual_session}" if actual_session else "")
            if processes:
                container_name += f"_{processes[0]}"

            container_log_dir = Path(output_dir) / "logs" / (processes[0] if processes else "")
            container_log_dir.mkdir(parents=True, exist_ok=True)
            (container_log_dir / "fin").mkdir(exist_ok=True)
            (container_log_dir / "error").mkdir(exist_ok=True)

            container_log_file = container_log_dir / "fin" / f"{container_name}.log"
            container_error_log_file = container_log_dir / "error" / f"{container_name}_error.log"

            # ---- Î∂ÑÍ∏∞: ÎØ∏ÎãàÎ©Ä vs ÏùºÎ∞ò ----
            if simple_structural:
                use_fs_licence_min = Path(fs_licence).exists() and ('proc_structural' in processes)

                # docker run Î≥ºÎ•® ÎßàÏö¥Ìä∏
                cmd = (
                    f"docker run --rm --name {container_name} "
                    f"-v {host_bids_dir}:{host_bids_dir} "
                    f"-v {host_output_dir}:{host_output_dir} "
                )
                if use_fs_licence_min:
                    cmd += f"-v {host_fs_licence}:{host_fs_licence} "

                cmd += (
                    "micalab/micapipe:v0.2.3 "
                    f"-bids {host_bids_dir} "
                    f"-out {host_output_dir} "
                    f"-sub {sub_id} "
                )
                if actual_session:  # ÎòêÎäî ses
                    cmd += f"-ses {actual_session} "

                cmd += "-proc_structural "
                if use_fs_licence_min:
                    cmd += f"-fs_licence {host_fs_licence} "

                cmd += f"> {container_log_file} 2> {container_error_log_file}"
            else:
                # ÏùºÎ∞ò
                base_switches = [f"-{p}" for p in processes]

                extra_tokens = (
                    #(proc_structural_flags or []) + 
                    #(proc_surf_flags or []) + 
                    (post_structural_flags or []) +
                    (proc_func_flags or []) +
                    (dwi_flags or []) +
                    (sc_flags or [])
                )
                normalized = normalize_flags(extra_tokens)
                process_flags = join_tokens(base_switches + normalized)

                use_fs_licence = Path(fs_licence).exists() and (
                    ('proc_structural' in processes) or
                    ('proc_surf' in processes and freesurfer)
                )

                fs_licence_mount = ""
                if use_fs_licence:
                    fs_licence_mount = f"-v {host_fs_licence}:{host_fs_licence}"

                cmd = (
                    f"docker run --rm --name {container_name} "
                    f"-v {host_bids_dir}:{host_bids_dir} "
                    f"-v {host_output_dir}:{host_output_dir} "
                )
                if fs_licence_mount:
                    cmd += f"{fs_licence_mount} "

                cmd += (
                    "micalab/micapipe:v0.2.3 "
                    f"-bids {host_bids_dir} "
                    f"-out {host_output_dir} "
                    f"-sub {sub_id} "
                )
                if actual_session:
                    cmd += f"-ses {actual_session} "

                cmd += f"-threads {threads} {process_flags} "

                if 'proc_surf' in processes:
                    cmd += f"-freesurfer {'TRUE' if freesurfer else 'FALSE'} "

                # ÎùºÏù¥ÏÑ†Ïä§Îäî ÏúÑ Ï°∞Í±¥(use_fs_licence)Ïùº Îïå Ìï≠ÏÉÅ ÎÑòÍπÄ
                if use_fs_licence:
                    cmd += f"-fs_licence {host_fs_licence} "

                cmd += f"> {container_log_file} 2> {container_error_log_file}"

            # Ïã§Ìñâ
            process = subprocess.Popen(
                cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
            )
            
            # DB Í∏∞Î°ù
            session = SessionLocal()
            try:
                log = CommandLog(
                    command=cmd,
                    output=f"Container started: {container_name} (PID: {process.pid})",
                    error=""
                )
                session.add(log)
                mica_job = MicaPipelineJob(
                    job_id=container_name,
                    subject_id=subject_id,
                    session_id=actual_session,
                    processes=",".join(processes),
                    container_name=container_name,
                    pid=process.pid,
                    status="processing",
                    progress=0.0,
                    log_file=str(container_log_file),
                    error_log_file=str(container_error_log_file)
                )
                session.add(mica_job)
                session.commit()
            finally:
                session.close()
            
            return {
                "success": True,
                "mode": "single_subject",
                "command": cmd,
                "output": f"‚úÖ MICA PipelineÏù¥ Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú ÏãúÏûëÎêòÏóàÏäµÎãàÎã§.\n\nÏª®ÌÖåÏù¥ÎÑà: {container_name}\nPID: {process.pid}\n\nüí° 'Î°úÍ∑∏ ÌôïÏù∏' ÌÉ≠ ÎòêÎäî 'Download Results'ÏóêÏÑú Ïã§Ìñâ ÏÉÅÌÉúÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.",
                "error": "",
                "returncode": 0,
                "subject_id": subject_id,
                "session_id": actual_session,
                "session_auto_detected": bool(actual_session and not session_id),
                "processes": processes,
                "container_name": container_name,
                "pid": process.pid,
                "job_id": container_name,
                "timestamp": datetime.now().isoformat()
            }
            
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": f"Command timeout after {data.get('timeout', 3600)} seconds",
            "returncode": -1
        }
    except HTTPException:
        # HTTPExceptionÏùÄ Í∑∏ÎåÄÎ°ú Ï†ÑÎã¨
        raise
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)


@app.get("/mica-logs")
async def get_mica_logs(output_dir: str = "/app/data/derivatives"):
    """MICA Pipeline Î°úÍ∑∏ Î™©Î°ùÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§."""
    try:
        logs_dir = Path(output_dir) / "logs"
        
        if not logs_dir.exists():
            return {
                "success": True,
                "logs": [],
                "message": "Î°úÍ∑∏ ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏïÑÏßÅ ÏÉùÏÑ±ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§."
            }
        
        logs = []
        
        # Í∞Å ÌîÑÎ°úÏÑ∏Ïä§ ÎîîÎ†âÌÜ†Î¶¨ ÏàúÌöå
        for process_dir in logs_dir.iterdir():
            if not process_dir.is_dir():
                continue
            
            process_name = process_dir.name
            
            # fin ÎîîÎ†âÌÜ†Î¶¨Ïùò Î°úÍ∑∏ ÌååÏùº
            fin_dir = process_dir / "fin"
            error_dir = process_dir / "error"
            
            if fin_dir.exists():
                for log_file in fin_dir.iterdir():
                    if log_file.is_file() and log_file.suffix == ".log":
                        error_file = error_dir / f"{log_file.stem}_error.log"
                        
                        logs.append({
                            "process": process_name,
                            "subject": log_file.stem,
                            "log_file": str(log_file),
                            "error_file": str(error_file) if error_file.exists() else None,
                            "size": log_file.stat().st_size,
                            "modified": log_file.stat().st_mtime,
                            "has_error": error_file.exists() and error_file.stat().st_size > 0
                        })
        
        # ÏàòÏ†ï ÏãúÍ∞Ñ Í∏∞Ï§Ä ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨
        logs.sort(key=lambda x: x["modified"], reverse=True)
        
        return {
            "success": True,
            "logs": logs,
            "count": len(logs)
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.get("/mica-log-content")
async def get_mica_log_content(log_file: str, lines: int = 100):
    """MICA Pipeline Î°úÍ∑∏ ÌååÏùº ÎÇ¥Ïö©ÏùÑ ÏùΩÏäµÎãàÎã§."""
    try:
        log_path = Path(log_file)
        
        # Î≥¥Ïïà: /app/data Ïô∏Î∂Ä Í≤ΩÎ°ú Ï†ëÍ∑º Ï∞®Îã®
        if not str(log_path).startswith("/app/data"):
            raise HTTPException(status_code=403, detail="Access denied")
        
        if not log_path.exists():
            raise HTTPException(status_code=404, detail="Log file not found")
        
        # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏
        file_size = log_path.stat().st_size
        
        # ÎßàÏßÄÎßâ NÏ§Ñ ÏùΩÍ∏∞
        with log_path.open("r", encoding="utf-8", errors="ignore") as f:
            all_lines = f.readlines()
            content_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
        
        return {
            "success": True,
            "file": str(log_path),
            "size": file_size,
            "total_lines": len(all_lines),
            "returned_lines": len(content_lines),
            "content": "".join(content_lines)
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.get("/mica-containers")
async def get_mica_containers():
    """Ïã§Ìñâ Ï§ëÏù∏ micapipe Ïª®ÌÖåÏù¥ÎÑà Î™©Î°ùÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§."""
    try:
        # docker ps Î™ÖÎ†π Ïã§Ìñâ
        result = subprocess.run(
            "docker ps --filter 'name=sub-' --format '{{.Names}}\t{{.Status}}\t{{.Image}}\t{{.RunningFor}}'",
            shell=True,
            capture_output=True,
            text=True,
            timeout=10
        )
        
        containers = []
        if result.stdout.strip():
            for line in result.stdout.strip().split('\n'):
                parts = line.split('\t')
                if len(parts) >= 4:
                    containers.append({
                        "name": parts[0],
                        "status": parts[1],
                        "image": parts[2],
                        "running_for": parts[3]
                    })
        
        return {
            "success": True,
            "containers": containers,
            "count": len(containers)
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.post("/mica-container-stop")
async def stop_mica_container(container_name: str):
    """micapipe Ïª®ÌÖåÏù¥ÎÑàÎ•º Ï¢ÖÎ£åÌï©ÎãàÎã§."""
    try:
        # Î≥¥Ïïà: sub- Î°ú ÏãúÏûëÌïòÎäî Ïª®ÌÖåÏù¥ÎÑàÎßå Ï¢ÖÎ£å Í∞ÄÎä•
        if not container_name.startswith("sub-"):
            raise HTTPException(status_code=403, detail="Only sub-* containers can be stopped")
        
        # docker stop Î™ÖÎ†π Ïã§Ìñâ
        result = subprocess.run(
            f"docker stop {container_name}",
            shell=True,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        return {
            "success": result.returncode == 0,
            "container": container_name,
            "message": f"Container {container_name} stopped" if result.returncode == 0 else "Failed to stop container",
            "output": result.stdout,
            "error": result.stderr
        }
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Timeout while stopping container"
        }
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.get("/mica-jobs")
async def get_mica_jobs(status: str = None):
    """MICA Pipeline Job Î™©Î°ùÏùÑ Ï°∞ÌöåÌï©ÎãàÎã§."""
    try:
        db = SessionLocal()
        try:
            query = db.query(MicaPipelineJob)
            
            # ÏÉÅÌÉú ÌïÑÌÑ∞ÎßÅ
            if status:
                query = query.filter(MicaPipelineJob.status == status)
            
            jobs = query.order_by(MicaPipelineJob.started_at.desc()).all()
            
            # Ïã§ÏãúÍ∞ÑÏúºÎ°ú Ïª®ÌÖåÏù¥ÎÑà/Airflow ÏÉÅÌÉú ÌôïÏù∏ Î∞è ÏóÖÎç∞Ïù¥Ìä∏
            for job in jobs:
                if job.status == "processing":
                    # AirflowÎ°ú Ïã§ÌñâÎêú jobÏù∏ÏßÄ ÌôïÏù∏ (job_idÍ∞Ä "mica_"Î°ú ÏãúÏûë)
                    if job.job_id.startswith("mica_"):
                        # Airflow DAG Run ÏÉÅÌÉú ÌôïÏù∏
                        try:
                            airflow_response = requests.get(
                                f"{AIRFLOW_API}/dags/mica_pipeline/dagRuns/{job.job_id}",
                                auth=_auth(),
                                timeout=5
                            )
                            if airflow_response.status_code == 200:
                                airflow_data = airflow_response.json()
                                airflow_state = airflow_data.get("state")
                                
                                if airflow_state in ["success", "failed"]:
                                    job.status = "completed" if airflow_state == "success" else "failed"
                                    job.completed_at = datetime.utcnow()
                                    job.progress = 100.0
                                    
                                    if airflow_state == "failed":
                                        # Airflow Î°úÍ∑∏ÏóêÏÑú ÏóêÎü¨ Î©îÏãúÏßÄ Ï∂îÏ∂ú (Í∞ÑÎã®Ìûà Ï≤òÎ¶¨)
                                        job.error_message = "Airflow DAG execution failed. Check Airflow UI for details."
                                    
                                    db.commit()
                        except Exception as e:
                            print(f"Failed to check Airflow status: {e}")
                    
                    # ÏßÅÏ†ë Ïã§ÌñâÎêú jobÏùò Í≤ΩÏö∞ Docker Ïª®ÌÖåÏù¥ÎÑà ÌôïÏù∏
                    else:
                        result = subprocess.run(
                            f"docker inspect {job.container_name}",
                            shell=True,
                            capture_output=True,
                            text=True,
                            timeout=5
                        )
                        
                        if result.returncode != 0:
                            # Ïª®ÌÖåÏù¥ÎÑàÍ∞Ä ÏóÜÏùå = ÏôÑÎ£å ÎòêÎäî Ïã§Ìå®
                            # Î°úÍ∑∏ ÌååÏùºÏóêÏÑú ÏóêÎü¨ ÌôïÏù∏
                            has_error = False
                            error_message = None
                            
                            # 1. ÏóêÎü¨ Î°úÍ∑∏ ÌååÏùº ÌôïÏù∏
                            error_log_path = Path(job.error_log_file) if job.error_log_file else None
                            if error_log_path and error_log_path.exists():
                                error_size = error_log_path.stat().st_size
                                if error_size > 0:
                                    has_error = True
                                    with open(error_log_path, 'r', encoding='utf-8', errors='ignore') as f:
                                        error_message = f.read()[-500:]
                            
                            # 2. ÌëúÏ§Ä Ï∂úÎ†• Î°úÍ∑∏ÏóêÏÑú "[ ERROR ]" ÌôïÏù∏ (MICA PipelineÏùÄ exit 0Î°ú Ï¢ÖÎ£åÌï¥ÎèÑ ÏóêÎü¨ Î∞úÏÉù Í∞ÄÎä•)
                            if not has_error:
                                log_path = Path(job.log_file) if job.log_file else None
                                if log_path and log_path.exists():
                                    with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
                                        log_content = f.read()
                                        if "[ ERROR ]" in log_content or "ERROR" in log_content:
                                            has_error = True
                                            # ÏóêÎü¨ Î∂ÄÎ∂Ñ Ï∂îÏ∂ú
                                            error_lines = [line for line in log_content.split('\n') if 'ERROR' in line]
                                            error_message = '\n'.join(error_lines[-5:]) if error_lines else log_content[-500:]
                            
                            # ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
                            job.status = "failed" if has_error else "completed"
                            job.completed_at = datetime.utcnow()
                            job.progress = 100.0
                            if has_error:
                                job.error_message = error_message
                            db.commit()
            
            # JSON ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
            jobs_data = []
            for job in jobs:
                jobs_data.append({
                    "id": job.id,
                    "job_id": job.job_id,
                    "subject_id": job.subject_id,
                    "session_id": job.session_id,
                    "processes": job.processes,
                    "container_name": job.container_name,
                    "pid": job.pid,
                    "status": job.status,
                    "progress": job.progress,
                    "log_file": job.log_file,
                    "error_log_file": job.error_log_file,
                    "started_at": job.started_at.isoformat() if job.started_at else None,
                    "completed_at": job.completed_at.isoformat() if job.completed_at else None,
                    "error_message": job.error_message,
                    "duration": (job.completed_at - job.started_at).total_seconds() if job.completed_at else None
                })
            
            return {
                "success": True,
                "jobs": jobs_data,
                "count": len(jobs_data),
                "summary": {
                    "processing": sum(1 for j in jobs if j.status == "processing"),
                    "completed": sum(1 for j in jobs if j.status == "completed"),
                    "failed": sum(1 for j in jobs if j.status == "failed")
                }
            }
        finally:
            db.close()
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@app.post("/mica-job-update")
async def update_mica_job_status(data: dict):
    """MICA Pipeline Job ÏÉÅÌÉúÎ•º ÏàòÎèôÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏Ìï©ÎãàÎã§."""
    try:
        job_id = data.get("job_id")
        status = data.get("status")
        progress = data.get("progress")
        error_message = data.get("error_message")
        
        if not job_id:
            raise HTTPException(status_code=400, detail="job_id is required")
        
        db = SessionLocal()
        try:
            job = db.query(MicaPipelineJob).filter(MicaPipelineJob.job_id == job_id).first()
            
            if not job:
                raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
            
            if status:
                job.status = status
                if status in ["completed", "failed"]:
                    job.completed_at = datetime.utcnow()
                    job.progress = 100.0
            
            if progress is not None:
                job.progress = progress
            
            if error_message:
                job.error_message = error_message
            
            db.commit()
            
            return {
                "success": True,
                "job_id": job_id,
                "status": job.status,
                "message": "Job status updated"
            }
        finally:
            db.close()
    except Exception as e:
        import traceback
        error_detail = f"{str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        print(f"ERROR in /run-mica-pipeline: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

# --- Ïú†Ìã∏: Îç∞Ïù¥ÌÑ∞ Î£®Ìä∏ Í≤∞Ï†ï (/app/data Í∏∞Î≥∏) ---
def pick_existing_data_root() -> Path:
    # Ïó¨Îü¨ Í≤ΩÎ°ú ÏãúÎèÑ (Ïª®ÌÖåÏù¥ÎÑà ÎÇ¥Î∂Ä Í≤ΩÎ°ú Ïö∞ÏÑ†)
    possible_roots = [
        Path("/app/data"),  # Ïª®ÌÖåÏù¥ÎÑà ÎÇ¥Î∂Ä Í≤ΩÎ°ú (ÎßàÏö¥Ìä∏Îê®)
        Path(os.getenv("HOST_DATA_DIR", "/home/admin1/Documents/aimedpipeline/data")),  # Ìò∏Ïä§Ìä∏ Í≤ΩÎ°ú
    ]
    
    for root in possible_roots:
        derivatives_path = root / "derivatives"
        if derivatives_path.exists():
            return root
    
    # Î™®Îì† Í≤ΩÎ°úÍ∞Ä Ïã§Ìå®Ìïú Í≤ΩÏö∞
    tried_paths = [str(r / "derivatives") for r in possible_roots]
    raise HTTPException(
        status_code=404, 
        detail=f"Derivatives not found. Tried: {', '.join(tried_paths)}"
    )

# --- Î≥¥ÏïàÏö©: derivatives Î∞îÍπ• Ï†ëÍ∑º Î∞©ÏßÄ ---
def _ensure_inside(root: Path, target: Path) -> Path:
    root_r = root.resolve()
    target_r = target.resolve()
    if not str(target_r).startswith(str(root_r)):
        raise HTTPException(status_code=403, detail="Path traversal detected")
    return target_r

# --- Ï†ÑÏ≤¥ derivativesÎ•º ZIPÏúºÎ°ú Î∞òÌôò ---
@app.get("/download-derivatives")
def download_derivatives():
    data_root = pick_existing_data_root()
    derivatives_root = (data_root / "derivatives").resolve()

    if not derivatives_root.exists():
        raise HTTPException(status_code=404, detail=f"Target not found: {derivatives_root}")

    # ZIP ÌååÏùº Í≤ΩÎ°ú Ï§ÄÎπÑ
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"derivatives_all_{ts}.zip"
    tmp_dir = Path("/app/tmp")
    tmp_dir.mkdir(parents=True, exist_ok=True)
    zip_path = tmp_dir / zip_name

    # ZIP ÏÉùÏÑ±
    import zipfile
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
        for p in derivatives_root.rglob("*"):
            if p.is_file():
                arcname = p.relative_to(derivatives_root)
                zf.write(p, arcname=str(arcname))

    return FileResponse(
        path=str(zip_path),
        filename=zip_name,
        media_type="application/zip",
    )